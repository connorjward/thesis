\documentclass[thesis]{subfiles}

\begin{document}

% TODO: mention that:
% \pyop3 uses pymbolic\footnote{https://documen.tician.de/pymbolic/index.html} to store and manipulate the symbolic expressions.
% TODO: Discuss how data dependencies are (not) handled.
% TODO: It's written in Python, but that's fine because it's a compiler - Julia/another faster language would not help us
% TODO: embedded DSL is used for interoperability with other libraries, and Python is everywhere, can also use parser of parent language (reference UFL for its thoughts on this?)

\chapter{\pyop3}
\label{chapter:pyop3}

The contributions of this thesis may be split into two parts:
(a) the development of novel \emph{abstractions} for mesh stencils, and
(b) the \emph{implementation} of these abstractions as part of a working piece of software.
Whereas the previous two chapters have focussed on the former of these, in this chapter we present information pertaining to contribution (b).
Namely, we present the new mesh stencil package \pyop3.

\section{The execution model}

The interface to \pyop3 may be thought of as being two different parts: the \emph{data model} and the \emph{execution model}.
The former of these has already been introduced, being the subject of the last two chapters: data structures, axis trees, and index trees.
Here we present details of the latter.

In \pyop3, the execution model refers to the elements of the language used to express actual computations.
It has been designed to resemble the pseudocode for mesh stencil algorithms (e.g. \cref{alg:fem_assembly}) as closely as possible within the constraints of an embedded DSL.
Its core components therefore are \emph{loops}, \emph{assignments}, and \emph{function calls}.

\subsection{Code generation}

In order to use \pyop3, users create symbolic \emph{loop expressions} from these core components, matching the algorithm they wish to apply.
Then, behind the scenes, runtime code generation is used to \emph{lower} the symbolic expression down to executable code.

\begin{figure}
  \centering
  \includegraphics{codegen_flowchart.pdf}
  \caption{
    The code generation pipeline for the compilation of a loop expression into a callable function.
    The input (`Loop expression') and output (`Compiled function') are shown in blue whilst the intermediate processes are red.
    The dashed line from input expression to output function is included to represent the fact that the compiled function requires additional data from the loop expression in order to be executable.
  }
  \label{fig:codegen_flowchart}
\end{figure}

This code generation pipeline is shown in \cref{fig:codegen_flowchart}.
Loop expressions are lowered first to loopy (\cref{sec:pyop2_codegen}) and then to C, before calling a traditional compiler to build the compiled library.
To execute the computation, the compiled function is then called using appropriate data from the input loop expression.
For example, the simple loop expression

\begin{pyinline}
  loop(p,
       kernel(dat0[f(p)], dat1[p]))
\end{pyinline}

\noindent
executing some local kernel (\pycode{kernel}) for each loop index entry (\pycode{p}), expects, at a minimum, two arguments to the generated code: one for \pycode{dat0} and another for \pycode{dat1}.
Further arguments may be necessary if, say, \pycode{dat0} is ragged and requires a tabulated layout function or if map \pycode{f} is a lookup table.

Both \pyop3 loop expressions and loopy kernels are \emph{transformable representations}, so, as the code is compiled, these layers are able to reason about the program and transform it to alternative but equivalent forms.
This is typically done for optimisation purposes but \pyop3 also uses the technique more functionally to facilitate compilation.
The former of these is discussed in \cref{sec:codegen_optimisation} whilst the latter is explained in the forthcoming examples.

\begin{example}{Constant assignment}
\label{example:assign_codegen}

As a first example of the code generation process in \pyop3, we consider the following assignment operation:

\begin{pyinline}
  axes = AxisTree.from_iterable([Axis(5), Axis(3)])
  dat = Dat(axes)
  dat[::2, 1:].assign(666)
\end{pyinline}

\noindent
Here, we assign the scalar value \pycode{666} to entries in the indexed linear array \pycode{dat[::2, 1:]}.
Since a newly initialised \pycode{Dat} starts with a memory buffer of all zeros, following this operation the entries in \pycode{dat} will be

\begin{equation*}
  \pycode{[0, 666, 666, 0, 0, 0, 0, 666, 666, 0, 0, 0, 0, 666, 666]}.
\end{equation*}

When the \pycode{assign()} method is called, the following single-node loop expression is produced:

\begin{center}
  \includegraphics{codegen_example1_expr.pdf}
\end{center}

To generate code for this expression, the expression tree is descended and appropriate loopy constructs - domains, instructions, and arguments - are generated.
These are then used to construct a complete loopy kernel.

Given that the example only has a single node the traversal of the loop expression is trivial.

Before processing the \pycode{write} node we make the following observations:

\begin{itemize}
  \item
    If we assume the axes of the indexed array to be labelled $c$ and $d$, mapping from $a$ and $b$ respectively then the indexed array has the following structure:

    \begin{center}
      \includegraphics{codegen_example1_axis_tree.pdf}
    \end{center}

  \item
    The substituted layout function (\cref{chapter:indexing}) is therefore given by

    \begin{equation*}
      \textnormal{offset}(i_c, i_d) = 6 i_c + i_d + 1.
    \end{equation*}
\end{itemize}

\begin{listing}
  \centering
  \begin{minipage}{.9\textwidth}
    \inputminted{text}{./scripts/artefacts/codegen_example1_loopy_kernel_tidy.txt}
  \end{minipage}
  \caption{
    Abbreviated textual representation of the loopy kernel generated for the example expression in \cref{example:assign_codegen}.
  }
  \label{listing:codegen_example1_loopy_kernel}
\end{listing}

We are now in a position to generate code for the node:

\begin{enumerate}
  \item
    Loops are generated for the axes of the \emph{indexed} array and the mapping from axis label to loop index name is recorded.
    In this case, two loops with extents 3 and 2 would be created.
    If the former has a loop index called \pycode{"i_0"} and the latter \pycode{"i_1"} then the mapping $\{ c \to \pycode{"i_0"},\ d \to \pycode{"i_1"}\}$ is also stored.

  \item
    The assignment instruction is generated with the appropriate substituted layout function, replacing axis indices as appropriate.
    For this example this produces the instruction:

    \begin{pyinline}
      dat[6*i_0 + i_1 + 1] = 666
    \end{pyinline}
\end{enumerate}

\begin{listing}
  \caption{
    The C code generated from the loopy kernel in \cref{listing:codegen_example1_loopy_kernel}.
  }
  \centering
  \begin{minipage}{.9\textwidth}
    \inputminted{c}{./scripts/artefacts/codegen_example1_c_code_tidy.c}
  \end{minipage}
  \label{listing:codegen_example1_c_code}
\end{listing}

The actual loopy kernel generated for this expression is shown in \cref{listing:codegen_example1_loopy_kernel}, and the subsequently generated C code in \cref{listing:codegen_example1_c_code}.
Note that the input array has been passed through as an argument in the generated code but that it has been given a new name: \pycode{array_0}.
This is because kernel arguments are renamed during code generation to avoid unnecessary cache misses.

\end{example}

\begin{example}{Outer loops and maps}

We now consider a more complex example that better represents the sorts of computations that might occur within a continuum mechanics simulation:

\todo[inline]{Should make this a listing instead. Then we can reference it. Also it doesn't format terribly well. Also then we can reference particular line numbers.}

\begin{pyinline}
  axis_a = Axis(5, "a")
  axis_b = Axis(3, "b")
  dat0 = Dat(AxisTree.from_iterable([axis_a, axis_b]))
  dat1 = Dat(axis_a)
  map0 = Map({"a": MapComponent("a", arity=2, ...)})

  loop(
    ia := axis_a.index(),
    kernel(dat0[map0(ia), :], dat1[ia]),
  )
\end{pyinline}

We remark upon the following:

\begin{itemize}
  \item
    \pycode{dat0} is a two-dimensional array with shape \pycode{(5, 3)} and axis labels $a$ and $b$, whereas \pycode{dat1} is one-dimensional with shape \pycode{(5,)}, having only axis $a$ in its axis tree.

  \item
    \pycode{map0} maps entries in axis $a$ to other entries in axis $a$.
    It has arity 2.

  \item
    The \pycode{loop} expression declares that one should:

    \begin{itemize}
      \item
        Loop over the 5 entries in axis $a$.
      \item
        Evaluate the local kernel (\pycode{kernel}) passing in indexed versions of \pycode{dat0} and \pycode{dat1}.
    \end{itemize}

  \item
    \pycode{dat0} is indexed with an arity 2 tabulated map and a slice, giving it an indexed axis tree like:

    \begin{center}
      \includegraphics{codegen_example2_axis_tree.pdf}
    \end{center}

    \noindent
    and substituted layout

    \begin{equation*}
      \textnormal{offset}(L^p_a,i_b, i_c) = 3 \pycode{[...][?$L^p_a$?,?$i_c$?]} + i_b.
    \end{equation*}

    \noindent
    where \pycode{[...][?$L^p_a$?,?$i_c$?]} is the lookup expression for the tabulated map.

  \item
    \pycode{dat1} is fully indexed by the loop index and so contains just a single entry.
    It simply has the substituted layout

    \begin{equation*}
      \textnormal{offset}(L^p_a) = L^p_a.
    \end{equation*}

  \item
    \pycode{kernel} is a function taking two arguments with size 6 and intent \pycode{READ}, and size 1 and intent \pycode{INC} respectively.
    The computation performed by \pycode{kernel} is represented by a loopy kernel.
\end{itemize}

\begin{table}
  \centering

  \begin{tblr}{|[1pt]c|[1pt]l|[1pt]l|[1pt]}
    \hline[1pt]
    \textbf{Intent} & \textbf{Pack instruction} & \textbf{Unpack instruction} \\
    \hline[1pt]
    \pycode{READ} & \ccode{write(temporary, indexed)} & \tableDash \\
    \hline
    \pycode{WRITE} & \tableDash & \ccode{write(indexed, temporary)} \\
    \hline
    \pycode{RW} & \ccode{write(temporary, indexed)} & \ccode{write(indexed, temporary)} \\
    \hline
    \pycode{INC} & \ccode{write(temporary, 0)} & \ccode{inc(indexed, temporary)} \\
    % \hline
    % \pycode{MIN_WRITE} & \tableDash & \ccode{min(indexed, temporary)} \\
    % \hline
    % \pycode{MIN_INC} & \ccode{write(temporary, 0)} & \ccode{min(indexed, temporary)} \\
    % \hline
    % \pycode{MAX_WRITE} & \tableDash & \ccode{max(indexed, temporary)} \\
    % \hline
    % \pycode{MAX_INC} & \ccode{write(temporary, 0)} & \ccode{max(indexed, temporary)} \\
    \hline[1pt]
  \end{tblr}

  \caption{
    Intent values supported by \pyop3 kernels and their corresponding pack/unpack instructions.
    In the instructions, the variable `\ccode{indexed}' is used to represent the indexed view of some piece of global data (e.g. \pycode{dat0[map0(p)]}) and the variable `\ccode{temporary}' is the temporary buffer for storing the materialised data.
    Table entries marked with a `\pycode{-}' indicate that no pack/unpack instruction is emitted for this intent.
  }
  \label{tab:intents}
\end{table}

\subsubsection{Kernel intents}

% why do we provide the intents? genuinely why? gives us some memory optimisations (e.g. needn't load if overwriting anyway)
% summarised in the table - explain both write and inc

% initial expression tree contains implicit packs and unpacks:

\begin{center}
  \includegraphics{codegen_example2_expr_before.pdf}
\end{center}

% we convert to explicit operations by transforming the expression tree - this gives:

\begin{center}
  \includegraphics{codegen_example2_expr_after.pdf}
\end{center}

We are now ready to lower the loop expression to loopy.

\subsubsection{Code generation}

% LOWERING/COMPILATION
% first the loop is visited, loop over 5, gives us $L^p_a$.

\begin{listing}
  \centering
  \begin{minipage}{.9\textwidth}
    \inputminted{c}{./scripts/artefacts/codegen_example2_c_code_tidy.c}
  \end{minipage}
  \caption{
    % TODO: want to reference the particular listing!
    C code generated for the loop expression.
  }
  \label{listing:codegen_example2_c_code}
\end{listing}

% For our demo the generated C code can be seen in \cref{lst:codegen_example_c_code}.

\end{example}

\subsection{Optimisation}
\label{sec:codegen_optimisation}

% "At present no optimisations are applied to either the \pyop3 or loopy internal representations.
% This is considered future work and some potentially fruitful optimising transformations are detailed in \cref{sec:future_work}.

\section{Data structures}
\label{sec:data_structures}

Thus far we have only considered
Currently \pycode{Dats} use \numpy arrays as the underlying data storage mechanism, but we intend to permit further array types to enable targeting accelerator architectures like CUDA GPUs (see \cref{sec:future_work}).

As well as \pycode{Dats}, \pyop3 has \pycode{Globals}, for global scalar values, and \pycode{Mats}, for matrices.
% globals are really simple
% indexed with 0, 1, or 2 index trees, row, col axis trees

% table of differences? # axis trees, memory rep, possible dtypes?
% think it's overkill. just needs a sentence

\subsection{Matrices}
\label{sec:impl_matrices}

% initialisation...
% uses PETSc, support for different formats (block, nest, CUDA, ...) - restricted dtype

\pycode{Mats} require 2 axis trees: one for the rows of the matrix and one for the columns.
They rely on PETSc \ccode{Mat} objects for the underlying data storage.

\subsubsection{Indexing}

% NOTE: This sort of thing needs to be clearer - we index axis trees, so they have two...
Since \pycode{Mats} have two axis trees, two indices are needed when indexing.

% and it also returns a temporary that can be manipulated? (adds row and col labels...)

\subsubsection{Code generation}

Since \pycode{Mats} are not internally represented by a flat buffer they must be accessed through PETSc's C API.
For example, to read values from a PETSc \ccode{Mat} the following function must be used:

\begin{cinline}
  MatGetValues(Mat mat,
               PetscInt m, PetscInt idxm[],
               PetscInt n, PetscInt idxn[],
               PetscScalar v[])
\end{cinline}

This function returns a dense block of values (\ccode{v[]}) read from matrix \ccode{mat} at the row and column indices specified by arrays \ccode{idxm[]} and \ccode{idxn[]} respectively.
Analogous routines exist for writing and incrementing.
% this is naturally less efficient than direct insertion into an array, can be a bottleneck

For \pyop3 to be able to interface with PETSc matrices calls to these getter and setter routines have to be emitted.

One challenge presented by this API is that it is no longer possible for \pyop3 to represent offsets purely as a symbolic quantity because PETSc expects these to be packed into the index arrays \ccode{idxm} and \ccode{idxn}.
To fix this \pyop3 performs an additional step where these arrays are explicitly tabulated by evaluating the layout functions. % show? speed up?

% NOTE: This is a specific instance of a general "compression" operation (future work)

\end{document}
