\documentclass[thesis]{subfiles}

\begin{document}

\chapter{The execution model}
\label{chapter:execution_model}

% TODO: Discuss how data dependencies are (not) handled.
% TODO: It's written in Python, but that's fine because it's a compiler - Julia/another faster language would not help us
% TODO: embedded DSL is used for interoperability with other libraries, and Python is everywhere, can also use parser of parent language (reference UFL for its thoughts on this?)

Thus far we have established a new abstraction for mesh-like data structures, and an approach for symbolically representing smaller ``packed" parts of them.
We now move on to consider their concrete implementation, and how to express computations with them.

\section{Code generation}

\begin{figure}
  \centering
  \includegraphics{codegen_flowchart.pdf}
  \caption{
    The code generation pipeline for the compilation of a loop expression into a callable function.
    The input (``Loop expression") and output (``Compiled function") are shown in blue whilst the intermediate processes are red.
    The dashed line from input expression to output function is included to represent the fact that the compiled function additionally requires data from the input loop expression in order to be usable.
  }
  \label{fig:codegen_flowchart}
\end{figure}

% pyop3 has loop expressions that are lowered to loopy kernels that are lowered to C/etc
% (ref loopy ch. 2)
% "like pyop2, pyop3 uses loopy as an intermediate representation..."

% (paragraph)
% pyop3 takes a similar approach to loopy in that computations are expressed in a transformable representation
% pyop3 does things at a higher level... (ref later bits)
% (transformations can be split into functional ones and optimisation ones)
% "At present no optimisations are applied to either the \pyop3 or loopy internal representations.
% This is considered future work and some potentially fruitful optimising transformations are detailed in \cref{sec:future_work}.

% % dual role of loop expression: codegen spec *and* data to pass through (ref. figure)

\begin{example}
\label{example:assign_codegen}

"we start with something simple..."

\begin{pyinline}
  axes = AxisTree.from_iterable([5, 3])
  dat = Dat(axes)
  # set entries to [0, 666, 666, 0, 0, 0, 0, 666, 666, 0, 0, 0, 0, 666, 666]
  dat[::2, 1:].assign(666)
\end{pyinline}

% (what does this do)
% dat[::2, 1:] is a view, creates assignment expression that is eagerly evaluated

\begin{center}
  \includegraphics{codegen_example1_expr.pdf}
\end{center}

% we have a single node in our pyop3 expression
% no transformations of the IR are necessary - single node (show)

To generate code from this expression the expression tree is descended and the appropriate loopy constructs are emitted.
In this case, as the expression tree only has a single node, this traversal is trivial.
% (old)
% % single pass
% % tree is traversed in a pre-order fashion (parents visited first)
% % loops emit domains and perhaps assignments (ragged)
% % then terminals (i.e. function calls and assignments) also emit domains and assignments

\begin{listing}
  \centering
  \begin{minipage}{.9\textwidth}
    \inputminted{text}{./scripts/artefacts/codegen_example1_loopy_kernel_tidy.txt}
  \end{minipage}
  \caption{
    Abbreviated textual representation of the loopy kernel generated for the example expression in \cref{example:assign_codegen}.
  }
  \label{listing:codegen_example1_loopy_kernel}
\end{listing}

To emit code for the \pycode{Assignment} node...
% loop over source axes...
% talk about substituted layouts here, connects source axis indices back to offset
The resulting loopy kernel is shown in \cref{listing:codegen_example1_loopy_kernel}.

\begin{listing}
  \centering
  \begin{minipage}{.9\textwidth}
    \inputminted{c}{./scripts/artefacts/codegen_example1_c_code_tidy.c}
  \end{minipage}
  \caption{
    TODO
  }
  \label{listing:codegen_example1_c_code}
\end{listing}

Having generated a loopy kernel, \pyop3 instructs loopy to generate a C string from it (\cref{listing:codegen_example1_c_code}).
This C code may then be compiled and run.
The arguments to the generated code (here simply \ccode{array0}) are managed by \pyop3 internally that passes the correct pointers.
% (old)
% Once at the level of a loopy kernel, the rest of the compilation becomes straightforward.
% Depending on the \textit{target} attribute belonging to a kernel, loopy can generate an appropriate string of C code that \pyop3 writes to a file and compiles with a traditional C compiler (e.g. gcc).
% Once compiled, \pyop3 can load the function pointer from the shared object file, allowing it to be executed.
% This process is unchanged from \pyop2.

% Unsurprisingly it looks very similar to the input loopy kernel (\cref{lst:codegen_example_loopy_kernel}).

\end{example}

\begin{example}  % outer loops and maps

We now consider a more complex example that is more typical of a continuum mechanics simulation:

\begin{pyinline}
  a = Axis(5, "a")
  b = Axis(3, "b")
  dat0 = Dat(AxisTree.from_iterable([a, b]))
  dat1 = Dat(a)

  # map0 maps from axis "a" to itself
  map0 = Map({"a": MapComponent("a", arity=2, ...)})

  loop(
    ia := a.index(),
    kernel(dat0[map0(ia), :], dat1[ia]),
  )
\end{pyinline}

% SEMANTICS
% add loop expression and kernel (explain)
% % loop expressions, made of a loop index and statements

% dat0 is a lot like an indexing example from before... (ref)

% In this loop expression a number of terms require further explanation.
% Firstly, \pycode{dat0} and \pycode{dat1} are defined to be arrays with shape \pycode{(8, 3)} and \pycode{(5,)} respectively, with their axis trees appearing as follows:
%
% \begin{center}
%   \includegraphics{codegen_example_dat_axes.pdf}
% \end{center}
%
% \pycode{dat1} is entirely indexed by the loop index \pycode{p}, and so the indexed array \pycode{dat1[p]} only has size 1.
% The situation for \pycode{dat0} is more complicated.
% \pycode{map0} is a map from axis $a$ to axis $x$ with arity 2, and the slice notation ``\pycode{:}" indicates a full slice over the inner axis $y$.
% The indexed object \pycode{dat0[map0(p), :]} passed through to \pycode{kernel} therefore has size 6.
%
% Lastly, the local kernel (\pycode{kernel}) is defined to be some function taking two arguments with size 6 and intent \pycode{READ}, and size 1 and intent \pycode{INC} respectively.

%TRANSFORM

\begin{table}
  \centering

  \begin{tblr}{|[1pt]c|[1pt]l|[1pt]l|[1pt]}
    \hline[1pt]
    \textbf{Intent} & \textbf{Pack instruction} & \textbf{Unpack instruction} \\
    \hline[1pt]
    \pycode{READ} & \ccode{write(temporary, indexed)} & \tableDash \\
    \hline
    \pycode{WRITE} & \tableDash & \ccode{write(indexed, temporary)} \\
    \hline
    \pycode{RW} & \ccode{write(temporary, indexed)} & \ccode{write(indexed, temporary)} \\
    \hline
    \pycode{INC} & \ccode{write(temporary, 0)} & \ccode{inc(indexed, temporary)} \\
    \hline
    \pycode{MIN_WRITE} & \tableDash & \ccode{min(indexed, temporary)} \\
    \hline
    \pycode{MIN_INC} & \ccode{write(temporary, 0)} & \ccode{min(indexed, temporary)} \\
    \hline
    \pycode{MAX_WRITE} & \tableDash & \ccode{max(indexed, temporary)} \\
    \hline
    \pycode{MAX_INC} & \ccode{write(temporary, 0)} & \ccode{max(indexed, temporary)} \\
    \hline[1pt]
  \end{tblr}

  \caption{
    Intent values supported by \pyop3 kernels and their corresponding pack/unpack instructions.
    In the instructions, the variable ``\ccode{indexed}" is used to represent the indexed view of some piece of global data (e.g. \pycode{dat0[map0(p)]}) and the variable ``\ccode{temporary}" is the temporary buffer for storing the materialised data.
    Table entries marked with a ``\pycode{-}" indicate that no pack/unpack instruction is emitted for this intent.
  }
  \label{tab:intents}
\end{table}

% need transform expression due to kernel needing contiguous data (temporaries)
% The expression tree transformation expanding implicit pack and unpack instructions for the example loop expression (\cref{sec:codegen}).
% The input loop expression is shown on the left and the output, expanded, loop expression is shown on the right.
% \pycode{kernel} has argument intents of \pycode{READ} and \pycode{INC} for its first and second argument respectively and so the transformed expression contains \pycode{write} and \pycode{inc} instructions where appropriate.

\begin{center}
  \includegraphics{codegen_example2_expr_transform.pdf}
\end{center}

% LOWERING/COMPILATION
% first the loop is visited, loop over 5, gives us $L^p_a$.

\begin{listing}
  \centering
  \begin{minipage}{.8\textwidth}
    \inputminted{c}{./scripts/artefacts/codegen_example2_c_code_tidy.c}
  \end{minipage}
  \caption{
    TODO
  }
  \label{listing:codegen_example2_c_code}
\end{listing}

% For our demo the generated C code can be seen in \cref{lst:codegen_example_c_code}.
% Unsurprisingly it looks very similar to the input loopy kernel (\cref{lst:codegen_example_loopy_kernel}).

\end{example}

\subsection{Optimisation}

% inspector-executor model? probably best to put in chapter 5 where it is relevant
% inspector-executor model. cite Saltz and Strout
% two programs, an inspector that generates a schedule, and an executor that uses it. Executor
% is a transformed original program.
% these aim to improve data locality and parallelisation opportunities.
% important point is that I/E strategies utilise runtime information to generate optimal schedules
% this is very important for unstructured applications where the compilers would have a really hard time!
\cite{stroutSparsePolyhedralFramework2018} % review article
\cite{mirchandaneyPrinciplesRuntimeSupport1988} % old (general purpose) example
\cite{arenazInspectorExecutorAlgorithmIrregular2004} % fem example but specifically parallelisation
% perhaps also cite Luporini for sparse tiling? yes I think that would be good.
% Interesting note: composing inspector-executor transformations is difficult.
% see "The Sparse Polyhedral Framework: Composing Compiler-Generated Inspector-Executor Code"
% DSLs like loopy and pyop3 can make this easier to handle.
% mesh numbering is an example of an inspector-executor thing.
% so is determining core and owned to overlap communication and computation
% DSLs help a lot to implement this sort of thing because transformations can be a lot easier to
% express using a high-level representation.

\section{Data structures}
\label{sec:data_structures}

Thus far we have only considered
Currently \pycode{Dats} use \numpy arrays as the underlying data storage mechanism, but we intend to permit further array types to enable targeting accelerator architectures like CUDA GPUs (see \cref{sec:future_work}).

As well as \pycode{Dats}, \pyop3 has \pycode{Globals}, for global scalar values, and \pycode{Mats}, for matrices.
% globals are really simple
% indexed with 0, 1, or 2 index trees, row, col axis trees

% table of differences? # axis trees, memory rep, possible dtypes?
% think it's overkill. just needs a sentence

\subsection{Matrices}
\label{sec:impl_matrices}

% initialisation...
% uses PETSc, support for different formats (block, nest, CUDA, ...) - restricted dtype

\pycode{Mats} require 2 axis trees: one for the rows of the matrix and one for the columns.
They rely on PETSc \ccode{Mat} objects for the underlying data storage.

\subsubsection{Indexing}

% NOTE: This sort of thing needs to be clearer - we index axis trees, so they have two...
Since \pycode{Mats} have two axis trees, two indices are needed when indexing.

% and it also returns a temporary that can be manipulated? (adds row and col labels...)

\subsubsection{Code generation}

Since \pycode{Mats} are not internally represented by a flat buffer they must be accessed through PETSc's C API.
For example, to read values from a PETSc \ccode{Mat} the following function must be used:

\begin{cinline}
  MatGetValues(Mat mat,
               PetscInt m, PetscInt idxm[],
               PetscInt n, PetscInt idxn[],
               PetscScalar v[])
\end{cinline}

This function returns a dense block of values (\ccode{v[]}) read from matrix \ccode{mat} at the row and column indices specified by arrays \ccode{idxm[]} and \ccode{idxn[]} respectively.
Analogous routines exist for writing and incrementing.
% this is naturally less efficient than direct insertion into an array, can be a bottleneck

For \pyop3 to be able to interface with PETSc matrices calls to these getter and setter routines have to be emitted.

One challenge presented by this API is that it is no longer possible for \pyop3 to represent offsets purely as a symbolic quantity because PETSc expects these to be packed into the index arrays \ccode{idxm} and \ccode{idxn}.
To fix this \pyop3 performs an additional step where these arrays are explicitly tabulated by evaluating the layout functions. % show? speed up?

% NOTE: This is a specific instance of a general "compression" operation (future work)

\end{document}
