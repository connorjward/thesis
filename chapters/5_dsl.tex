\documentclass[thesis]{subfiles}

\begin{document}

\chapter{\pyop3}
\label{chapter:pyop3}

The contributions of this thesis may be split into two parts:
(a) the development of novel \emph{abstractions} for mesh stencils, and
(b) the \emph{implementation} of these abstractions as part of a working piece of software.
Whereas the previous two chapters have focussed on the former of these, in this chapter we present information pertaining to contribution (b).
Namely, we present the new mesh stencil package \pyop3.

\section{The execution model}

The interface to \pyop3 may be thought of as being two different parts: the \emph{data model} and the \emph{execution model}.
The former of these has already been introduced, being the subject of the last two chapters: data structures, axis trees, and index trees.
Here we present details of the latter.

In \pyop3, the execution model refers to the elements of the language used to express actual computations.
It has been designed to resemble the pseudocode for mesh stencil algorithms (e.g. \cref{alg:fem_assembly}) as closely as possible within the constraints of an embedded DSL.
Its core components therefore are \emph{loops}, \emph{assignments}, and \emph{function calls}.

In order to use \pyop3, users create symbolic \emph{loop expressions} from these core components, matching the algorithm they wish to apply.
Then, behind the scenes, runtime code generation is used to lower the symbolic expression down to executable code.

\begin{figure}
  \centering
  \includegraphics[width=\textwidth]{codegen_flowchart.pdf}
  \caption{
    The code generation pipeline for the compilation of a loop expression into a callable function.
    The input (`Loop expression') and output (`Compiled function') are shown in blue whilst the intermediate processes are red.
    The dashed line from input expression to output function is included to represent the fact that the compiled function requires additional data from the loop expression in order to be executable.
  }
  \label{fig:codegen_flowchart}
\end{figure}

This code generation pipeline is shown in \cref{fig:codegen_flowchart}.
Loop expressions are lowered first to loopy (\cref{sec:pyop2_codegen}) and then to C, before calling a traditional compiler to build the compiled library.
To execute the computation, the compiled function is then called using appropriate data from the input loop expression.

Both \pyop3 loop expressions and loopy kernels are \emph{transformable representations}, so, as the code is compiled, these layers are able to reason about the program and transform it to alternative but equivalent forms.
This is typically done for optimisation purposes but \pyop3 also uses the technique more functionally to facilitate compilation.
The former of these is discussed in \cref{sec:codegen_optimisation} whilst the latter is explained in the forthcoming examples.

\subsection{Example 1: Constant assignment}
\label{sec:example1_assign}

As a first example of the code generation process in \pyop3, we consider the following assignment operation:
\begin{pyinline}
  axes = AxisTree.from_iterable([Axis(5), Axis(3)])
  dat = Dat(axes)
  dat[::2, 1:].assign(666)
\end{pyinline}
Here, we assign the scalar value \pycode{666} to entries in the indexed linear array \pycode{dat[::2, 1:]}.
Since a newly initialised \pycode{Dat} starts with a memory buffer of all zeros, following this operation the entries in \pycode{dat} will be
\begin{equation}
  \pycode{[0, 666, 666, 0, 0, 0, 0, 666, 666, 0, 0, 0, 0, 666, 666]}.
\end{equation}

If we assume the axes of the indexed array to be labelled $c$ and $d$, mapping from $a$ and $b$ respectively, then the indexed array has the following structure:
\begin{center}
  \includegraphics{codegen_example1_axis_tree.pdf}
\end{center}
which has
\begin{equation}
  \textnormal{offset}(i_c, i_d) = 6 i_c + i_d + 1
\end{equation}
as the substituted layout function (\cref{chapter:indexing}).

\subsubsection{Code generation}

When the \pycode{assign()} method is called, the following single-node loop expression is produced:
\begin{center}
  \includegraphics{codegen_example1_expr.pdf}
\end{center}
To generate code for this expression, the expression tree is descended and appropriate loopy constructs - domains, instructions, and arguments - are generated.
These are then used to construct a complete loopy \pycode{LoopKernel}.
Given that the example only has a single node the traversal of the loop expression is trivial and only the \ccode{write} node is visited.

\begin{listing}
  \centering
  \begin{minipage}{.9\textwidth}
    \inputminted[linenos]{text}{./scripts/artefacts/codegen_example1_loopy_kernel_tidy.txt}
  \end{minipage}
  \caption{
    Abbreviated textual representation of the loopy kernel generated for the example expression in \cref{sec:example1_assign}.
  }
  \label{listing:codegen_example1_loopy_kernel}
\end{listing}

We are now in a position to generate code:
\begin{enumerate}
  \item
    First, loops are generated for the axes of the \emph{indexed} array, and the mapping from axis label to loop index name is recorded.
    In this case, two loops with extents 3 and 2 and index names \pycode{"i_0"} and \pycode{"i_1"} are created, and the stored mapping is thus $\{ c \to \pycode{"i_0"},\ d \to \pycode{"i_1"}\}$.

    These loops are written in loopy as
    \begin{pyinline}
      "{ [i_0]: 0 <= i_0 <= 2 }"
      "{ [i_1]: 0 <= i_1 <= 1 }"
    \end{pyinline}
    The labels \pycode{"i_0"} and \pycode{"i_1"} are referred to by loopy as \emph{inames}.

  \item
    Then, the right assignment instruction is generated with the appropriate substituted layout function, replacing axis indices as appropriate.
    For this example, recalling that the substituted layout is $6 i_c + i_d + 1$, this produces the instruction:
    \begin{pyinline}
      dat[6*i_0 + i_1 + 1] = 666
    \end{pyinline}
\end{enumerate}

\begin{listing}
  \caption{
    The C code generated from the loopy kernel in \cref{listing:codegen_example1_loopy_kernel}.
  }
  \centering
  \begin{minipage}{.9\textwidth}
    \inputminted[linenos]{c}{./scripts/artefacts/codegen_example1_c_code_tidy.c}
  \end{minipage}
  \label{listing:codegen_example1_c_code}
\end{listing}

The loopy kernel generated for this expression is shown in \cref{listing:codegen_example1_loopy_kernel}, and the subsequently generated C code in \cref{listing:codegen_example1_c_code}.
Note that \pycode{dat} has been passed through as an argument in the generated code but that it has been given a new name: \pycode{array_0}.
This is because kernel arguments are renamed during code generation to avoid unnecessary cache misses.

\subsection{Example 2: Outer loops and maps}
\label{sec:example2_outer_loops}

We now consider a more complex example that more closely approximates a mesh stencil calculation:
\begin{pyinline}
  axis_a = Axis(5, "a")
  axis_b = Axis(3, "b")
  dat0 = Dat(AxisTree.from_iterable([axis_a, axis_b]))
  dat1 = Dat(axis_a)
  map0 = Map({"a": MapComponent("a", arity=2, ...)})

  loop(
    ia := axis_a.index(),
    kernel(dat0[map0(ia), :], dat1[ia]),
  )
\end{pyinline}
Here we loop over the entries in \pycode{axis_a} and apply local kernel \pycode{kernel}, a \pyop3 \pycode{Function} whose implementation is externally provided as a loopy kernel, to indexed arguments \pycode{dat0} and \pycode{dat1}.

\pycode{dat0} is a two-dimensional array with shape \pycode{(5, 3)} and axis labels $a$ and $b$, whereas \pycode{dat1} is one-dimensional with shape \pycode{(5,)}, having only axis $a$ in its axis tree.
\pycode{dat0} is indexed with an arity 2 tabulated map and a slice, giving it an indexed axis tree like:
\begin{center}
  \includegraphics{codegen_example2_axis_tree.pdf}
\end{center}
and substituted layout
\begin{equation}
  \textnormal{offset}(L^p_a,i_b, i_c) = 3 \pycode{[...][?$L^p_a$?,?$i_c$?]} + i_b.
  \label{eq:indexed_layout}
\end{equation}
where \pycode{[...][?$L^p_a$?,?$i_c$?]} is the lookup expression for the tabulated map.
\pycode{dat1} is fully indexed by the loop index and so contains just a single entry.
It simply has the substituted layout
\begin{equation}
  \textnormal{offset}(L^p_a) = L^p_a.
\end{equation}


\begin{table}
  \centering

  \begin{tblr}{|[1pt]c|[1pt]l|[1pt]l|[1pt]}
    \hline[1pt]
    \textbf{Intent} & \textbf{Pack instruction} & \textbf{Unpack instruction} \\
    \hline[1pt]
    \pycode{READ} & \ccode{write(temporary, indexed)} & \tableDash \\
    \hline[1pt]
    \pycode{WRITE} & \ccode{write(temporary, 0)}  & \ccode{write(indexed, temporary)} \\
    \hline[1pt]
    \pycode{RW} & \ccode{write(temporary, indexed)} & \ccode{write(indexed, temporary)} \\
    \hline[1pt]
    \pycode{INC} & \ccode{write(temporary, 0)} & \ccode{inc(indexed, temporary)} \\
    % \hline
    % \pycode{MIN_WRITE} & \tableDash & \ccode{min(indexed, temporary)} \\
    % \hline
    % \pycode{MIN_INC} & \ccode{write(temporary, 0)} & \ccode{min(indexed, temporary)} \\
    % \hline
    % \pycode{MAX_WRITE} & \tableDash & \ccode{max(indexed, temporary)} \\
    % \hline
    % \pycode{MAX_INC} & \ccode{write(temporary, 0)} & \ccode{max(indexed, temporary)} \\
    \hline[1pt]
  \end{tblr}

  \caption{
    Intent values supported by \pyop3 kernels and their corresponding pack/unpack instructions.
    In the instructions, the variable `\ccode{indexed}' is used to represent the indexed view of some piece of global data (e.g. \pycode{dat0[map0(p)]}) and the variable `\ccode{temporary}' is the temporary buffer for storing the materialised data.
    Table entries marked with a `\pycode{-}' indicate that no pack/unpack instruction is emitted for this intent.
  }
  \label{tab:intents}
\end{table}

\subsubsection{Kernel intents}

For this example the local kernel is created with intents \pycode{READ} and \pycode{INC} for its two arguments.
These intents serve as descriptors explaining the manner in which the argument is accessed.
Possible values are: \pycode{READ} (read-only), \pycode{WRITE} (write-only), \pycode{RW} (read and write), and \pycode{INC} (increment).

By providing intents, \pyop3 is able to optimise the generated code to reduce the amount of memory that need be accessed.
For instance, read-only accesses only need to pack global data for the kernel, unpacking the values back again is redundant.

The pack/unpack instructions required for each intent are shown in \cref{tab:intents}.
Only two distinct operations are required: \pycode{write} and \pycode{inc}, where \pycode{write(x, y)} represents the operation $\pycode{x} \gets \pycode{y}$, and \pycode{inc(x, y)} means $\pycode{x} \gets \pycode{x} + \pycode{y}$.

If we inspect the input loop expression from the listing, we see that these pack/unpack operations are implicit:
\begin{center}
  \includegraphics{codegen_example2_expr_before.pdf}
\end{center}
Since the local kernel expects dense local arrays to be passed to it, rather than symbolically indexed arrays, we \emph{transform} the loop expression to make the pack/unpack operations explicit.
This gives:
\begin{center}
  \includegraphics{codegen_example2_expr_after.pdf}
\end{center}
Note that the temporaries \ccode{t0} and \ccode{t1} are the same shape as their indexed equivalents but are unindexed; they do not carry any indexing information and do not have substituted layouts.
For \ccode{t0} this means that it has an axis tree identical to the indexed expression \pycode{dat0[map0(ia), :]}:
\begin{center}
  \includegraphics{codegen_example2_axis_tree.pdf}
\end{center}
But with the simpler layout function:
\begin{equation}
  \textnormal{offset}(i_c, i_b) = 3 i_c + i_b.
  \label{eq:temporary_layout}
\end{equation}

\subsubsection{Code generation}

Having transformed the loop expression we are now able to lower it to a loopy kernel.
As before, we traverse the loop expression and process each node in turn to collate the necessary loopy constructs:
\begin{itemize}
  \item
    When we visit the \ccode{loop} node we generate the single loop over axis $a$:
    \begin{pyinline}
      \pycode{"{ [i_0]: 0 <= i_0 < 5 }"}
    \end{pyinline}
    The iname \pycode{i_0} is arbitrary.
    In addition to this loop, we also produce the mapping $\{ L^p_a \to \pycode{"i_0"} \}$, which is propagated through to other nodes in the loop expression tree.
  \item
    The \ccode{write} and \ccode{inc} nodes are handled in the same way as \cref{sec:example1_assign}.
    Inames for the outer loop indices (here $L^p_a$) are substituted into the layout functions in the same way as the inames originating from the axes themselves.
  \item
    The \ccode{kernel} node is trivial, simply emitting a call to the function passing in the arguments \ccode{t0} and \ccode{t1}.
\end{itemize}

\begin{listing}
  \centering
  \begin{minipage}{.9\textwidth}
    \inputminted[linenos]{c}{./scripts/artefacts/codegen_example2_c_code_tidy.c}
  \end{minipage}
  \caption{
    C code generated for the loop expression used in \cref{sec:example2_outer_loops}.
    :.
  }
  \label{listing:codegen_example2_c_code}
\end{listing}

For this example, the generated C code can be seen in \cref{listing:codegen_example2_c_code}.
Observe that when reading \pycode{dat0} (line 12) the substituted layout from \cref{eq:indexed_layout} has been used for the right hand side, turning into the expression `\ccode{array_1[2 * i_0 + i_1] * 3 + i_2}', whereas the assignee, \ccode{t0}, uses the unindexed layout (\cref{eq:temporary_layout}) which turns into `\ccode{i_1 * 3 + i_2}'.
As a tabulated map is required to execute the loop expression, the generated code expects an integer array \ccode{array_1} in addition to the input and output arrays for \pycode{dat0} and \pycode{dat1} (\ccode{array_0} and \ccode{array_2}).

\section{Matrices}
\label{sec:impl_matrices}

\pyop3 uses \pycode{Mat} objects to represent matrices.
Unlike \pycode{Globals}, with no axis trees, and \pycode{Dats}, with one, \pycode{Mats} are constructed with two axis trees, one for the rows of the matrix and one for the columns.
As an example, to construct a \pycode{Mat} coupling a vector-valued space to a scalar one one might run:
\begin{pyinline}
  vector_axes = AxisTree.from_iterable([Axis(5, "a"),
                                        Axis(3, "b")])
  scalar_axes = AxisTree(Axis(5, "a"))
  mat = Mat(vector_axes, scalar_axes)
\end{pyinline}

Further, \pycode{Mats} do not use \numpy{} arrays internally to store data and instead use PETSc matrices (also called \ccode{Mats}).
Unlike vectors, which are almost always represented as some variant of a flat array, matrices typically use a wide range of different storage techniques to account for properties like sparsity, symmetry, block structure, distributed in memory, and more.
PETSc has wide support for a variety of matrix types, and exposes them via a generic interface, making adding support for them to \pyop3 relatively straightforward.
As an additional benefit, using PETSc matrices means that \pyop3 is compatible with PETSc's wide range of solvers and preconditioners.

\subsection{Indexing}

In order to index a \pycode{Mat} two index trees are needed, one for each of the axis trees.
These compose together in the normal way to give two indexed axis trees, that are stored in a new \pycode{Mat} representing the indexed object:
\begin{pyinline}
  row_index_tree = IndexTree(...)
  column_index_tree = IndexTree(...)
  indexed_mat = mat[row_index_tree, column_index_tree]
\end{pyinline}

\subsection{Precomputing sparsity patterns}

As we know from \cref{sec:stokes_equations}, mesh stencil calculations produce \emph{sparse} matrices, where the majority of entries are zeroes.
To avoid redundant storage and computation, special sparse matrix formats exist that only store the non-zero values.
For such formats to be performant, it is sometimes advantageous for the locations of the non-zeros - its \emph{sparsity pattern} - be known in advance, because inserting new values would otherwise trigger expensive memory allocation.

To avoid this cost, \pyop3 can \emph{preallocate} sparse matrices by precomputing the sparsity pattern and using this when building the \pycode{Mat}.
The process of doing this is as follows:
\begin{enumerate}
  \item
    First, one creates a \pycode{Sparsity} object, which is simply a \pyop3 \pycode{Mat} where the underlying PETSc matrix has type \ccode{MATPREALLOCATOR}:
    \begin{pyinline}
      sparsity = Sparsity(row_axis_tree, column_axis_tree)
    \end{pyinline}
    Preallocator matrices in PETSc are not capable of storing any values, but they provide an efficient mechanism for tracking where entries are inserted.
  \item
    Then, any parallel loops that would insert values in the real matrix should be simulated, passing in \pycode{sparsity} instead.
    For example:
    \begin{pyinline}
      loop(p := iterset.index(), dummy_kernel(sparsity[f(p), f(p)]))
    \end{pyinline}
    Note that the local kernel, here \pycode{dummy_kernel}, should only set placeholder values in \pycode{sparsity}.
    There is no reason to perform any computation because the inserted values are discarded.
  \item
    Lastly, a `regular' \pycode{Mat} can be constructed from the sparsity:
    \begin{pyinline}
      mat = Mat.from_sparsity(sparsity)
    \end{pyinline}
    This matrix will use the sparsity pattern from \pycode{sparsity} so will not trigger any memory allocation when it is later used.
\end{enumerate}

\subsection{Code generation}
\label{sec:impl_matrices_codegen}

\begin{listing}
  \caption{
    Example loop expression involving a \pycode{Mat}.
  }
  \centering
  \begin{minipage}{.9\textwidth}
    \begin{pyalg2}
      # build the matrix, passing in 2 axis trees
      axis_a, axis_b = Axis(5, "a"), Axis(3, "b")
      axes = AxisTree.from_iterable([axis_a, axis_b])
      mat = Mat(axes, axes)?\label{code:mat_init}?

      # array to hold the result
      axis_x = Axis(10, "x")
      dat = Dat(axis_x)?\label{code:mat_dat_init}?

      # map from "x" to "a" with arity 3
      f = Map({"x": [MapComponent("a", arity=3, ...)]})

      # make the loop expression, the kernel arguments
      # have intents READ and INC
      loop(ix := axis_x.index(),?\label{code:mat_loop_expr_init}?
           kernel(mat[(f(ix),:), (f(ix),:)], dat[ix]))
    \end{pyalg2}
  \end{minipage}
  \label{listing:mat_packing}
\end{listing}

Whilst \pycode{Mats} may be treated the same as \pycode{Globals} and \pycode{Dats} in many circumstances, additional complications arise during code generation.
As an illustrative example consider \cref{listing:mat_packing}.
In it, we execute a loop expression (line~\ref{code:mat_loop_expr_init}) that reads values from a $15 \times 15$ matrix (\pycode{mat}, line~\ref{code:mat_init}) and increments the result into a vector (\pycode{dat}, line~\ref{code:mat_dat_init}).
The two axis trees of \pycode{mat} are identical and are indexed with the same map, slice combination such that the index transformations are:
\begin{center}
  \includegraphics{index_map_data_layout.pdf}
\end{center}
The indexed matrix therefore has size $9 \times 9$ as both the row and column axis trees contain 9 entries.

If we begin to generate code for the loop expression, after expanding implicit pack/unpack instructions we have the loop expression:
\begin{center}
  \includegraphics[width=\textwidth]{mat_loop_expr.pdf}
\end{center}
Generating code from these nodes follows an identical process to that described in \cref{sec:example1_assign,sec:example2_outer_loops} with one exception: the matrix packing operation `\ccode{write(t0, mat[(f(ix),:), (f(ix),:)])}'.
Since the data are stored in a PETSc \ccode{Mat} instead of a \numpy{} array, its values must be accessed using the PETSc C API, instead of directly addressing offsets in a memory buffer.

In order to read from and write to a PETSc \ccode{Mat}, the functions \ccode{MatGetValues()} and \ccode{MatSetValues()} must be used.
They have near-identical function signatures, with that for \ccode{MatGetValues()} being:
\begin{cinline}
  MatGetValues(Mat mat,
               PetscInt m, PetscInt idxm[],
               PetscInt n, PetscInt idxn[],
               PetscScalar v[])
\end{cinline}
\ccode{mat} is the PETSc matrix object and \ccode{v[]} the dense array containing the read values.
The arrays \ccode{idxm[]} and \ccode{idxn[]} specify the row and column indices whose values are to be read (with \ccode{m} and \ccode{n} their respective extents).

We remark that the data type of \ccode{v[]} is \ccode{PetscScalar}; unlike other data structures in \pyop3, matrices are only able to store values with whichever floating point type PETSc was configured with.

To generate code for the \ccode{write} node above we therefore need to emit a function call like:
\begin{cinline}
  MatGetValues(mat, 9, map0[?$L^p_x$?*9], 9, map0[?$L^p_x$?*9], t0)
\end{cinline}
Where \ccode{map0} is an array relating entries in the outer loop (i.e. $L^p_x$) to the appropriate 9 entries in \ccode{mat}.
This is not as straightforward as it seems because \ccode{map0} \emph{does not exist}.
The relation between entries in the outer loop and entries in the \ccode{mat} is instead represented by the \emph{symbolic} layout function
\begin{equation}
  \textnormal{offset}(L^p_x,i_c,i_b) = 3 f(L^p_x,i_c) + i_b,
\end{equation}
which cannot be passed as an argument to \ccode{MatGetValues()}.

As a result, prior to generating code for matrix insertion, \pyop3 has to pre-tabulate the layout function to produce \ccode{map0}:
\begin{pyinline}
  for ?$Lp$? in range(...):
    for ?$i_c$? in range(...):
      for ?$i_b$? in range(...):
        map0[?$L^p$?,?$i_c$?,?$i_b$?] = ?$3 f(L^p_x,i_c) + i_b$?
\end{pyinline}
From which point code generation may proceed.

Note that \pyop2 does not need to do this as the `compressed' maps are always passed in as input.

\section{Future performance optimisations}
\label{sec:codegen_optimisation}

In the same way that loopy provides a convenient abstraction level for expressing loop-level optimisations, \pyop3 provides a good level of abstraction for reasoning about higher-level optimisations.
Notably, by having access to both the symbolics of the computation \emph{and} the input data, inspector-executor optimisations (\cref{sec:intro_software_codegen}) become expressible.

None of the listed optimisations have yet been implemented, but they are included in this thesis as examples of the potential that the abstraction has.

\subsection{Expression compression}
\label{sec:pyop3_expression_compression}

In \cref{sec:impl_matrices_codegen} we saw that we needed to `compress' the layout functions for matrices into a flat map between the iteration set and the row/column DoFs.
Generalising this operation to other settings constitutes a performance optimisation.

Consider, for example, a \pycode{Dat} indexed with the composed map $g(f(p))$ with $p$ some outer loop and $f$ and $g$ maps tabulated with arrays \pycode{map_f} and \pycode{map_g} respectively.
With these, the indexed \pycode{Dat} would have the substituted layout function
\begin{equation}
  \textnormal{offset}(L^p,i_f,i_g) = \pycode{map_g[map_f[?$L^p$?,?$i_f$?],?$i_g$?]}.
\end{equation}
Which, if we further stipulate $g$ to have arity 1, simplifies to
\begin{equation}
  \textnormal{offset}(L^p,i_f) = \pycode{map_g[map_f[?$L^p$?,?$i_f$?]]}.
\end{equation}

Since these maps are available at compile-time, \pyop3 can pack the results into a new array, \pycode{map_fg}:
\begin{pyinline}
  for ?$Lp$? in range(...):
    for ?$i_f$? in range(...):
      map_fg[?$L^p$?, ?$i_f$?] = map_g[map_f[?$L^p$?,?$i_f$?]]
\end{pyinline}
Giving us the new substituted layout
\begin{equation}
  \textnormal{offset}(L^p,i_f) = \pycode{map_fg[?$L^p$?,?$i_f$?]}.
\end{equation}

Compared with the uncompressed version, using this new layout is advantageous for two reasons:
\begin{description}
  \item[Better streaming memory accesses]
    In the original expression, \pycode{map_g} is accessed indirectly, incurring performance slowdowns due to an increased rate of hardware cache misses.
    By combining the maps together we ensure that entries are read in a contiguous fashion, avoiding this problem.
  \item[Less memory is used]
    Over the course of a calculation, all entries in \pycode{map_fg} will be streamed through memory.
    Recalling that $g$ has arity 1, if the outer loop has extent $n$ and $f$ has arity $a$ then $n\cdot a$ entries will be loaded.
    Comparing this with the uncompressed expression we can immediately see that more memory is required.
    \pycode{map_f} has the same size as \pycode{map_fg} and so the memory needed for the calculation is $n\cdot a$ \emph{plus} however much memory is read from \pycode{map_g}.
\end{description}

Optimising access patterns like this is similar to the approach taken by~\cite{dasSlicingAnalysisIndirect1994}.

Note that this optimisation does not need to be implemented in \pyop2 because the `compressed' maps are provided at parallel loop construction.
It therefore provides a way to reproduce existing \pyop2 performance.

\subsection{Runtime data layout transformations}

As described in \cref{sec:intro_data_layout_flex}, it is desirable for mesh stencil codes to have support for expressing the same data in a variety of layouts, so as to take advantage of the performance characteristics of different hardware.
In sections~\ref{sec:axis_tree_alternative_layouts} and~\ref{sec:indexing_data_layout_transformations} we established that this is entirely possible within \pyop3.
However, we have assumed thus far that any data layout is immutable for the duration of a program.
This is invalid since there are circumstances where one data layout may be optimal for one loop and another layout be optimal for another, for example if one is executed on a GPU and the other on the CPU.
At present \pyop3 requires that the user commit to one layout or the other, potentially sacrificing performance.

As an alternative to this, \pyop3 \pycode{Dats} could internally store a number of arrays, each with a different data layout.
The optimal layout for different loops could then be used, transparently to the user.
To ensure data consistency between the arrays, the \pycode{Dat} would need to employ state-tracking to lazily copy between them, similarly to how packages ensure consistency between data on a CPU and GPU (e.g.~\cite{millsPerformancePortablePETScGPUbased2020}).

Despite the differing layouts, axis trees make performing this copy trivial.
For example, given two arrays, \pycode{dat_soa} and \pycode{dat_aos}, with axis trees representing different-but-equivalent data layouts, copying one to the other is as simple as executing:
\begin{pyinline}
  dat_soa.assign(dat_aos)
\end{pyinline}

\section{Outlook}

TODO.

\end{document}
