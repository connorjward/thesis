\documentclass[thesis]{subfiles}

\begin{document}

\chapter{Parallel}
\label{chapter:parallel}

By now we have presented enough information about \pyop3 to be able to deploy it to compute mesh stencil calculations in a serial setting.
To extend its functionality to support distributed-memory environments, a number of changes to the described infrastructure are required.
Specifically, \pyop3 now needs to consider both how parallel data are arranged (\cref{sec:parallel_data_layouts}), and how ghost data are exchanged (\cref{sec:parallel_star_forests,sec:communication_optimisations}).

Note that most of the following exposition is specific to parallel arrays (i.e. \pycode{Dats} and \pycode{Globals}).
\pycode{Mats}, by using PETSc matrices as their underlying storage mechanism, are parallel by default and so require little intervention by \pyop3.

\section{Parallel data layouts}
\label{sec:parallel_data_layouts}

As described in \cref{sec:intro_parallelism,sec:dmplex_parallel}, the usual way to distribute a stencil computation between multiple processors is to \emph{partition} the mesh and distribute the DoFs of the different vectors such that DoFs associated with `owned' mesh entities are stored on that process.
Since mesh stencils typically require DoFs from adjacent entities in order to be computable, a small number of \emph{ghost} DoFs must also be kept in the local array.

Whilst essential for the stencil computation, ghost points can be inconvenient for other operations occurring during a simulation.
Using the finite element method as an example, having assembled the linear system, ghost DoFs need to be discarded so as not to appear in the global vector/matrix during solving.
To avoid copying data between ghost-inclusive and ghost-free representations of an array, \pyop3 follows the example of \pyop2 (\cref{sec:pyop2_interleave}) and reorders the DoFs so that ghost entries are always stored after owned ones.
This allows one to transform between the different representations without needing to copy data - the ghost-free version is simply a truncated slice of the full, ghost-inclusive array.

In order to represent distributed parallel data layouts using axis trees, axis components are passed an additional \pycode{StarForest} object - simply a thin wrapper around a PETSc star forest (\cref{sec:dmplex_parallel}) - that encodes the point-to-point equivalence between processes:
\begin{pyinline}
  star_forest = StarForest(...)
  parallel_component = AxisComponent(size, sf=star_forest)
  parallel_axis = Axis(parallel_component)
\end{pyinline}
The ghost points are then stored at the end of the resulting data layout, giving a layout like the following:
\begin{center}
  \includegraphics{parallel_partition_mesh_axis.pdf}
\end{center}
where ghost points are denoted by the hatching pattern.

\subsection{Parallel layout functions}

Whilst straightforward for linear axis trees, partitioning ghost points to the end of the array becomes more difficult in a multi-component setting.
As an example, consider the ghost/non-ghost partitioning for the nested axis tree:
\begin{center}
  \includegraphics{parallel_partition_mixed.pdf}
\end{center}
The natural concatenation of axis components we have seen until now results in an interleaving of owned and ghost points:
\begin{center}
  \includegraphics{parallel_partition_mixed_flat_bad.pdf}
\end{center}
Whereas the desired layout should instead look like:
\begin{center}
  \includegraphics{parallel_partition_mixed_flat.pdf}
\end{center}

The need for the latter layout suggests some limitations in our existing layout tabulation algorithms (\cref{sec:axis_tree_layouts}).
First, the layout functions for $V_h$ and $Q_h$ both now have discontinuous jumps in the location of their points and so can no longer be expressed as affine functions of the form $\textnormal{\it index} \times \textnormal{\it step} + \textnormal{\it start}$, and must instead tabulate offsets into an array in a similar manner to the approach used for ragged axis trees.
Further, tabulating layouts for the ghost portions of $V_h$ and $Q_h$ now requires that \emph{the layouts for both their owned portions be tabulated first}.
This is because the offset for the ghost section of $V_h$ depends on the size of the owned section of $Q_h$, and vice versa for $Q_h$.
This differs from the existing approach where different axis components may be visited independently.

To address these new conditions, we introduce the concept of \emph{axis component regions}.
Instead of giving axis components a single size attribute, we instead stipulate that they have a collection of $(\textnormal{\it label}, \textnormal{\it size})$ 2-tuples representing the different classes of point.
For the owned/ghost point classification this means that distributed axis components would represent their extents with the regions:
\begin{pyinline}
  [("owned", num_owned), ("ghost", num_ghost)]
\end{pyinline}

\begin{algorithm}
  \caption{
    Algorithm for computing the layout functions of an axis tree with multiple regions.
  }
  \begin{center}
    \begin{minipage}{.9\textwidth}
      \begin{pyalg2}
        def collect_layouts(axis_tree: AxisTree):
          layouts = {}

          # counter to track the 'shift' between owned and ghost points
          offset = 0?\label{code:region_offset}?
          # loop over owned and then ghost points
          for region in axis_tree.regions:?\label{code:region_loop}?
            # call the layout collection function from ?\cref{sec:axis_tree_layouts}?, but pass
            # in an extra offset counter and existing layout functions
            size = collect_layouts_inner(axis_tree.root, layouts,
                                         region, offset)
            offset += size
          return layouts
      \end{pyalg2}
    \end{minipage}
  \end{center}
  \label{alg:collect_layouts_parallel}
\end{algorithm}

With the introduction of regions, we now modify the layout tabulation algorithms of \cref{sec:axis_tree_layouts} to account for them.
Shown in \cref{alg:collect_layouts_parallel}, the core differences are:
\begin{itemize}
  \item
    There is now an additional outer loop over the different regions (line~\ref{code:region_loop}).
  \item
    To account for offset contributions from other axis components a further \pycode{offset} variable is used (line~\ref{code:region_offset}).
    It indicates the initial offset from which offset tabulation should start.
\end{itemize}

Axis regions are a generic concept and so can be applied to other settings where a DoF partitioning is desirable.
One example of where this occurs is with Dirichlet boundary conditions, where values on the boundary are constrained to a particular value.
Like ghost DoFs, it can be desirable to store these values in the array but still be able to discard them when solving the assembled system, and hence the constrained DoFs should also be collected towards the end of the array~\cite{rothwellImplementingRestrictedFunction2024}.
This can be expressed in \pyop3 by adding additional regions to the axis components:
\begin{pyinline}
  [("owned", num_owned),
   ("constrained_owned", num_constrained_owned),
   ("ghost", num_ghost)
   ("constrained_ghost", num_constrained_ghost)]
\end{pyinline}

\subsubsection{Comparison with \pyop2}

For the multi-field data layout example shown above, \pyop2 would not store the fields in a contiguous array, but instead store each field separately.
This results in a data layout that looks like:
\begin{center}
  \includegraphics{parallel_partition_mixed_flat_pyop2.pdf}
\end{center}
Constructing a monolithic global vector from this layout therefore requires copying the owned portions of each array into contiguous storage, which can be expensive on certain hardware.
\pyop3, by using regions to partition the ghost data to the end of the array regardless of the field structure, is able to avoid this copy.

\subsection{DoF star forests}
\label{sec:parallel_star_forests}

% \begin{algorithm}
%   \caption{
%     TODO
%   }
%   \begin{center}
%     \begin{minipage}{.9\textwidth}
%       \begin{pyalg2}
%         def create_dof_sfs(axis: Axis, ...):
%           dof_sfs = []
%           for component in axis.components:
%             if component.sf:
%               # create a DoF SF by composing the point SF and section
%               dof_sf = create_dof_sf_inner(axis, component, ...)
%               dof_sfs.append(dof_sf)
%
%             # else create a star forest per point
%             elif has_subaxis(axis, component):
%               subaxis = get_subaxis(axis, component)
%
%               # loop over all entries in the axis and recurse
%               for pt in range(component.count):
%                 sub_dof_sfs = create_dof_sfs(subaxis, ...)
%                 dof_sfs.extend(sub_dof_sfs)
%
%           return dof_sfs
%       \end{pyalg2}
%     \end{minipage}
%   \end{center}
%   \label{alg:compose_star_forests}
% \end{algorithm}

As described in \cref{sec:dmplex_parallel}, in order to truly represent parallel data layouts the point-to-point star forests of the axis components must be transformed and combined to form a single DoF-to-DoF star forest that maps the actual unknown values between processes for the distributed arrays.
This DoF star forest is generated in two steps:
\begin{enumerate}
  \item
    DoF star forests are generated for each parallel axis component from their point star forests and stride information.
  \item
    These per-axis DoF star forests are then combined to form the final DoF star forest for the entire axis tree.
\end{enumerate}

\section{Communication optimisations}
\label{sec:communication_optimisations}

With the DoF star forest, \pyop3 \pycode{Dats} can now send data correctly between shared points.
Since parallel communication is an expensive operation, we now present optimisations done in \pyop3 to minimise their impact whilst retaining data validity.

\subsection{Minimising communication}
\label{sec:parallel_minimising_communication}

In order to make sure that the data held by each \pycode{Dat} are valid, two attributes are stored:
\begin{description}
  \item[\pycode{leaves_valid}]
    A boolean value indicating whether or not ghost DoFs (leaves of the DoF star forest) are up-to-date.
    Updating them requires a broadcast from root to leaves.

  \item[\pycode{pending_reduction}]
    A \pyop3 access descriptor indicating the appropriate reduction operation that is needed for owned DoFs (roots of the DoF star forest) to be valid.
    For instance, if ghost values were just incremented into then \pycode{pending_reduction} would be \pycode{INC}, meaning that a summation reduction is needed for root values to be valid.
    If \pycode{pending_reduction} is \pycode{None} then root values are considered up-to-date.
\end{description}

The reason that these attributes are tracked by \pyop3 is that they enable communication to occur lazily, meaning that under certain circumstances the exchanges may be skipped entirely.
Examples include:
\begin{itemize}
  \item If the array is being read from and all values are already up-to-date, no exchange is necessary.
  \item If ghost values are not being read then they need not be made valid.
  \item If the array is being written to, pending communications can be discarded since the data are being overwritten.
  \item If the array is being repeatedly incremented into, the summation reduction need not be executed between operations because the reductions commute.
\end{itemize}

This approach is similar to that of \pyop2 with the exception of \pycode{pending_reduction}, which is a novel contribution.
Instead \pyop2 performs leaf-to-root reductions eagerly, which has been found to be a bottleneck in certain finite element simulations with multiple assembly loops executed in sequence.

\subsection{Overlapping communication with computation}
\label{sec:parallel_overlap_computation_communication}

\begin{algorithm}
  \caption{The \pyop3 parallel loop execution algorithm to interleave computation and communication.}
  \begin{algorithmic}[1]
    \State \Call{ReduceLeavesToRootsBegin}{} \Comment{Begin updating \rootiter{} points}

    \For{\textit{item} \textbf{in} \textit{iterset.core}} \Comment{Compute \coreiter{} points}
      \State \Call{Compute}{\textit{item}}
    \EndFor

    \State \Call{ReduceLeavesToRootsEnd}{} \Comment{Finish updating \rootiter{} points}
    \State \Call{BroadcastRootsToLeavesBegin}{} \Comment{Start updating \leafiter{} points}

    \For{\textit{item} \textbf{in} \textit{iterset.root}} \Comment{Compute \rootiter{} points}
      \State \Call{Compute}{\textit{item}}
    \EndFor

    \State \Call{BroadcastRootsToLeavesEnd}{} \Comment{Finish updating \leafiter{} points}

    \For{\textit{item} \textbf{in} \textit{iterset.leaf}} \Comment{Compute \leafiter{} points}
      \State \Call{Compute}{\textit{item}}
    \EndFor
  \end{algorithmic}
  \label{alg:pyop3_comp_comm_overlap}
\end{algorithm}

Similarly to \pyop2, \pyop3 parallel loops overlap communication and computation to minimise the impact of network latency on performance.
Since reductions are not done eagerly in \pyop3 the transfer algorithm (\cref{alg:pyop3_comp_comm_overlap}) includes an additional step: before one can update ghost values via a broadcast from roots to leaves, the owned values themselves must be updated with a reduction.

\begin{figure}
  \centering
  \includegraphics{split_mesh_partition.pdf}
  \caption{
    \coreiter{} (\textbf{C}), \rootiter{} (\textbf{R}), and \leafiter{} (\textbf{L}) labels for a cell-wise stencil computation with the stencil shown by the shaded region.
    Owned DoFs are filled in and ghost DoFs are hollow.
    Entries that exist in the DoF star forest (those that are duplicated across processes) are shown as diamonds.
  }
  \label{fig:split_mesh_partition}
\end{figure}

To perform this overlap we first partition the iteration set into 3 disjoint subsets:
\begin{description}
  \item[\coreiter{}]
    Entities that can be evaluated without waiting for any parallel exchanges to complete because all of the data necessary for the computation is already correctly stored locally.

  \item[\rootiter{}]
    Entities that use data from roots of the DoF star forest.
    Computing at these points therefore requires that the root data are (potentially) updated via a reduction.

  \item[\leafiter{}]
    Entities that access data from the leaves of the DoF star forest.
    Executing computations at these points requires that both reductions and broadcasts have been completed beforehand.
\end{description}

An example iteration set partitioning is shown in \cref{fig:split_mesh_partition}, with \coreiter{}, \rootiter{}, and \leafiter{} points marked as \textbf{C}, \textbf{R}, and \textbf{L} respectively.
It can be seen that points in the iteration set (the cells) are labelled as \leafiter{} if their stencil includes a ghost point (hollow diamond), and \rootiter{} if it includes a shared but owned point (filled diamond).
All other points do not require any shared data and hence are marked \coreiter{}.

From the figure it is clear to see that the partitioning of the iteration set is a \emph{stencil-dependent process}.
Larger stencils would result in more points being classified as \rootiter{} and \leafiter{}.
\pyop3 therefore addresses the stated limitation of \pyop2 (\cref{sec:pyop2_parallel_limitation}) where the iteration set partition is decided at array initialisation, leading to incorrect partitioning for different stencil sizes and parallel decompositions.

\subsubsection{Determining \coreiter{}, \rootiter{}, and \leafiter{}}

\begin{algorithm}
  \caption{
    Algorithm to partition an iteration set into \coreiter{}, \rootiter{}, and \leafiter{}.
  }
  \begin{algorithmic}[1]
    \Require \textit{iteration set}
    \Require \textit{arrays} \Comment{Arrays used in the computation}
    \Require \textit{labels} \Comment{Label for each iterate, defaults to \coreiter{}}

    \For{\textit{point} \textbf{in} \textit{iteration set}}
      \For{\textit{array} \textbf{in} \textit{arrays}} \Comment{Check the stencil of each array}
        \If {\textit{labels}[\textit{point}] = \leafiter{}} \Comment{Skip if already \leafiter{}}
          \State \textbf{continue}
        \EndIf

        \For{\textit{stencil point} \textbf{in} \Call{CollectStencil}{\textit{array}, \textit{point}}}
          \If {\textit{stencil point} \textbf{in} \textit{array.star forest}}
            \State \textit{star forest label} $\gets$ \Call{GetStarForestLabel}{\textit{stencil point}}
            \If {\textit{star forest label} = \leafiter{}}
              \State \textit{labels}[\textit{point}] $\gets$ \leafiter{}
              \State \textbf{break} \Comment{Most restrictive case, stop further checks}
            \Else
              \State \textit{labels}[\textit{point}] $\gets$ \rootiter{}
            \EndIf
          \EndIf
        \EndFor
      \EndFor
    \EndFor
  \end{algorithmic}
  \label{alg:partition_iterset}
\end{algorithm}

The algorithm to partition the iteration set into \coreiter{}, \rootiter{}, and \leafiter{} subsets is straightforward.
Shown in \cref{alg:partition_iterset}, one simulates the stencil operation and, for each array, inspects the points touched at each iterate (line 6) to see whether they are a shared point (i.e. whether they are present in the DoF star forest).
If so then the iterate is marked as \rootiter{} or \leafiter{} as appropriate.
\leafiter{}, as the most restrictive condition, takes precendence over \rootiter{}.
The algorithm is very similar to the approach taken by \pyop2 except the loop over arrays (line 2) is omitted, and the function \textsc{CollectStencil} is hard-coded as the point's closure (\cref{sec:dmplex_queries}), meaning that other stencil sizes are not supported.

\section{Outlook}

In this chapter we have presented the modifications necessary to extend the heretofore serial-only version of \pyop3 to work in a distributed memory parallel environment.
For this to work, additional care must be taken to ensure that data are stored correctly on a single process, and that they are correctly mapped between processes.
Further consideration is also given to minimise the impact of any communication that may take place.

Matrices were not discussed because their parallel implementation is entirely provided by PETSc.

Now having a parallel implementation, all of the fundamental features of \pyop3 have been established and we may now consider integrating it into existing PDE-solving frameworks.
This is the subject of the next chapter.

\end{document}
