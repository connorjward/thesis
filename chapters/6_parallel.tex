\documentclass[thesis]{subfiles}

\begin{document}

\chapter{Parallelism}
\label{chapter:parallel}

Just like Firedrake (e.g.~\cite{betteridgeCodeGenerationProductive2021}) and PETSc (e.g.~???), \pyop3 is designed to be run efficiently on even the world's largest supercomputers.
Accordingly, \pyop3 is designed to work SPMD with MPI/distributed memory.

\section{Parallel data layouts}

% don't need to worry about PETSc matrices

\subsection{Partitioning ghost points}

Whilst it is essential for ghost points to be present for finite element assembly, it is equally essential for the inverse to be true for solving the assembled problem.
Ghost points are copies of owned points held on other processes so when considering the \textit{global} vector they should be discarded.
Since FEM simulations often involve alternating assembly with solving and copying, and copying arrays can be expensive, \pyop3 reorders the data such that ghost unknowns are always stored after owned ones.
This allows one to reuse the array between assembly and solving - solving just uses a truncated view.

% this can also occur when one has constrained DoFs (i.e. non-physical) (e.g. boundary conditions) and these also want to be dropped from the global vector

% show an example for a mixed system, the parallel bits are handled at the root level
% compared to pyop2, mixed things are done truly at the end - building global vector used to involve copying (bottleneck on GPU)


% to solve this we introduce the concept of AxisComponents having "regions" (tabulation algorithm) - talk about layout functions (must be tabulated)
% all DoFs for the first region precede latter regions
% extra region DoFs may be discarded simply by taking a slice

\subsection{Star forests}
% NOTE: ref sec:dmplex_parallel to talk about star forests

% parallel axis trees have an associated DoF star forest that manages sends/recvs/updating things
% this is constructed with an algorithm (show)

% for parallel, axis components also have an SF... how does this feed into the algorithm above?

\section{Communication optimisations}

\subsection{Overlapping communication with computation}

In order to hide the often expensive latencies associated with halo exchanges, \pyop3 uses non-blocking MPI operations to interleave computation and communication.
Since distributed meshes only need to communicate data at their boundary, and given the surface-area-to-volume ratio effect, the bulk of the required computation can happen without using any halo data.
The algorithm for overlapping computation and communication therefore looks like this:

% but this is more complicated - also see chapter 2...
\begin{enumerate}
  \item Initiate non-blocking halo exchanges.
  \item Compute results for data that does not rely on the completion of these halo exchanges.
  \item Block until the halo exchanges are complete.
  \item Compute results for data that requires up-to-date halo data.
\end{enumerate}

This interleaving approach is used in \pyop2 and has been reimplemented, with slight improvements, in \pyop3.

% compared with pyop2 we have taken a slightly different approach...
\pyop2 is able to track leaf validity, but does not have a transparent solution for commuting reductions.

% NOTE: reference sec:pyop2_parallel. I mention that two concepts are conflated: data partitioning and iteration set partitioning.
% in pyop3 we split into owned+ghost (data) and core+noncore (iterset)

% "BUT...
% (express as a limitation, rather than a mistake.)
\begin{itemize}
  \item
    \textbf{The iteration is conflated with the data layout}\\
    ...
    For instance, it is not possible to perform parallel loops using larger stencils because the \textit{core}-\textit{owned} split is different.
    Embedding the iteration set into the data layout means that the underlying sets of the data structures are invalid.

  \item
    \textbf{The parallel decomposition is assumed to be the same for all data structures}\\
    It is not always the case that all data structures will be defined on the same mesh (e.g. mesh transfer operations).
    In such a case the algorithms used to determine \textit{core} and \textit{owned} points no longer work because the algorithm does not take into account the parallel decomposition of the other mesh.
    \textit{Core} points on one mesh may be \textit{ghost} in another, and hence the \textit{core} points ought to be labelled \textit{owned}.
\end{itemize}

These limitations suggest a new approach: the partitioning of data and the partitioning of the iteration set should be \textit{distinct processes}.
In \pyop3 we adopt the following new terminology:


Loop expression iteration sets are split into \textit{core} and \textit{non-core} sets.
This partitioning is established by running over the iteration set and checking the access pattern for all arguments.
If any of the arguments require halo data then the iteration point is classified as \textit{non-core}.
All remaining points are then classified \textit{core}.
%Rather than being contiguous, the indices are split into two ordered and interleaved subsets. % easy to implement!
As ghost points are not iterated over they are not included in the subsets.

\subsection{Lazy communication}

Coupled with the principle of ``don't wait for data you don't need", \pyop3 also obeys of ``don't send data if you don't have to".

% something about "redundant messages"

\pyop3 associates with each parallel data structure two attributes: \pycode{leaves_valid} and \pycode{pending_reduction}.
The former tracks whether or not leaves (ghost points) contain up-to-date values.
The latter tracks, in a manner of speaking, the validity of the roots of the star forest.
If the leaves of the forest were modified, \pycode{pending_reduction} stores the reduction operation that needs to be executed for the roots of the star forest to contain correct values.
As an example, were values to be incremented into the leaves\footnote{For this to be valid the leaves need to be zeroed beforehand.}, a \ccode{SUM} reduction would be required for owned values to be synchronised.
If there is no pending reduction, the roots are considered to be valid.

The advantage to having these attributes is that they allow \pyop3 to only perform halo exchanges when absolutely necessary.
Some pertinent cases include:

\begin{itemize}
  \item If the array is being written to \pycode{op3.WRITE}, all prior writes may be discarded.
  \item If the array is being read from (\pycode{op3.READ}) and all values are already up-to-date, no exchange is necessary.
  \item If the array is being incremented into (\pycode{op3.INC}) multiple times in a row, no exchange is needed as the reductions commute.
\end{itemize}


% ------------------------------ OLD =====================================

\begin{figure}
  \begin{subfigure}{\textwidth}
    \includegraphics{iterset_partition_cell}
    \caption{TODO}
    \label{fig:iterset_partition_cell}
  \end{subfigure}
  %
  \begin{subfigure}{\textwidth}
    \includegraphics{iterset_partition_facet}
    \caption{TODO}
    \label{fig:iterset_partition_facet}
  \end{subfigure}
  %
  \caption{TODO}
  \label{fig:iterset_partition}
\end{figure}


% TODO validate that these statements are correct
Although this interleaving approach may seem like the most sensible approach to this problem, it is worthwhile to note that there are subtle performance considerations that affect the effectiveness of the algorithm over a simpler blocking halo exchange approach.
\cite{bisbasAutomatedMPICode2023} showed that, in the (structured) finite difference setting, it is in fact often a better choice to use blocking exchanges because
(a) the background thread running the non-blocking communication occasionally interrupts the stream of execution, and
(b) looping over entries that touch halo data separately adversely affects data locality.
With \pyop3 we have only implemented the non-blocking approach for now, though a comparison with blocking exchanges in the context of an unstructured mesh would be interesting to pursue in future.



\end{document}
