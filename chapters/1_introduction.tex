\documentclass[thesis]{subfiles}

\begin{document}

\chapter{Introduction}
\label{chapter:introduction}

% TODO: don't love this sentence
The numerical solution of \glspl{pde} is ubiquitous across a wide range of applications, from climate modelling to structural mechanics and manufacturing.
In order to solve them numerically, depending on the numerical method chosen, different elements of the geometry and/or function spaces of the physical problem must be \textit{discretised} such that they may be represented on a computer.

% Solving such problems requires enormous amounts of computational power
% Software for solving them is important

\begin{figure}
  \centering
  \includegraphics[scale=1.5]{mesh_stencil.pdf}
  \caption{
    An example of a mesh stencil operation where cells are looped over and adjacent entities are accessed by the stencil (grey).
    DoFs stored on the cells, edges, and vertices are represented by blue, red, and green dots respectively.
  }
  \label{fig:mesh_stencil}
\end{figure}

For a number of numerical methods such as the finite volume, finite difference, and finite element methods, a \textit{mesh stencil} approach is taken.
With such methods the geometry of the problem is discretised to form a \textit{mesh} composed of topological entities termed \textit{cells}, with each having sub-entities such as faces, edges and vertices.
Depending on the method and implementation choices, this mesh may be either \textit{structured}, where adjacent entities may be addressed by applying offsets, or \textit{unstructured}, where an indirection map of the form \ccode{A[B[i]]} is needed.
To these topological entities are associated unknowns, termed \glspl{dof}, representing physical quantities in the PDE like, say, the pressure.

For all of these methods, an integral part of their usage is the application of a stencil over mesh entities.
One loops over topological entities, typically cells or faces, and computes some value using data that is `close' the current iterate (e.g. \cref{fig:mesh_stencil}).

Mesh stencil computations crop up repeatedly in continuum mechanics simulations and developing methods for their application is the foundational concept behind this thesis.

\section{Mesh stencils in practice: the finite element method}
\label{sec:stokes_equations}

As an introductory example to a mesh stencil calculation, we consider solving the Stokes equations, a linearisation of the Navier-Stokes equations in common use in the fluid dynamics community, using the \gls{fem}.
Our exposition will focus on the aspects of the computation that are relevant for mesh stencils, for a more complete review of \gls{fem} we refer the reader to~\cite{brennerMathematicalTheoryFinite2008} and~\cite{larsonFiniteElementMethod2013}.
The Stokes equations are given as follows: for domain $\Omega$ with boundary $\Gamma$, find the fluid velocity ($u$) and pressure ($p$) such that

\begin{subequations}
  \begin{align}
    - \nu \Delta u + \nabla p &= 0 \quad \textrm{in} \ \Omega, \\
    %
    \nabla \cdot u &= 0 \quad \textrm{in} \ \Omega, \\
    %
    u &= g \quad \textrm{on} \ \Gamma,
  \end{align}
  %
  \label{eq:strong_stokes}
\end{subequations}

\todo[inline]{I don't talk about the viscosity at all. It is convenient for later if it is in P3.}

where $\nu$ is the viscosity and $g$ is some prescribed Dirichlet boundary condition fixing the velocity across the entire boundary.
Forcing terms are omitted for simplicity.
As we have a coupled system of two variables ($u$ and $p$), we refer to this system as being \textit{mixed}.

\subsection{Deriving a weak formulation}

\todo[inline]{I need someone to check over my function spaces. In particular when do I use $V_0$ vs $V$?}
\todo[inline]{Once that's done I need some consistent notation: $V_0$ or $\hat V$?}

For the finite element method we seek the solution to the \textit{variational}, or \textit{weak}, formulation of these equations.
These are obtained by multiplying each equation by a suitable \textit{test function} and integrating over the domain.
For \cref{eq:strong_stokes}, using $v$ and $q$ as the test functions and integrating by parts this gives

% taken from 
% https://nbviewer.org/github/firedrakeproject/firedrake/blob/master/docs/notebooks/06-pde-constrained-optimisation.ipynb
% and Larson and Bengzon (pg. 293)
\begin{subequations}
  \begin{align}
    \int \nu \nabla u : \nabla v \, \textrm{d}\Omega
    - \int p \nabla \cdot v \, \textrm{d}\Omega
    - \int \nu (\nabla u \cdot n) \cdot v \, \textrm{d}\Gamma
    - \int p n \cdot v \, \textrm{d}\Gamma
    &= 0
    &\forall v \in \hat V
    \label{eq:weak_stokes_extra_V} \\
    %
    \int q \, \nabla \cdot u \, \textrm{d}\Omega
    &= 0
    &\forall q \in Q.
  \end{align}
\end{subequations}

From these weak forms it is now possible to classify the function spaces for $u$ and $p$.
For $u$, we already know that the space must be vector-valued and constrained to $g$ on the boundary.
\Cref{eq:weak_stokes_extra_V} further shows us that $u$ must have at least one weak derivative.
We can therefore say that $u \in V$ where, for dimension $d$,

\begin{equation}
  V = \{ \ v \in [H^1(\Omega)]^d : v |_{\Gamma} = g \ \}.
  \label{eq:stokes_velocity_space}
\end{equation}

Unlike $u$, $p$ is scalar-valued, needs no derivatives of $p$, and does not have any boundary conditions applied to it, so we can write that $p \in Q$ where

\begin{equation}
  Q = \{ \ q \in L^2(\Omega) \ \}
  \label{eq:stokes_pressure_space}
\end{equation}

\todo[inline]{This is the bit I really don't know how to properly explain.}

Since the values of $u$ at the boundary are already prescribed, the function space of the test function $v$ is defined to be zero at those nodes

\begin{equation}
  \hat V = \{\ v \in [H^1(\Omega)]^d : v|_{\Gamma} = 0 \ \}.
\end{equation}

This allows us to drop some terms from \cref{eq:weak_stokes_extra_V}, allowing us to state the final problem as follows: find $(u, p) \in V \times Q$ such that

\begin{subequations}
  \begin{align}
    \int \nu \nabla u : \nabla v \, \textrm{d}\Omega
    - \int p \nabla \cdot v \, \textrm{d}\Omega
    &= 0
    &\forall v \in \hat V
    \label{eq:weak_stokes_V} \\
    %
    \int q \, \nabla \cdot u \, \textrm{d}\Omega
    &= 0
    &\forall q \in Q.
    \label{eq:weak_stokes_Q}
  \end{align}
  \label{eq:weak_stokes}
\end{subequations}

\subsection{Discretising the system of equations}

In order to solve this weak formulation using the finite element method we discretise the function spaces in use by replacing them with finite dimensional equivalents:

\begin{align*}
  &V \quad \to \quad V_h \subset V, \\
  &\hat V \quad \to \quad \hat V_h \subset \hat V, \\
  &Q \quad \to \quad Q_h \subset Q.
\end{align*}

Each of these discrete spaces is spanned by a set of basis functions, so any function can be expressed as a linear combination of the basis functions and their coefficients.
For example, we can write the function $u_h \in V_h$ as

\begin{equation}
  u_h = \sum^N_{i=1} \hat u_i \psi^{V_h}_i
\end{equation}

for basis functions $\psi^{V_h}_i$ and coefficients $\hat u_i$.

Substituting these discrete spaces back into \cref{eq:weak_stokes}, and discarding the basis coefficients for the arbitrary functions $v_h$ and $q_h$, we obtain the discrete problem: find $(u_h, p_h) \in V_h \times Q_h$ such that

\begin{subequations}
  \begin{align}
    \int \nu u_h \nabla \psi^{V_h} : \nabla \psi^{\hat V_h} \, \textrm{d}\Omega
    - \int p_h \psi^Q \nabla \cdot \psi^{\hat V_h} \, \textrm{d}\Omega
    &= 0
    \quad \forall \psi^{\hat V} \\
    %
    \int \psi^Q \, u_h \nabla \cdot \psi^{V_h} \, \textrm{d}\Omega
    &= 0
    \quad \forall \psi^{Q},
  \end{align}
  \label{eq:weak_stokes_discrete}
\end{subequations}

which can be written as the (saddle point) linear system

\todo[inline]{Do I want $V$ or $\hat V$ in the matrix? I feel like we should want the former...}

\begin{equation}
  \left (
  \begin{array}{c|c}
    \int \nu \nabla \psi^{V_h} : \nabla \psi^{\hat V_h} \, \textrm{d}\Omega
    &
    - \int \psi^Q \nabla \cdot \psi^{\hat V_h} \, \textrm{d}\Omega \\
    \hline
    \int \psi^Q \, \nabla \cdot \psi^{V_h} \, \textrm{d}\Omega
    &
    0
  \end{array}
  \right )
  \left (
  \begin{array}{c}
    u_h \\
    \hline
    p_h
  \end{array}
  \right )
  =
  \left (
  \begin{array}{c}
    0 \\ \hline 0
  \end{array}
  \right )
  %
  \label{eq:stokes_linear_system}
\end{equation}

Solving the Stokes equations using the finite element method therefore boils down to constructing, or \textit{assembling}, the left-hand-side matrix and the - here trivial - right-hand-side vector before solving for the coefficients of $u_h$ and $p_h$.

\subsection{Choosing a basis}

\begin{figure}
  \centering
  \begin{subfigure}{.4\textwidth}
    \centering
    \includegraphics{lagrange_element_2.pdf}
    \vspace{1em}
    \caption{
      The $P_2$ (Lagrange, degree 2) finite element for a triangle.
      Point evaluation DoFs are associated with edges (red) and vertices (green).
    }
    \label{fig:lagrange_element_2}
  \end{subfigure}
  %
  \begin{subfigure}{.58\textwidth}
    \centering
    \begin{tabular}{c c c}
      \includegraphics[width=.3\textwidth]{element_Lagrange_2_dof0.pdf}
      &
      \includegraphics[width=.3\textwidth]{element_Lagrange_2_dof1.pdf}
      &
      \includegraphics[width=.3\textwidth]{element_Lagrange_2_dof2.pdf}
      \\
      \includegraphics[width=.3\textwidth]{element_Lagrange_2_dof3.pdf}
      &
      \includegraphics[width=.3\textwidth]{element_Lagrange_2_dof4.pdf}
      &
      \includegraphics[width=.3\textwidth]{element_Lagrange_2_dof5.pdf}
    \end{tabular}
    %
    \caption{
      The basis functions of a $P_2$ finite element.
      The images were reused (with permission) from DefElement~\cite{defelement}.
    }
    \label{fig:lagrange_element_2_basis}
  \end{subfigure}
\end{figure}

In order to numerically evaluate the integrals in \cref{eq:stokes_linear_system} the basis functions $\psi^{V_h}$ and $\psi^Q$ must be known.
In the finite element method these are generated from the choice of \textit{finite element}.

First formalised by Ciarlet~\parencite{ciarletElement2002}, a finite element is the triple $(K, P, N)$, where:

\begin{itemize}
  \item $K$ is a bounded closed set, or \textit{cell}, with non-empty interior and piecewise smooth boundary,
  \item $P$ is a finite-dimensional space of functions on $K$, and
  \item $N$ is a set of linear functionals that form a basis for the dual space of $P$.
\end{itemize}

A simple example of a finite element, the Lagrange degree 2 element, is shown in \cref{fig:lagrange_element_2}.
For this element $K$ (the cell) is a triangle, $P$ (the function space) is the space of order 2 polynomials, and $N$ (the dual basis) is defined to be point evaluation at each of the nodes.
That is to say, linear functional $l_i$, taking as input some function $v$, is defined to be

\begin{equation*}
  l_i(v) = v(x_i),
\end{equation*}

\noindent
where $x_i$ is the coordinates of the $i$-th node.

Given this definition, one can determine a basis, $\psi_j$, for $P$ by imposing that

\todo[inline]{Not sure how to write forall i, j bit. And $n_k$ currently undefined.}

\begin{equation*}
  l_i(\psi_j) = \delta_{ij} \quad i, j = 0, 1, \dots, n_k,
\end{equation*}

\noindent
which for the $P_2$ element yields the basis functions (\cref{fig:lagrange_element_2_basis}):

% see https://defelement.com/elements/examples/triangle-lagrange-equispaced-2.html
\begin{align}
  &\psi_0 = 2x^2 + 4xy - 3x + 2y^2-3y+1,
  &
  &\psi_3 = 4xy, \\
  %
  &\psi_1 = x(2x-1),
  &
  &\psi_4 = 4y(-x-y+1), \\
  %
  &\psi_2 = y(2y-1),
  &
  &\psi_5 = 4x(-x-y+1).
  \label{eq:basis_functions}
\end{align}

\subsubsection{Mapping to reference space}

\todo[inline]{bit ick, reword}

The basis functions enumerated above are given in \textit{reference space}, that is, relative to the reference finite element.
By contrast, the basis functions of the variational problem exist in \textit{physical space}: cells in the mesh are rarely exactly regular in shape.
% not a lot of detail, revisit
In order to be able to use the reference basis functions one needs to introduce Jacobian-like terms into the integrals termed \textit{pullbacks}.

From \cref{fig:lagrange_element_2} one can see that the linear functionals, and hence the basis functions and DoFs, are associated with different topological entities, and this is emphasised by showing them in different colours.
When the finite element definition above is transformed from \textit{reference space} to \textit{physical space}, DoFs associated with topological entities of the reference cell are associated with equivalent entities on the mesh.
\Cref{fig:mesh_stencil} for example shows a mesh storing DoFs for a $P_3$ finite element.
Just as cell sub-entities (e.g. edges and vertices) are shared between adjacent cells, so are the DoFs associated with those entities.


\subsubsection{Choosing a basis for the Stokes equations}

\begin{figure}
  \centering
  \begin{subfigure}{.35\textwidth}
    \centering
    \includegraphics{lagrange_element_3_vec.pdf}
    \label{fig:scott_vogelius_element_P3}
  \end{subfigure}
  %
  \begin{subfigure}{.35\textwidth}
    \centering
    \includegraphics{lagrange_element_2_dg.pdf}
  \end{subfigure}
  %
  \caption{
    The Scott-Vogelius element, degree 3 ($[P_3]^2 \oplus P_2^\mathrm{disc}$).
    The lower-order space (right) is discontinuous and hence all of the DoFs are marked as belonging to the cell (blue).
  }
  \label{fig:scott_vogelius_element}
\end{figure}

The choice of basis functions used by the function spaces has significant implications for the convergence and stability of the method.
For the two-dimensional Stokes equations, a common choice of element pair, or \textit{mixed} element, with properties matching the constraints given in \cref{eq:stokes_velocity_space} and \cref{eq:stokes_pressure_space} is the Scott-Vogelius element~\cite{scottNormEstimatesMaximal1985}.
The element consists of a continuous vector-valued degree $k$ Lagrange element for the velocity space, and a discontinuous Lagrange element of degree $k-1$.
This is shown in \cref{fig:scott_vogelius_element} for $k = 3$\footnote{The two-dimensional Scott-Vogelius element is technically known to be stable only for $k \geq 4$~\cite{guzmanScottVogeliusFiniteElements2018}, but we show $k = 3$ for simplicity.}.

\subsection{Finite element assembly}

Having established the basis functions and pullbacks of the discrete function spaces we are now able to numerically evaluate, via a quadrature scheme, the integrals of \cref{eq:stokes_linear_system}.
This step is referred to as \textit{local assembly}, and code for its evaluation is termed the \textit{local kernel}.
The final piece required for finite element assembly is an outer loop, where local kernel is evaluated for each basis function in the discrete space.
This process is termed \textit{global assembly}.

If we again consider the Stokes problem above, to assemble the top left block of the matrix in \cref{eq:stokes_linear_system} we see that we need to evaluate

\begin{equation}
  \int \nu \nabla \psi^{V_h}_i : \nabla \psi^{\hat V_h}_j \, \textrm{d}\Omega
  \label{eq:stokes_top_left}
\end{equation}

\noindent
for each possible pair of basis functions $\psi^V_i$ and $\psi^V_j$, suggesting an assembly algorithm with quadratic complexity.
However, this may be improved by observing that finite element basis functions are only non-zero in a small region around their owning mesh entity (e.g. \cref{fig:lagrange_element_2_basis}) - we say that they have \textit{local support}.
This means that most of the integrals are \textit{zero by definition}, and the assembled matrix is \textit{sparse}.

\begin{algorithm}
  \caption{
    Algorithm for assembling a finite element data structure with a single coefficient in the expression.
  }
  %
  \begin{algorithmic}[1]
    \Require \textit{coefficient}, \textit{output} \Comment{Input and output variables}
    \Require \textit{temp0}, \textit{temp1} \Comment{Work arrays}

    \For{\textit{cell} \textbf{in} \textit{mesh.cells}}
      \State \Call{Read}{\textit{coefficient}, \textit{cell}, \textit{temp0}}
      \State \Call{Zero}{\textit{temp1}}
      \State \Call{LocalKernel}{\textit{temp0}, \textit{temp1}} \Comment{Evaluate the integral for each cell}
      \State \Call{Increment}{\textit{output}, \textit{temp1}} \Comment{Scatter the result}
    \EndFor
  \end{algorithmic}
  \label{alg:fem_assembly}
\end{algorithm}

\begin{figure}
  \centering
  \includegraphics[scale=1.4]{fem_assembly.pdf}
  \caption{
    Diagram of the assembly process for \cref{eq:stokes_top_left}.
    For each cell in the mesh the viscosity coefficient is packed into a temporary array before the local kernel is evaluated.
    The result, a dense matrix, is then scattered to the global sparse matrix.
  }
  \label{fig:fem_assembly}
\end{figure}

In order to avoid this quadratic complexity, finite element assembly typically proceeds by iterating over cells and evaluating the local kernel on each cell.
Only basis functions with non-zero support over the cell - DoFs on the cell or its sub-entities - need be considered for the assembly.
The global assembly may therefore be expressed as the sum of cell-wise integrals

\todo[inline]{define $\mathcal{T}$ somewhere above}

\begin{equation}
  \sum_{K \in \mathcal{T}} \int_K \nu \nabla \psi_i^K : \nabla \psi_j^K \, \textrm{d}\Omega,
\end{equation}

\noindent
with $\phi^K$ are the basis functions over the cell.

This process is shown in \cref{fig:fem_assembly}, and an algorithm for its execution is summarised in \cref{alg:fem_assembly}.
From these, it is clear why finite element assembly is referred to as a mesh stencil process.

\subsection{Finite element data structures}

\begin{figure}
  \centering
  \includegraphics{scott_vogelius_element_dof_layout_packed.pdf}
  \caption{
    The packing transformation taking the global data (top) to the packed local data (bottom) for a two-dimensional Scott-Vogelius function space.
    The global data stores all DoFs for the entire function space whilst the packed data only contains the DoFs present in the current stencil iterate.
    \\
    The data layout has been split into multiple levels to emphasise the hierarchical nature of function spaces.
    For instance, every edge in the mesh (red) for the velocity space $[P_3]^2$ ($V_h$) has 2 nodes: $n_0$ and $n_1$, with each node having 2 components: \labelComponent{0} and \labelComponent{1}.
    \\
    As the packed local data only exists as a temporary, its contents are parametrised by the current cell iterate, $c_i$.
    Sub-entities are identified via a relation from this, for instance the first vertex incident on $c_i$ is represented by $f_0^v(c_i)$.
  }
  \label{fig:scott_vogelius_element_dof_layout_packed}
\end{figure}

Before we can write any assembly code we need to formalise what it means to pack and unpack the local data (lines~\ref{code:pack_demo} and~\ref{code:unpack_demo} in \cref{alg:fem_assembly}).
This requires that the \textit{data layouts} of both the global and local data structures be defined.

Despite the fact that the mesh is unstructured, and hence `adjacent' topological entities may be far from it when stored in memory, function spaces have an amount of inherent structure that must be taken into account when determining the right pack/unpack transformations.
A potential structure for a two-dimensional Scott-Vogelius function space is demonstrated in \cref{fig:scott_vogelius_element_dof_layout_packed}.
The top part of the figure represents the data layout for the global data structure and the bottom for the local packed one passed to the local kernel.
The size of the global data layout is given by

\begin{equation*}
  \underbrace{2 ( 1 \times n_\textnormal{cells} + 2 \times n_\textnormal{edges} + 1 \times n_\textnormal{vertices} )}_{V_h} + \underbrace{6 \times n_\textnormal{cells}}_{Q_h},
\end{equation*}

\noindent
and the size of the local layout is

\begin{equation*}
  \underbrace{2 ( 1 \times 1 + 2 \times 3 + 1 \times 3 )}_{V_h} + \underbrace{6 \times 1}_{Q_h} = 26.
\end{equation*}

From \cref{fig:scott_vogelius_element_dof_layout_packed} it is clear that both the global and local data layouts have some inherent structure: DoFs for the different spaces $V_h$ and $Q_h$ are kept apart and nodes and components are stored contiguously.
Representing this hierarchical structure in code is challenging and forms one of the main contributions of this thesis (\cref{chapter:axis_trees,chapter:indexing}).

\section{Execution models for mesh stencil calculations}

% TODO: start with "so this is the algorithm we want to execute, we want to automate its
% application"
% this crops up with subtle variations all over the place, changes to multitude of variables constitute a rewrite - also cannot layer UFL on top?
% BECAUSE PRODUCTIVITY!

% models always need to push what is possible, not just in terms of raw performance, but also cutting-edge numerical methods
% constantly want to push scale - ever increasing demand for computational resources

% developing software that meets this demand is a multidisciplinary effort
% from scratch can take many person years to develop, and often specific to a particular system of equations or application

% also composable (high-level) abstractions - do more with less code
% the use of code generation allows for a separation of concerns


% TODO:: cite dune, deal.ii, Firedrake, FENIcs, devito...?
At this point we have established, excluding the local kernels and global solve, the algorithms and data structures necessary to solve a finite element problem.
Subsequently, we are interested in how to manifest these in software.
Writing these codes by hand is prohibitively difficult: writing a performant and scalable simulation would take months or years of programmer effort and any changes to the \glspl{pde}, discretisation or hardware might constitute a substantial rewrite.
To counter this, numerous frameworks exist providing the building blocks from which a domain specialist, without expertise in high performance computing nor months of programmer time, might build a simulation.
This creates a separation of concerns between the framework maintainers, who specialise in low-level optimisation, and the users, who can instead reason about the problem in terms of the mathematics.

% talk about BLIS, Spiral, FFTW, Halide etc?
In addition to the step-change in programmer productivity, high-level abstractions also facilitate advanced performance optimisations that would be very difficult to implement for a low-level code.
Sometimes, high-level algorithmic changes (discretisation, solver, etc) are required to achieve acceptable performance on a given machine and having a high-level of abstraction means that tweaking these options is minimally invasive \parencite{betteridgeCodeGenerationProductive2021}.
Further, having a high-level representation of the problem enables optimisations best expressed at the level of the mathematics that would otherwise be very challenging to implement (e.g.~\cite{homolyaExposingExploitingStructure2017}).

% inspector-executor model?
% inspector-executor model. cite Saltz and Strout
% two programs, an inspector that generates a schedule, and an executor that uses it. Executor
% is a transformed original program.
% these aim to improve data locality and parallelisation opportunities.
% important point is that I/E strategies utilise runtime information to generate optimal schedules
% this is very important for unstructured applications where the compilers would have a really hard time!
\cite{stroutSparsePolyhedralFramework2018} % review article
\cite{mirchandaneyPrinciplesRuntimeSupport1988} % old (general purpose) example
\cite{arenazInspectorExecutorAlgorithmIrregular2004} % fem example but specifically parallelisation
% perhaps also cite Luporini for sparse tiling? yes I think that would be good.
% Interesting note: composing inspector-executor transformations is difficult.
% see "The Sparse Polyhedral Framework: Composing Compiler-Generated Inspector-Executor Code"
% DSLs like loopy and pyop3 can make this easier to handle.
% mesh numbering is an example of an inspector-executor thing.
% so is determining core and owned to overlap communication and computation
% DSLs help a lot to implement this sort of thing because transformations can be a lot easier to
% express using a high-level representation.

\subsection{Design requirements}

% NOTE: This needs a rewrite, key idea is to enumerate interface choices that impact productivity
% Mention expressivity power?
For the unstructured mesh traversal operation we are interested in, we need an abstraction that:

\begin{itemize}
  \item
    Expresses operations in terms of loops, compact kernels and restricted data structures,
  \item
    Supports the indirection mappings necessary for unstructured meshes (e.g. the map from cells to supported DoFs), and
\end{itemize}


% we want things to be both performant and expressive - here we care about performance really

% assembly can be a bottleneck (when?)
% can be slow because of: data movement, payload or matrix insertion.

\subsubsection{Distributed memory parallelism}
\label{sec:intro_mpi}

% large scale simulations need to be run on massively parallel machines, necessitates dist. memory (MPI)

In massively parallel simulations the data structures are often too large to be stored in the memory of any individual node.
Instead, they are \textit{distributed} between all of the processes, with each process owning, and seeing, only a small piece of the entire structure.

In order to run simulations at scale global data structures are broken apart and each process stores and operates on its own local piece.

% TODO: do not fade some out - just the shared ones.
% NOTE: closure is novel concept...
\begin{figure}
  \centering
  \includegraphics{split_mesh.pdf}
  \caption{
    \pyop2 entity classes for a mesh distributed between 2 processes.
    Points belonging to process 1 (left) are shown in red and points belonging to process 2 (right) are shown in blue.
    \textit{Ghost} points are indicated by points whose colour match the other process, \textit{owned} points are faded and the rest are labelled \textit{core}.
    We assume that one is only computing cell-wise integrals on the mesh and so the mesh overlap need only contain enough ghost points to ensure that all the cells have a complete closure.
  }
  \label{fig:pyop2_split_mesh}
\end{figure}

% TODO: I think it is better to not bother talking about "closed" meshes here. Then can move
% some things back to DMPlex section. Hmm but overlap is important for PyOP2 exposition!
% Can phrase as "local computations may require adjacent DoFs that "belong" on other processes - these are called "ghost" points and care must be taken to ensure that the values are kept up-to-date across processes.

For unstructured meshes and vectors of mesh data it is usual to \textit{partition} the mesh.
Since one needs all DoFs incident on the cell to compute things (need a closed local mesh) boundary values may be duplicated on adjacent processes (\cref{fig:star_forest}).
These are termed \textit{ghost} points.
The amount of overlap required depends on both the stencil and also the amount of redundant computation/frequency of transfers (look into this, Fabio's thesis, Devito?).

The global matrix is also partitioned, usually by row, so each process only sees a portion of it

% show the mesh figure here? not in chapter 2?
It is often the case when solving PDEs that unknowns must be stored on multiple processes simultaneously.
Consider the partitioned mesh shown in \cref{fig:???}.
In order to be able to compute the cell-wise integrals for the finite element method, the cell stencils overlap with each other and the shared overlap region must be stored on both processes.
Since unknowns must have only one owning process, their representation on other ranks are known as ``ghosts".


% Now how to get good performance? reduce amount of communication + overlap computation and communication (non-blocking), avoid expensive comm patterns (like all to one rank)

\subsubsection{Performance portability}

% ie GPUs - they are now in the biggest supercomputers, good energy efficiency, bandwidth, FLOP/s

% potentially very effective for this sort of work - very parallel

% challenging programming model, different programming languages (and vendor specific) (apart from Kokkos etc), kernels need to be launched (can be slow, latency), memory transers between host and device

% OpenMP? hybrid?
% As with Firedrake and PETSc, MPI is chosen as the sole parallel abstraction; hybrid models also using shared memory libraries like OpenMP (cite) are not used because the posited performance advantages are contentious \parencite{knepleyExascaleComputingThreads2015} and would increase the complexity of the code.

\subsubsection{Interoperability with existing software}

% PETSc is used in MFEM, FEniCS, MOOSE, deal.ii? so many, definitely a "standard"

% high level algorithmic changes are essential for performance (JB paper) - want to use top quality solvers

% also, need to reimplement a lot of things best done by existing packages (partitioning, I/O etc)

% certainly hard to do in parallel, sparse matrices etc
% PETSc is great, name check

\subsubsection{Data locality optimisations}
\label{sec:intro_mesh_numbering}

% mesh numbering
% --------------
% cite PyOP2 paper, also perhaps extruded paper

% Numbering can reference the FEM pseudo-code - we need cellwise data to be "close"
% "put closure data close together"

% \subsubsection{Data layout transformations}

\begin{figure}
  \centering
  \includegraphics{scott_vogelius_element_dof_layout_swap.pdf}
  \caption{
    TODO
  }
  \label{fig:scott_vogelius_element_dof_layout_swap}
\end{figure}

% many packages "commit early" to a particular representation, potentially sacrificing performance

% TODO: cite AoS/SoA performance paper or something.

% NOTE: do not talk about OP2/PyOP2 here, mentioned first below

% Lastly, OP2 and \pyop2 both commit eagerly to storing the data in a particular format.
% Depending on the problem at hand, better performance may be achieved by rearranging the way in which DoFs are stored such that data is streamed from memory in the most efficient manner possible.
% An example demonstrating a different data layout for the data shown in \cref{fig:scott_vogelius_element_dof_layout} is shown in \cref{fig:scott_vogelius_element_dof_layout_swap}.
% In the latter the vector component part of the data layout has been ``lifted" such that, for the $V_h$ function space, all \glspl{dof} corresponding with the 0th component are stored before all \glspl{dof} for the 1st component.
% This is equivalent to transforming from an ``array-of-structs" (AoS) layout to a ``struct-of-arrays" (SoA) layout.
% Neither layout is inherently faster than the other and different applications may want to use one over another.
% Both OP2 and \pyop2 only support storing data in the AoS format (\cref{fig:scott_vogelius_element_dof_layout_pyop2}) and so cannot do this.

% TODO: mention taichi in this chapter somewhere as a similar project, maybe also TACO (and SPF?)

% % AoS-SoA etc? (taichi)
% \textbf{Taichi} is a programming language embedded in C\+\+ for operating on complex data structures~\cite{huTaichiLanguageHighperformance2019}.
% Just like \pyop3, Taichi declares data structures hierarchically and the data layout is kept distinct from the operations applied to them.
% Taichi has no concept of a mesh, and it does not work on distributed machines.
% https://www.taichi-lang.org/
% very successful!

% TODO: This should actually be covered in the indexing chapter.
% \section{Renumbering for data locality}
% \label{sec:renumbering}
%
% For memory-bound codes, performance is synonymous with data locality.
% In the case of stencil codes like finite element assembly, one should aim to arrange the data such that the data required for a single stencil calculation is contiguous in memory and can be read from memory into cache with only a single instruction.
%
% For simulations involving unstructured meshes, data reorderings that provide perfect streaming access to memory are not possible and so renumbering strategies have been developed to try and maximise locality.
% For example, the data layout shown in \cref{fig:two_cell_mesh_lagrange_data_layout_flat} approximates the strategy taken by \pyop2, cells are traversed according to some RCM ordering and the cell closures are packed next to the cell~\cite{langeEfficientMeshManagement2016}.
% The is effective for finite element codes because finite element assembly (usually) involves iterating over cells and accessing the data in their closures.
%
% In \pyop3, we choose a simple approach and defer to PETSc to provide us with an appropriate RCM numbering for the points.
% This is communicated to the axis tree by giving an axis, in this case the \pycode{"mesh"} one, a \pycode{numbering} argument.
% This numbering consists of the flat indices of the axis and is exactly the object given to us from PETSc.
% This is not quite the case in parallel (see \cref{chapter:parallel}).

\subsection{Existing software}

% NOTE: A checklist comparing different packages was considered but I think it
% shouldn't be included because there are too many unknowns. I don't know enough
% about Liszt, Simit or Ebb and it makes things a bit more complicated to explain.

A number of packages exist that already meet most or all of these requirements:

\paragraph{Liszt}{
  is a \gls{dsl} embedded in Scala~\cite{devitoLisztDomainSpecific2011}.
  Mesh connectivity is expressed through built-in topological relations and mesh data is associated with specific topological entities.
  Liszt uses a custom mesh implementation with support for parallel partitioning and hence works in a distributed memory environment.
  Liszt is also capable of generating code for use in a multi-threaded or GPU context.
}

\paragraph{Simit}{
  is another \gls{dsl} for mesh simulations~\cite{kjolstadSimitLanguagePhysical2016}.
  It has a unique design where mesh data structures have a dual representation: they can either be viewed as a hypergraph or as a multi-dimensional tensor.
  This enables for both mesh-like queries to be applied to the data structures as well as enabling linear algebra operations to be expressed.
  Simit is capable of targeting both CPUs and GPUs without needing to change the input code, though distributed memory computing is not available.
}

\paragraph{Ebb}{
  is another \gls{dsl} embedded in Lua~\cite{bernsteinEbbDSLPhysical2016}.
  It uses a relational database model to describe the mesh and has a 3-layer infrastructure that separates simulation code from data structure specification and different code generation targets.
  It has support for execution on GPUs but distributed memory computing is not available.
}

% TODO: mention for these two that they are less expressive than the others.
% TODO: How to write "C++"?
\paragraph{OP2}{
  Unlike the frameworks mentioned above, OP2 is an \textit{active library} that provides source-to-source translation from C, C++ or Fortran to target a range of different backends including OpenMP, CUDA and OpenCL~\cite{mudaligeOP2ActiveLibrary2012}.
  OP2 uses a simplified model of a mesh where entities are represented as \textit{sets}.
  One can store data on these sets and mappings exist between sets.
  Computations are termed \textit{kernels} and are provided by the user.
  Distributed memory computing is possible and OP2 is even able to interleave computation and communication to provide improved scaling.
}

\paragraph{\pyop2}{
  is a reimplementation of OP2 in Python~\cite{rathgeberPyOP2HighLevelFramework2012}.
  The same core abstraction of sets, mappings and kernels is used but runtime code generation is used instead of source-to-source translation.
  \pyop2 currently only targets execution on CPUs though a proof-of-concept GPU extension has been created~\cite{fenics2021-kulkarni}.
  Distributed memory computing is supported and \pyop2 can also assemble sparse matrices with PETSc~\cite{petsc-web-page,petsc-user-ref,petsc-efficient}.
}
% Not flexible:
  % "Whilst appropriate for a great many operations, there are occasions where one needs to be able to execute multiple kernels or have nested loops - for example when developing certain types of preconditioners (e.g. \cite{gibsonSlateExtendingFiredrake2020}, \cite{farrellPCPATCHSoftwareTopological2021}).
  % "In these cases one has to extend the compiler in a sui-generis manner to achieve the desired result, resulting in code that is harder to maintain and not composable with other features.

\subsection{Data layout limitations}

\begin{figure}
  \centering
  \includegraphics{scott_vogelius_element_dof_layout_pyop2.pdf}
  \caption{
    The data layout matching \cref{fig:scott_vogelius_element_dof_layout} as it would be stored by \pyop2.
    Data for each function space ($V_h$ and $Q_h$) are stored in separate arrays.
  }
  \label{fig:scott_vogelius_element_dof_layout_pyop2}
\end{figure}

\begin{figure}
  \centering
  \includegraphics{scott_vogelius_element_dof_layout_topological.pdf}
  \caption{
    The closest possible data layout for \cref{fig:scott_vogelius_element_dof_layout} for a library that associates unknowns with topological entities.
    Data for each topological entity are stored in separate arrays.
  }
  \label{fig:scott_vogelius_element_dof_layout_topological}
\end{figure}

% * Liszt, Simit and Ebb are very flexible but they all tie data structures to topological entities. Data layouts
%   like SV element one are not expressible as it requires interleaving/mesh numbering is not possible.
%   * also unnatural to treat function spaces like this.
% * old:
  % "All of the frameworks described above have had to make a tradeoff between expressivity and flexibility (?).
  % Liszt, Simit and Ebb are considerably more flexible than either of OP2 or \pyop2 because they can execute any program written in their respective \glspl{dsl}."

% * OP2/PyOP2 (and Simit and Ebb?) use a set model that does not have this restriction (i.e. node sets) (ref figure)
%   * however this makes the code hard to reason about as info is getting thrown away, also makes things hard to compose.
% * old:
% Further, the treatment of data as belonging to sets can lead to information being discarded.
% Considering the data layout shown in \cref{fig:scott_vogelius_element_dof_layout}, \pyop2 would store it as a tuple of two \textit{node sets} (\cref{fig:scott_vogelius_element_dof_layout_pyop2}).
% Compared with the ``lossless" representation in \cref{fig:scott_vogelius_element_dof_layout}, this approach simplifies the implementation because the data can now be stored as 2 rectangular arrays with shapes \pycode{(10, 2)} and \pycode{(6, 1)}.
% However, this approach \textit{discards topological information}: the ``Mesh" layer of the representation is gone.
% This means that the library user has to do the bookkeeping to correctly handle the mappings from mesh entities to nodes.

\subsection{The missing abstraction}

% This mostly belongs in chapter 3
Clearly, there is something missing here.
The designs of the existing libraries all require that one either use topological information in a simplified way - associating data with particular mesh entities only - or that one take ownership of the data, discarding topological information that is helpful for having a composable abstraction.
To get around this difficulty we have developed a new abstraction for data layouts, termed \textit{axis trees}, that bridges the gap between these worlds.
Axis trees allow the user to describe complex data layouts of the sort shown in \cref{fig:scott_vogelius_element_dof_layout} fully, without needing to discard any of the topological information.
As a convenient side benefit, expressing data layout transformations (\cref{sec:data_layout_transformations}) becomes natural to do.

The axis tree abstraction is included in the new Python library \pyop3.
\pyop3 is a near-total rewrite of \pyop2 that aims to substantially improve its expressivity power and composability.
It has support for distributed memory parallelism and integrates with PETSc.

% NOTE: I don't like including this but I think I have to be honest about this.
% put in a box
Disclaimer:
Whilst all of the functionality of \pyop3 in the following thesis has been tested and shown to work, the time constraints of the PhD mean that \pyop3 is unfinished.
Thus not all of the provided functionality has been merged into the \texttt{main} branch yet.
This is considered very high priority future work for the project.

\section{Thesis outline}

The remainder of this work is structured as follows\dots

\end{document}
