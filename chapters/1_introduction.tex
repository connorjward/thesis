\documentclass[thesis]{subfiles}

\begin{document}

\chapter{Introduction}
\label{chapter:introduction}

% * (numerically) solving PDEs is very important for lots of things:
%   * climate
%   * manufacturing
%   * fluids...
%   * ???

% mesh iteration crops up all over the place

% TODO: delete this, move everything up a step
\section{The problem domain: calculations involving mesh iteration}

\subsection{A motivating example: solving the Stokes equations using the finite element method}
\label{sec:stokes_equations}

As an introductory example to a calculation requiring iterating over a mesh, we consider solving the Stokes equations using the \gls{fem}.
Our exposition will focus on the aspects of the computation that are relevant for \pyop3, for a more complete review of \gls{fem} we refer the reader to~\cite{brennerMathematicalTheoryFinite2008} and~\cite{larsonFiniteElementMethod2013}.

The Stokes equations are a linearisation of the Navier-Stokes equations and are used to describe fluid flow for laminar (slow and calm) media.
For domain $\Omega$ and boundary $\Gamma$, omitting any viscosity or forcing terms for simplicity, they are given by

\begin{subequations}
  \begin{align}
    - \Delta u + \nabla p &= 0 \quad \textrm{in} \ \Omega, \\
    %
    \nabla \cdot u &= 0 \quad \textrm{in} \ \Omega, \\
    %
    u &= g \quad \textrm{on} \ \Gamma.
  \end{align}
  %
  \label{eq:strong_stokes}
\end{subequations}

with $u$ the fluid velocity and $p$ the pressure.
We also prescribe Dirichlet boundary conditions for the velocity across the entire boundary, setting $u$ to the value of function $g$.
Since we have a coupled system of two variables ($u$ and $p$), we refer to the Stokes system as being a \textit{mixed} problem.

\subsubsection{Deriving a weak formulation}

% TODO: Add a forcing term $f$ so the RHS is non-zero so we can happily show the assembly diagram

For the finite element method we seek the solution to the \textit{variational}, or \textit{weak}, formulation of these equations.
These are obtained by multiplying each equation by a suitable \textit{test function} and integrating over the domain.
For \cref{eq:strong_stokes}, using $v$ and $q$ as the test functions, drawn from function spaces $\hat V$ and $Q$ respectively, and integrating by parts this gives

% taken from 
% https://nbviewer.org/github/firedrakeproject/firedrake/blob/master/docs/notebooks/06-pde-constrained-optimisation.ipynb
% and Larson and Bengzon (pg. 293)
\begin{subequations}
  \begin{align}
    \int \nabla u : \nabla v \, \textrm{d}\Omega
    - \int p \nabla \cdot v \, \textrm{d}\Omega
    - \int (\nabla u \cdot n) \cdot v \, \textrm{d}\Gamma
    - \int p n \cdot v \, \textrm{d}\Gamma
    &= 0
    &\forall \ v \in \hat V
    \label{eq:weak_stokes_extra_V} \\
    %
    \int q \, \nabla \cdot u \, \textrm{d}\Omega
    &= 0
    &\forall \ q \in Q.
    \label{eq:weak_stokes_extra_Q}
  \end{align}
  \label{eq:weak_stokes_extra}
\end{subequations}

From these weak forms it is now possible to classify the function spaces for $u$ and $p$.
For $u$, we already know that the space must be vector-valued, since it stores a velocity, and constrained to $g$ on the boundary.
\Cref{eq:weak_stokes_extra_V} further shows us that $u$ must have at least one weak derivative.
We can therefore say that $u \in V$ where

\begin{equation}
  V = \{ \ v \in [H^1(\Omega)]^d : v |_{\Gamma} = g \ \}  \\
  \label{eq:stokes_velocity_space}
\end{equation}

$p$ is scalar-valued, no derivatives of $p$ are present in the weak formulation, nor are any boundary conditions applied to it and so we can write that $p \in Q$ where

\begin{equation}
  Q = \{ \ q \in L^2(\Omega) \ \}  \\
  \label{eq:stokes_pressure_space}
\end{equation}

Since the values of $u$ at the boundary are already prescribed, the function space of the test function $v$ is defined to be zero at those nodes

\begin{equation*}
  \hat V = \{\ v \in [H^1(\Omega)]^d : v|_{\Gamma} = 0 \ \}.
\end{equation*}

This allows us to drop some terms from \cref{eq:weak_stokes_extra_V}, allowing us to state the final problem as follows:

\vspace{1em}

%TODO: Not sure how to format this best
Find $(u, p) \in V \times Q$ such that

\begin{subequations}
  \begin{align}
    \int \nabla u : \nabla v \, \textrm{d}\Omega
    - \int p \nabla \cdot v \, \textrm{d}\Omega
    &= 0
    &\forall \ v \in \hat V
    \label{eq:weak_stokes_V} \\
    %
    \int q \, \nabla \cdot u \, \textrm{d}\Omega
    &= 0
    &\forall \ q \in Q.
    \label{eq:weak_stokes_Q}
  \end{align}
  \label{eq:weak_stokes}
\end{subequations}

\subsubsection{Discretising the system of equations}

In order to solve this weak formulation using the finite element method we discretise the function spaces in use by replacing them with a finite dimensional equivalent:

\begin{equation*}
  V \to V_h \subset V,
  \quad
  \hat V \to \hat V_h \subset \hat V,
  \quad
  Q \to Q_h \subset Q.
\end{equation*}

Each of these discrete spaces is spanned by a set of basis functions so any function can be expressed as a linear combination of the basis functions and their coefficients.
For example, we can write the function $u_h \in V_h$ as

\begin{equation*}
  u_h = \Sigma^N_{i=1} \hat u_i \psi^{V_h}_i
\end{equation*}

for basis functions $\psi^{V_h}_i$ and coefficients $\hat u_i$.

Substituting these discrete function spaces back into \cref{eq:weak_stokes}, and discarding the basis coefficients for the arbitrary functions $v_h$ and $q_h$, we obtain the discrete problem:

\vspace{1em}

%TODO: Not sure how to format this best
Find $(\hat u, \hat p)$ such that

\begin{subequations}
  \begin{align}
    \int \hat u \nabla \psi^{V_h} : \nabla \psi^{\hat V_h} \, \textrm{d}\Omega
    - \int \hat p \psi^Q \nabla \cdot \psi^{\hat V_h} \, \textrm{d}\Omega
    &= 0
    &\forall \ \psi^{\hat V} \\
    %
    \int \psi^Q \, \nabla \cdot \hat u \psi^{V_h} \, \textrm{d}\Omega
    &= 0
    &\forall \ \psi^{Q}
  \end{align}
  \label{eq:weak_stokes_discrete}
\end{subequations}

This can be reformulated as the (saddle point) linear system

\begin{equation}
  \left (
  \begin{array}{c|c}
    \int \nabla \psi^{V_h} : \nabla \psi^{\hat V_h} \, \textrm{d}\Omega
    &
    - \int \psi^Q \nabla \cdot \psi^{\hat V_h} \, \textrm{d}\Omega \\
    \hline
    \int \psi^Q \, \nabla \cdot \psi^{V_h} \, \textrm{d}\Omega
    &
    0
  \end{array}
  \right )
  \left (
  \begin{array}{c}
    \hat u \\
    \hline
    \hat p
  \end{array}
  \right )
  =
  \left (
  \begin{array}{c}
    0 \\ \hline 0
  \end{array}
  \right )
  %
  \label{eq:stokes_linear_system}
\end{equation}

Solving the Stokes equations using the finite element method therefore boils down to constructing, or \textit{assembling}, the left-hand-side matrix and the, here trivial, right-hand-side vector before solving for the coefficients $\hat u$ and $\hat p$.

\subsubsection{The assembly algorithm}

% TODO: I reckon that this would look better if it output a small matrix which was inserted into a larger one - might be overkill though
\begin{figure}
  \centering
  \includegraphics{fem_assembly.pdf}
  \caption{TODO}
  \label{fig:fem_assembly}
\end{figure}

% TODO: Decide on a consistent algorithm format
\begin{algorithm}
  \begin{verbatim}
    FOR EACH cell IN mesh.cells:
      FOR EACH coefficient IN expression:
        collect the coefficients of basis functions that have non-zero support over cell
      compute the integral numerically
      scatter the values of the computed integrals into the global matrix or vector
  \end{verbatim}
  \caption{TODO}
  \label{alg:fem_assembly}
\end{algorithm}

In order to assemble such a system, the integrals must be evaluated numerically for each pair of basis functions in the two function spaces.
In the finite element method this process can be done efficiently because the basis functions are defined to have \textit{local support}, that is, they are defined to be zero across almost the entire domain.
This means that, instead of iterating over all pairs of basis functions, the cells of the mesh may be visited in turn and only the basis functions with non-zero support on that cell are computed with.
These cell-wise contributions are then accumulated to form the global matrix.
Since most of the basis functions have zero overlap the resultant matrix is \textit{sparse}.

\begin{figure}
  \begin{subfigure}{.3\textwidth}
    \includegraphics{lagrange_element_2.pdf}
    \caption{P2, TODO}
    \label{fig:lagrange_element_2}
  \end{subfigure}
  \begin{subfigure}{.68\textwidth}
    \begin{tabular}{c c c}
      \includegraphics[width=.3\textwidth]{element_Lagrange_2_dof0.pdf}
      &
      \includegraphics[width=.3\textwidth]{element_Lagrange_2_dof1.pdf}
      &
      \includegraphics[width=.3\textwidth]{element_Lagrange_2_dof2.pdf}
      \\
      \includegraphics[width=.3\textwidth]{element_Lagrange_2_dof3.pdf}
      &
      \includegraphics[width=.3\textwidth]{element_Lagrange_2_dof4.pdf}
      &
      \includegraphics[width=.3\textwidth]{element_Lagrange_2_dof5.pdf}
    \end{tabular}
    %
    \caption{P2 basis functions, TODO}
    \label{fig:lagrange_element_2_basis}
  \end{subfigure}
  %
  \caption{TODO, \cite{defelement}}
\end{figure}

The basis functions are derived from a finite element definition.
First formalised by Ciarlet~\parencite{ciarletElement2002}, a finite element is the triple $(K, P, N)$, where:

\begin{itemize}
  \item $K$ is a cell in the mesh with non-empty interior and piecewise smooth boundary,
  \item $P$ is a finite-dimensional space of functions on $K$, and
  \item $N$ is a set of linear functionals that form a basis for the dual space of $P$.
\end{itemize}

% TODO: Reword this because the point evaluation stuff is a bit confusingly worded
A simple example of a finite element, the degree 2 Lagrange element, is shown in \cref{fig:lagrange_element_2}.
For this element $K$ (the cell) is a triangle, $P$ (the function space) is the space of order 2 polynomials, and $N$ (the dual basis) is defined to be point evaluation at each of the nodes:

\begin{equation*}
  l_i(v) = v(x_i),
\end{equation*}

where $l_i$ is the linear functional associated with node $i$, $v$ is some function in $P$ and $x_i$ are the coordinates of the $i$-th node.

From these attributes, it is possible to determine a \textit{nodal basis} for $P$ by imposing that

% TODO: define n_k
\begin{equation*}
  l_i(\psi_j) = \delta_{ij} \quad i, j = 0, 1, \dots, n_k.
\end{equation*}

In the case of the degree 2 Lagrange element this yields the basis functions

% see https://defelement.com/elements/examples/triangle-lagrange-equispaced-2.html
\begin{align*}
  &\psi_0 = 2x^2 + 4xy - 3x + 2y^2-3y+1, \\
  &\psi_1 = x(2x-1), \\
  &\psi_2 = y(2y-1), \\
  &\psi_3 = 4xy, \\
  &\psi_4 = 4y(-x-y+1), \\
  &\psi_5 = 4x(-x-y+1),
\end{align*}

shown in \cref{fig:lagrange_element_2_basis}.

\subsubsection{Data structures for the finite element method}

\begin{figure}
  \centering
  %
  \hfill
  %
  \begin{subfigure}{.4\textwidth}
    \includegraphics{lagrange_element_3_vec.pdf}
    \label{fig:scott_vogelius_element_P3}
  \end{subfigure}
  %
  \hfill
  %
  \begin{subfigure}{.4\textwidth}
    \includegraphics{lagrange_element_2_dg.pdf}
  \end{subfigure}
  %
  \hfill
  %
  \caption{Scott-Vogelius element (degree 3?)}
  \label{fig:scott_vogelius_element}
\end{figure}

The choice of basis functions used by the function spaces has significant implications for the convergence and stability of the model.
For the Stokes equations in 2D, a common choice of element pair, or \textit{mixed} element, with properties matching the constraints given in \cref{eq:stokes_velocity_space} and \cref{eq:stokes_pressure_space} is the Scott-Vogelius element~\cite{scottNormEstimatesMaximal1985}.
Shown in \cref{fig:scott_vogelius_element}, the element consists of a continuous vector-valued degree $k$ Lagrange element for the velocity space, and a discontinuous Lagrange element of degree $k-1$.
Note that the Scott-Vogelius element is known to be inf-sup stable for degree $\geq 4$ but we only show degree 3 here for brevity~\cite{guzmanScottVogeliusFiniteElements2018}.

\begin{figure}
  \centering
  \includegraphics{scott_vogelius_element_dof_layout.pdf}
  \caption{TODO}
  \label{fig:scott_vogelius_element_dof_layout}
\end{figure}

With the degree 3 mixed element in \cref{fig:scott_vogelius_element}, for a one-cell mesh one has 26 unknown basis function coefficients, or \textit{degrees of freedom}: 20 for the velocity and 6 for the pressure.
As each basis function yields a single unknown, the size of the linear system in \cref{eq:stokes_linear_system} is $26 \times 26$.

% TODO: reword
In \cref{eq:stokes_linear_system} the different function spaces have been partitioned to produce a block matrix system.
Naturally the choice of how to lay these values out in memory is arbitrary, but a common approach is to split them by function space, then by topological entity, and then by vector component (\cref{fig:scott_vogelius_element_dof_layout})
% This choice of finite element pair means that one would have, for a single element mesh, 26 (?) unknown basis coefficients (DoFs), and the matrix would have 26 rows and 26 columns (and would in fact be dense)

\section{Performance considerations}

% NOTE: Talk about GPU too?

\subsection{Distributed memory parallelism}

% On massively parallel machines we parallelise our computation by partitioning the mesh between processes
% * Basis functions whose support crosses into another process are stored in that processes "halo"
% * Show a simple diagram - continue to use triangles (and basis functions)
% * The global matrix is also partitioned, usually by row, so each process only sees a portion of it

% mention MPI

\subsection{Mesh renumbering}

% cite PyOP2 paper, also perhaps extruded paper

% "put closure data close together"

\subsection{Data layout transformations}

\begin{figure}
  \centering
  \includegraphics{scott_vogelius_element_dof_layout_swap.pdf}
  \caption{
    TODO
  }
  \label{fig:scott_vogelius_element_dof_layout_swap}
\end{figure}

% many packages "commit early" to a particular representation, potentially sacrificing performance

% TODO: cite AoS/SoA performance paper or something.

% Lastly, OP2 and \pyop2 both commit eagerly to storing the data in a particular format.
% Depending on the problem at hand, better performance may be achieved by rearranging the way in which DoFs are stored such that data is streamed from memory in the most efficient manner possible.
% An example demonstrating a different data layout for the data shown in \cref{fig:scott_vogelius_element_dof_layout} is shown in \cref{fig:scott_vogelius_element_dof_layout_swap}.
% In the latter the vector component part of the data layout has been ``lifted" such that, for the $V_h$ function space, all \glspl{dof} corresponding with the 0th component are stored before all \glspl{dof} for the 1st component.
% This is equivalent to transforming from an ``array-of-structs" (AoS) layout to a ``struct-of-arrays" (SoA) layout.
% Neither layout is inherently faster than the other and different applications may want to use one over another.
% Both OP2 and \pyop2 only support storing data in the AoS format (\cref{fig:scott_vogelius_element_dof_layout_pyop2}) and so cannot do this.


\subsection{Interoperability with existing software}

% high level algorithmic changes are essential for performance (JB paper) - want to use top quality solvers

% also, need to reimplement a lot of things best done by existing packages (partitioning, I/O etc)

% certainly hard to do in parallel, sparse matrices etc
% PETSc is great, name check

\section{Execution models for unstructured meshes}

% TODO:: cite dune, deal.ii, Firedrake, FENIcs, devito...?
At this point we have established, excluding the local kernels and global solve, the algorithms and data structures necessary to solve a finite element problem.
Subsequently, we are interested in how to manifest these in software.
Writing these codes by hand is prohibitively difficult: writing a performant and scalable simulation would take months or years of programmer effort and any changes to the \glspl{pde}, discretisation or hardware might constitute a substantial rewrite.
To counter this, numerous frameworks exist providing the building blocks from which a domain specialist, without expertise in high performance computing nor months of programmer time, might build a simulation.
This creates a separation of concerns between the framework maintainers, who specialise in low-level optimisation, and the users, who can instead reason about the problem in terms of the mathematics.

% talk about BLIS, Spiral, FFTW, Halide etc?
In addition to the step-change in programmer productivity, high-level abstractions also facilitate advanced performance optimisations that would be very difficult to implement for a low-level code.
Sometimes, high-level algorithmic changes (discretisation, solver, etc) are required to achieve acceptable performance on a given machine and having a high-level of abstraction means that tweaking these options is minimally invasive \parencite{betteridgeCodeGenerationProductive2021}.
Further, having a high-level representation of the problem enables optimisations best expressed at the level of the mathematics that would otherwise be very challenging to implement (e.g.~\cite{homolyaExposingExploitingStructure2017}).

% inspector-executor model?
% inspector-executor model. cite Saltz and Strout
% two programs, an inspector that generates a schedule, and an executor that uses it. Executor
% is a transformed original program.
% these aim to improve data locality and parallelisation opportunities.
% important point is that I/E strategies utilise runtime information to generate optimal schedules
% this is very important for unstructured applications where the compilers would have a really hard time!
\cite{stroutSparsePolyhedralFramework2018} % review article
\cite{mirchandaneyPrinciplesRuntimeSupport1988} % old (general purpose) example
\cite{arenazInspectorExecutorAlgorithmIrregular2004} % fem example but specifically parallelisation
% perhaps also cite Luporini for sparse tiling? yes I think that would be good.
% Interesting note: composing inspector-executor transformations is difficult.
% see "The Sparse Polyhedral Framework: Composing Compiler-Generated Inspector-Executor Code"
% DSLs like loopy and pyop3 can make this easier to handle.
% mesh numbering is an example of an inspector-executor thing.
% so is determining core and owned to overlap communication and computation
% DSLs help a lot to implement this sort of thing because transformations can be a lot easier to
% express using a high-level representation.

For the unstructured mesh traversal operation we are interested in, we need an abstraction that:

\begin{itemize}
  \item
    Expresses operations in terms of loops, compact kernels and restricted data structures,
  \item
    Supports the indirection mappings necessary for unstructured meshes (e.g. the map from cells to supported DoFs), and
  \item
    Is backend agnostic: the same code should be able to target different architectures with almost no changes.
\end{itemize}

% NOTE: badly written, combine with the above into a single set of requirements
Along with these hard requirements we also have a number of strongly desirable features.
The software should:

\begin{itemize}
  \item
    Be composable: support nested loops, multiple kernels and map composition,

  \item
    Interoperate with external packages (PETSc),

  \item
    Support distributed memory parallelism (i.e. MPI).
\end{itemize}

\subsection{Existing software}

% Checklist comparing the features of the libraries, must include mesh numbering or similar
% include sparse matrices? (not for OP2!)
% include flexible data layouts? this is DIFFERENT to mesh numbering (PyOP2 is not flexible but can do mesh numbering via node sets)
% swap rows and cols?
% \begin{table}
%   \begin{tabular}{|l|l|}
%     \hline
%     % Flexible? Support for numbering?
%     Software & Distributed memory & GPU support \\
%     \hline
%     Liszt \\
%     \hline
%     Simit \\
%     \hline
%     Ebb \\
%     \hline
%     OP2 \\
%     \hline
%     \pyop2 \\
%     \hline
%   \end{tabular}
%
%   \caption{TODO}
%   \label{tab:capabilities}
% \end{table}

A number of packages exist that already meet most or all of these requirements:

\paragraph{Liszt}{
  is a \gls{dsl} embedded in Scala~\cite{devitoLisztDomainSpecific2011}.
  Mesh connectivity is expressed through built-in topological relations and mesh data is associated with specific topological entities.
  Liszt uses a custom mesh implementation with support for parallel partitioning and hence works in a distributed memory environment.
  Liszt is also capable of generating code for use in a multi-threaded or GPU context.
}

\paragraph{Simit}{
  is another \gls{dsl} for mesh simulations~\cite{kjolstadSimitLanguagePhysical2016}.
  It has a unique design where mesh data structures have a dual representation: they can either be viewed as a hypergraph or as a multi-dimensional tensor.
  This enables for both mesh-like queries to be applied to the data structures as well as enabling linear algebra operations to be expressed.
  Simit is capable of targeting both CPUs and GPUs without needing to change the input code, though distributed memory computing is not available.
}

\paragraph{Ebb}{
  is another \gls{dsl} embedded in Lua~\cite{bernsteinEbbDSLPhysical2016}.
  It uses a relational database model to describe the mesh and has a 3-layer infrastructure that separates simulation code from data structure specification and different code generation targets.
  It has support for execution on GPUs but distributed memory computing is not available.
}

% TODO: mention for these two that they are less expressive than the others.
% TODO: How to write "C++"?
\paragraph{OP2}{
  Unlike the frameworks mentioned above, OP2 is an \textit{active library} that provides source-to-source translation from C, C++ or Fortran to target a range of different backends including OpenMP, CUDA and OpenCL~\cite{mudaligeOP2ActiveLibrary2012}.
  OP2 uses a simplified model of a mesh where entities are represented as \textit{sets}.
  One can store data on these sets and mappings exist between sets.
  Computations are termed \textit{kernels} and are provided by the user.
  Distributed memory computing is possible and OP2 is even able to interleave computation and communication to provide improved scaling.
}

\paragraph{\pyop2}{
  is a reimplementation of OP2 in Python~\cite{rathgeberPyOP2HighLevelFramework2012}.
  The same core abstraction of sets, mappings and kernels is used but runtime code generation is used instead of source-to-source translation.
  \pyop2 currently only targets execution on CPUs though a proof-of-concept GPU extension has been created~\cite{fenics2021-kulkarni}.
  Distributed memory computing is supported and \pyop2 can also assemble sparse matrices with PETSc~\cite{petsc-web-page,petsc-user-ref,petsc-efficient}.
}
% Not flexible:
  % "Whilst appropriate for a great many operations, there are occasions where one needs to be able to execute multiple kernels or have nested loops - for example when developing certain types of preconditioners (e.g. \cite{gibsonSlateExtendingFiredrake2020}, \cite{farrellPCPATCHSoftwareTopological2021}).
  % "In these cases one has to extend the compiler in a sui-generis manner to achieve the desired result, resulting in code that is harder to maintain and not composable with other features.

\subsection{Data layout limitations}

\begin{figure}
  \centering
  \includegraphics{scott_vogelius_element_dof_layout_pyop2.pdf}
  \caption{
    The data layout matching \cref{fig:scott_vogelius_element_dof_layout} as it would be stored by \pyop2.
    Data for each function space ($V_h$ and $Q_h$) is stored in a separate array.
  }
  \label{fig:scott_vogelius_element_dof_layout_pyop2}
\end{figure}

% * Liszt, Simit and Ebb are very flexible but they all tie data structures to topological entities. Data layouts
%   like SV element one are not expressible as it requires interleaving/mesh numbering is not possible.
%   * also unnatural to treat function spaces like this.
% * old:
  % "All of the frameworks described above have had to make a tradeoff between expressivity and flexibility (?).
  % Liszt, Simit and Ebb are considerably more flexible than either of OP2 or \pyop2 because they can execute any program written in their respective \glspl{dsl}."

% * OP2/PyOP2 (and Simit and Ebb?) use a set model that does not have this restriction (i.e. node sets) (ref figure)
%   * however this makes the code hard to reason about as info is getting thrown away, also makes things hard to compose.
% * old:
% Further, the treatment of data as belonging to sets can lead to information being discarded.
% Considering the data layout shown in \cref{fig:scott_vogelius_element_dof_layout}, \pyop2 would store it as a tuple of two \textit{node sets} (\cref{fig:scott_vogelius_element_dof_layout_pyop2}).
% Compared with the ``lossless" representation in \cref{fig:scott_vogelius_element_dof_layout}, this approach simplifies the implementation because the data can now be stored as 2 rectangular arrays with shapes \pycode{(10, 2)} and \pycode{(6, 1)}.
% However, this approach \textit{discards topological information}: the ``Mesh" layer of the representation is gone.
% This means that the library user has to do the bookkeeping to correctly handle the mappings from mesh entities to nodes.

\subsection{The missing abstraction}

Clearly, there is something missing here.
The designs of the existing libraries all require that one either use topological information in a simplified way - associating data with particular mesh entities only - or that one take ownership of the data, discarding topological information that is helpful for having a composable abstraction.
To get around this difficulty we have developed a new abstraction for data layouts, termed \textit{axis trees}, that bridges the gap between these worlds.
Axis trees allow the user to describe complex data layouts of the sort shown in \cref{fig:scott_vogelius_element_dof_layout} fully, without needing to discard any of the topological information.
As a convenient side benefit, expressing data layout transformations (\cref{sec:data_layout_transformations}) becomes natural to do.

The axis tree abstraction is included in the new Python library \pyop3.
\pyop3 is a near-total rewrite of \pyop2 that aims to substantially improve its expressivity power and composability.
It has support for distributed memory parallelism and integrates with PETSc.

\section{Thesis outline}

The remainder of this work is structured as follows\dots

\end{document}
