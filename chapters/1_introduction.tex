\documentclass[thesis]{subfiles}

\begin{document}

\chapter{Introduction}
\label{chapter:introduction}

The numerical solution of partial differential equations (PDEs) is ubiquitous across a wide range of applications, from climate modelling to structural mechanics and manufacturing.
In order to solve them numerically, depending on the numerical method chosen, different elements of the geometry and/or function spaces of the physical problem must be \emph{discretised} such that they may be represented on a computer.

\begin{figure}
  \centering
  \includegraphics[scale=1.5]{mesh_stencil.pdf}
  \caption{
    An example of a mesh stencil operation where cells are looped over and adjacent entities are accessed by the stencil (grey).
    DoFs stored on the cells, edges, and vertices are represented by blue, red, and green dots respectively.
  }
  \label{fig:mesh_stencil}
\end{figure}

For a number of numerical methods such as the finite volume, finite difference, and finite element methods, the geometry of the problem is discretised to form a \emph{mesh} composed of topological entities termed cells, with each having sub-entities such as faces, edges and vertices.
Depending on the method and implementation choices, this mesh may be either \textit{structured}, where adjacent entities may be addressed by applying offsets, or \textit{unstructured}, where an indirection map of the form \ccode{A[B[i]]} is needed.
To these topological entities are associated unknowns, termed degrees of freedom (DoFs), representing physical quantities in the PDE such as the pressure.

For all of these methods, an integral part of their usage is the application of a computational kernel, or \emph{stencil}.
To apply one, one loops over a particular collection of mesh entities and, for each entity, computes a contribution to some global data structure using the DoFs that lie in the neighbourhood of the current iterate.
An example of a mesh stencil operation is shown in \cref{fig:mesh_stencil}.

Mesh stencil computations crop up repeatedly in continuum mechanics simulations and developing methods for their application is the central focus of this thesis.

\section{Mesh stencils in practice: the finite element method}
\label{sec:stokes_equations}

As a motivating example for a mesh stencil calculation, we consider solving the Stokes equations, a linearisation of the Navier-Stokes equations in common use in the fluid dynamics community, using the finite element method (FEM).
Our exposition will focus on the aspects of the computation that are relevant for mesh stencils, for a more complete review of FEM we refer the reader to~\cite{brennerMathematicalTheoryFinite2008} and~\cite{larsonFiniteElementMethod2013}.

The Stokes equations are given as follows: for domain $\Omega$ with boundary $\Gamma$, find the fluid velocity $u$ and pressure $p$ such that
\begin{equation} \label{eq:strong_stokes}
  \begin{aligned}
    - \nu \Delta u + \nabla p &= 0 \quad \textrm{in} \ \Omega, \\
    %
    \nabla \cdot u &= 0 \quad \textrm{in} \ \Omega, \\
    %
    u &= g \quad \textrm{on} \ \Gamma,
  \end{aligned}
\end{equation}
where $\nu$ is the viscosity and $g$ is some prescribed Dirichlet boundary condition fixing the velocity across the entire boundary.
Forcing terms are omitted for simplicity.
Since this is a coupled system of two variables ($u$ and $p$), we refer to it as being \emph{mixed}.

\subsection{Deriving a weak formulation}

For the finite element method we seek the solution to the \emph{variational}, or \emph{weak}, formulation of these equations.
These are obtained by multiplying each equation by a suitable \emph{test function} and integrating over the domain.
For \cref{eq:strong_stokes}, using $v$ and $q$ as the test functions for the velocity and pressure spaces respectively, and integrating by parts this gives
% taken from 
% https://nbviewer.org/github/firedrakeproject/firedrake/blob/master/docs/notebooks/06-pde-constrained-optimisation.ipynb
% and Larson and Bengzon (pg. 293)
\begin{equation} \label{eq:weak_stokes_full}
  \begin{aligned}
    \int \nu \nabla u : \nabla v \, \textrm{d}\Omega
    - \int p \nabla \cdot v \, \textrm{d}\Omega
    - \int \nu (\nabla u \cdot n) \cdot v \, \textrm{d}\Gamma
    - \int p n \cdot v \, \textrm{d}\Gamma
    &= 0
    &\forall v
    \\
    \int q \, \nabla \cdot u \, \textrm{d}\Omega
    &= 0
    &\forall q
  \end{aligned}
\end{equation}

\subsubsection{Choosing function spaces}

From this weak form it is now possible to classify the various function spaces for $u$, $p$, $v$ and $q$.
For $u$, we already know that the space must be vector-valued and constrained to $g$ on the boundary.
\Cref{eq:weak_stokes_full} further shows us that $u$ must have at least one weak derivative.
We can therefore say that $u \in V$ where, for dimension $d$,
\begin{equation}
  V = \{ \ v \in [H^1(\Omega)]^d : v |_{\Gamma} = g \ \}.
\end{equation}

The function space of the test function $v$ is similar, but, since the values of $u$ at the boundary are already prescribed, it is defined to be zero at those nodes
\begin{equation} \label{eq:vspace}
  V_0 = \{\ v \in [H^1(\Omega)]^d : v|_{\Gamma} = 0 \ \}.
\end{equation}

Unlike $u$ and $v$, $p$ and $q$ are scalar-valued and need no derivatives.
Further, since no boundary conditions are applied to the space of $p$, we can draw $p$ and $q$ from the same space
\begin{equation}
  Q = \{ \ q \in L^2(\Omega) \ \}.
  \label{eq:stokes_pressure_space}
\end{equation}

Given the definition of these spaces and in particular the fact that $v$ is defined to vanish on the boundary, which allows us to drop the surface integrals, we may restate the problem posed in \cref{eq:weak_stokes_full} as follows: find $(u, p) \in V \times Q$ such that
\begin{equation} \label{eq:weak_stokes_no_surface_terms}
  \begin{aligned}
    \int \nu \nabla u : \nabla v \, \textrm{d}\Omega
    - \int p \nabla \cdot v \, \textrm{d}\Omega
    &= 0
    &\forall v \in V_0 \\
    %
    \int q \, \nabla \cdot u \, \textrm{d}\Omega
    &= 0
    &\forall q \in Q.
  \end{aligned}
\end{equation}

\subsubsection{Resolving boundary conditions}

To better represent the boundary conditions on space $V$, we can split it in two:
\begin{equation*}
  V = V_0 \oplus V_\Gamma,
\end{equation*}
where $V_0$ has the usual definition from \cref{eq:vspace}, and $V_\Gamma$ is defined as the space spanned by the functions of $V$ that are non-zero on the boundary.
With this split we may then rewrite the function $u$ as the sum of the functions $u_0 \in V_0$ and $u_\Gamma \in V_\Gamma$ to give
\begin{equation*}
  u = u_0 + u_\Gamma.
\end{equation*}
$u_\Gamma$ is prescribed by the boundary condition and hence known.
This allows us to move it to the right hand side of \cref{eq:weak_stokes_no_surface_terms} giving: find $(u_0, p) \in V_0 \times Q$ such that
\begin{equation} \label{eq:weak_stokes}
  \begin{aligned}
    \int \nu \nabla u_0 : \nabla v \, \textrm{d}\Omega
    - \int p \nabla \cdot v \, \textrm{d}\Omega
    &=
    - \int \nu \nabla u_\Gamma : \nabla v \, \textrm{d}\Omega
    &\forall v \in V_0 \\
    %
    \int q \, \nabla \cdot u \, \textrm{d}\Omega
    &= 0
    &\forall q \in Q.
  \end{aligned}
\end{equation}

\subsection{Discretising the system of equations}

In order to solve this weak formulation using the finite element method we discretise the function spaces in use by replacing them with finite-dimensional equivalents:
\begin{equation*}
  V_0 \to V_{0,h} \subset V_0,
  \quad
  \quad
  Q \to Q_h \subset Q.
\end{equation*}
Each of these discrete spaces is spanned by a set of basis functions, allowing functions to be expressed as a linear combination of basis functions and coefficients.
For example, we can write the function $u_{0,h} \in V_{0,h}$ as
\begin{equation*}
  u_{0,h} = \sum^N_{i=1} \hat u_i \psi^{V_{0,h}}_i,
\end{equation*}
where $\psi^{V_{0,h}}_i$ are the basis functions that span $V_{0,h}$ and $\hat u_i$ the coefficients.

If we substitute these discrete spaces into \cref{eq:weak_stokes}, and discard the basis coefficients for the arbitrary functions $v_h$ and $q_h$, we obtain the discrete problem: find the vector $(\hat u, \hat p)$ such that
\small
\begin{equation}
  \begin{aligned}
    \int \nu \hat u \nabla \psi^{V_{0,h}} : \nabla \psi^{V_{0,h}} \, \textrm{d}\Omega
    - \int \hat p \psi^{Q_h} \nabla \cdot \psi^{V_{0,h}} \, \textrm{d}\Omega
    &=
    - \int \nu \nabla u_\Gamma : \nabla \psi^{V_{0,h}} \, \textrm{d}\Omega
    \quad
    &\forall \psi^{V_{0,h}}, \\
    %
    \int \psi^{Q_h} \, \hat u \nabla \cdot \psi^{V_{0,h}} \, \textrm{d}\Omega
    &= 0
    \quad
    &\forall \psi^{Q_h}.
  \end{aligned}
\end{equation}
\normalsize
This can be written as the (saddle point) linear system
\small
\begin{equation} \label{eq:stokes_linear_system}
  \left (
  \begin{array}{c|c}
    \int \nu \nabla \psi^{V_{0,h}} : \nabla \psi^{V_{0,h}} \, \textrm{d}\Omega
    &
    - \int \psi^{Q_h} \nabla \cdot \psi^{V_{0,h}} \, \textrm{d}\Omega \\
    \hline
    \int \psi^{Q_h} \, \nabla \cdot \psi^{V_{0,h}} \, \textrm{d}\Omega
    &
    0
  \end{array}
  \right )
  \left (
  \begin{array}{c}
    \hat u \\
    \hline
    \hat p
  \end{array}
  \right )
  =
  \left (
  \begin{array}{c}
    - \int \nu \nabla u_\Gamma : \nabla \psi^{V_{0,h}} \, \textrm{d}\Omega \\
    \hline
    0
  \end{array}
  \right ).
\end{equation}
\normalsize

Solving the Stokes equations using the finite element method therefore boils down to constructing, or \emph{assembling}, the left hand side matrix and right hand side vector, before solving for the coefficients $\hat u$ and $\hat p$.

\subsection{Choosing a basis}

\begin{figure}
  \centering
  \begin{subfigure}[b]{.4\textwidth}
    \centering
    \includegraphics{lagrange_element_2.pdf}
    \caption{
      The dual basis for a $P_2$ element consists of point evaluation at the vertices (green) and edges (red).
    }
    \label{fig:lagrange_element_2_dofs}
  \end{subfigure}
  \begin{subfigure}[b]{.58\textwidth}
    \centering
    \includegraphics[width=.3\textwidth]{element_Lagrange_2_dof0.pdf}\,%
    \includegraphics[width=.3\textwidth]{element_Lagrange_2_dof1.pdf}\,%
    \includegraphics[width=.3\textwidth]{element_Lagrange_2_dof2.pdf}

    \includegraphics[width=.3\textwidth]{element_Lagrange_2_dof3.pdf}\,%
    \includegraphics[width=.3\textwidth]{element_Lagrange_2_dof4.pdf}\,%
    \includegraphics[width=.3\textwidth]{element_Lagrange_2_dof5.pdf}
    \caption{
      The basis functions.
      The images were sourced from \href{https://defelement.com}{defelement.com}~\cite{defelement}.
    }
    \label{fig:lagrange_element_2_basis}
  \end{subfigure}

  \caption{The $P_2$ (Lagrange, polynomial degree 2) finite element.}
  \label{fig:lagrange_element_2}
\end{figure}

In order to numerically evaluate the integrals of \cref{eq:stokes_linear_system}, the basis functions $\psi^{V_{0,h}}$ and $\psi^{Q_h}$ must be known.
In the finite element method these are generated from the choice of \emph{finite element}.

First formalised by Ciarlet~\parencite{ciarletElement2002}, a finite element is the triple $(K, P, N)$, where:
\begin{itemize}
  \item $K$ is a bounded closed set, or \emph{cell}, with non-empty interior and piecewise smooth boundary,
  \item $P$ is a finite-dimensional space of functions on $K$, and
  \item $N$ is a set of linear functionals that form a basis for the dual space of $P$.
\end{itemize}

A simple example of a finite element - the Lagrange element with polynomial degree 2, written $P_2$ - is shown in \cref{fig:lagrange_element_2}.
For this element $K$ is a triangle, $P$ is the space of order 2 polynomials, and the linear functionals of $N$ are defined as the point evaluations
\begin{equation*}
  l_i(v) = v(x_i)
  \quad
  i = 0,1,\dots,5
\end{equation*}
for each of the 6 nodes shown in \cref{fig:lagrange_element_2_dofs}, with $x_i$ being the coordinates of the $i$-th node.

Given this definition, one can determine a basis, $\psi_j$, for $P$ by imposing that
\begin{equation*}
  l_i(\psi_j) = \delta_{ij} \quad i, j = 0, 1, \dots, 5
\end{equation*}
which for the $P_2$ element yields the basis functions:
\begin{align*}
  % see https://defelement.com/elements/examples/triangle-lagrange-equispaced-2.html
  &\psi_0 = 2x^2 + 4xy - 3x + 2y^2-3y+1,
  &
  &\psi_3 = 4xy, \\
  %
  &\psi_1 = x(2x-1),
  &
  &\psi_4 = 4y(-x-y+1), \\
  %
  &\psi_2 = y(2y-1),
  &
  &\psi_5 = 4x(-x-y+1),
\end{align*}
that are shown in \cref{fig:lagrange_element_2_basis}.

Note that the basis functions determined from the finite element have \emph{local support}.
That is, they are only defined to be non-zero in a small region around their associated entities.

\subsubsection{Choosing a basis for the Stokes equations}

\begin{figure}
  \centering
  \begin{subfigure}{.35\textwidth}
    \centering
    \includegraphics{lagrange_element_3_vec.pdf}
    \label{fig:scott_vogelius_element_P3}
  \end{subfigure}
  %
  \begin{subfigure}{.35\textwidth}
    \centering
    \includegraphics{lagrange_element_2_dg.pdf}
  \end{subfigure}
  %
  \caption{
    The degree 3 Scott-Vogelius element ($[P_3]^2 \oplus P_2^\mathrm{disc}$).
    The lower-order space (right) is discontinuous and hence all of the DoFs are marked as belonging to the cell (blue).
  }
  \label{fig:scott_vogelius_element}
\end{figure}

The choice of basis functions used by the function spaces has significant implications for the convergence and stability of the method.
For the two-dimensional Stokes equations, a common choice of element pair, or \emph{mixed element}, with properties matching the constraints given in \cref{eq:vspace} and \cref{eq:stokes_pressure_space} is the Scott-Vogelius element~\cite{scottNormEstimatesMaximal1985}.
The element consists of a continuous vector-valued degree $k$ Lagrange element for the velocity space, written $[P_k]^d$, and a discontinuous Lagrange element of degree $k-1$ for the pressure space, written $P_{k-1}^{\textnormal{disc}}$.
These\footnotemark are shown in \cref{fig:scott_vogelius_element} for $k = 3$.

\footnotetext{In two dimensions the Scott-Vogelius element is technically known to be stable only for $k \geq 4$~\cite{guzmanScottVogeliusFiniteElements2018}, but we show $k = 3$ for simplicity.}

\subsection{Finite element assembly}

The finite element basis functions from the Ciarlet definition above are given in \emph{reference space}.
For them to be usable for assembling the Stokes system of \cref{eq:stokes_linear_system} we must first draw a connection between the domain of the problem, $\Omega$, with the domain of the finite element, $K$.
To do this, we partition $\Omega$ into a set of disjoint cells, $\mathcal{K}$, called a \emph{mesh}, where
\begin{equation*}
  \bigcup \mathcal{K} = \Omega.
\end{equation*}

Owing to the local support property of finite element basis functions, one can now rewrite the integrals in \cref{eq:stokes_linear_system} as a sum of cell-wise contributions.
Using the top left block of the matrix as an example, the integral can be rewritten as
\begin{equation}
  \sum_{\mathcal{K}} \int \nu \nabla \psi_i^{\mathcal{K}} : \nabla \psi_j^{\mathcal{K}} \, \textnormal{d}\Omega_{\mathcal{K}},
\end{equation}
with $\phi^{\mathcal{K}}$ the basis functions over the cell and $i$ and $j$ indicating the different element nodes.
Since the basis functions have only local support the resulting matrix is \emph{sparse}.

Subject to the addition of some Jacobian-like terms, called \emph{pullbacks}, to map between the geometry of the physical and reference cells, these cell-wise integrals can now be evaluated numerically using a quadrature scheme.
This evaluation is termed \emph{local assembly}, and code that performs it is called the \emph{local kernel}.

\subsubsection{Global assembly}

Given such a local kernel, the final piece required for finite element assembly is an outer loop over the cells, $\mathcal{K}$, that adds cell-wise contributions into the global matrix or vector.
This process is called \emph{global assembly} and is an example of a mesh stencil algorithm.

\begin{figure}
  \centering
  \includegraphics[scale=1.4]{fem_assembly.pdf}
  \caption{
    Diagram of the finite element global assembly process.
    For each cell in the mesh a coefficient is packed into a temporary array before the local kernel is evaluated.
    The result, a dense matrix, is then scattered to the global sparse matrix.
  }
  \label{fig:fem_assembly}
\end{figure}

\begin{algorithm}
  \caption{
    Algorithm for assembling a finite element data structure with a single coefficient in the expression.
  }
  %
  \begin{algorithmic}[1]
    \Require \textit{coefficient}, \textit{output} \Comment{Input and output variables}
    \Require \textit{temp0}, \textit{temp1} \Comment{Work arrays}

    \For{\textit{cell} \textbf{in} \textit{mesh.cells}}
      \State \Call{Read}{\textit{temp0}, \textit{coefficient}, \textit{cell}} \Comment{Pack the coefficient}\label{code:pack_demo}
      \State \Call{Zero}{\textit{temp1}} \Comment{Reset \textit{temp1}}
      \State \Call{LocalKernel}{\textit{temp0}, \textit{temp1}} \Comment{Evaluate the integral for each cell}
      \State \Call{Increment}{\textit{output}, \textit{temp1}, \textit{cell}} \Comment{Scatter (unpack) the result}\label{code:unpack_demo}
    \EndFor
  \end{algorithmic}
  \label{alg:fem_assembly}
\end{algorithm}

Since the local kernel operates at the level of a single cell, it expects data to be provided to it as a small dense array, containing only those DoFs necessary for the calculation.
However, due to the complex nature of unstructured mesh data layouts, the global data structures will have arranged the DoFs differently, and so preparing the data for the local kernel requires an explicit transformation between these representations.
Transforming from global to local representations is called \emph{packing}, and from local to global \emph{unpacking}.

A diagram showing the global assembly loop process, including explicit pack/unpack steps, is shown in \cref{fig:fem_assembly} and an algorithm for its execution in \cref{alg:fem_assembly}.
For the latter there is a single pack instruction, \textsc{Read}, on line~\ref{code:pack_demo}, and a single unpack instruction, \textsc{Increment}, on line~\ref{code:unpack_demo}.
Both instructions take the current iterate, \textit{cell}, as an argument because the data that is packed/unpacked is dependent upon it.

\subsection{Finite element data structures}

\begin{figure}
  \centering
  \includegraphics[width=\textwidth]{scott_vogelius_element_dof_layout_packed.pdf}
  \caption{
    The packing transformation taking global data (top) to packed local data (bottom) for a two-dimensional Scott-Vogelius function space.
    The global data stores all DoFs for the entire function space whilst the packed data only contains the DoFs present in the current stencil iterate.
    DoFs associated with vertices, edges, and cells are coloured green, red, and blue respectively.
  }
  \label{fig:scott_vogelius_element_dof_layout_packed}
\end{figure}

Determining the right pack/unpack operations for a finite element global assembly loop is challenging because:
(a) unstructured mesh data has a non-trivial structure that is difficult to reason about, and
(b) DoFs corresponding to different types of mesh entity (e.g. edges and vertices) must be packed together contiguously.
These issues are jointly demonstrated in \cref{fig:scott_vogelius_element_dof_layout_packed}, which shows a packing operation for a Scott-Vogelius function space suitable for solving the Stokes equations.
Both the local and global representations of the data have some hierarchical structure.
For instance:
\begin{itemize}
  \item All of the DoFs in space $V_h$ precede those of $Q_h$.
  \item The \labelComponent{0} and \labelComponent {1} vector components of $V_h$ are stored adjacently in memory.
  \item For $V_h$, vertices and cells have one node, whilst edges have two.
\end{itemize}
Since the packed local data (bottom) only exists as a temporary its contents are parametrised by the current cell iterate, $c_i$.
For example, the first vertex that is packed for a particular cell is represented by the function $f^v_0(c_i)$ where $f^v_i$ is the map relating cells to incident vertices.

Developing systems to represent this hierarchical structure is challenging, and forms one of the main contributions of this thesis.

\section{Abstractions for mesh stencils}
\label{sec:introduction_software}

When writing continuum mechanics simulations there are myriad considerations that must be taken into account.
These include both high-level mathematical choices such as the PDE to be solved, the discretisation, and the choice of solver; as well as low-level software design choices including the target backend (CPU or GPU), the sparse matrix format, and the layout of vector data.

Even assuming a specific PDE, the unfortunate fact that unites the remaining implementation choices is that they are all \emph{hardware specific}.
Variations in characteristics between different high-performance computing (HPC) systems mean that, to get optimal performance, any of these choices may need to be changed.
This limitation is obvious in some cases, for example CPU-only applications cannot be run on a GPU, but less obvious in others where, for instance, one may need to modify the numerics of the simulation (e.g.~\cite{betteridgeCodeGenerationProductive2021}).

Many continuum mechanics software packages commit eagerly to particular implementation choices and `bake them in' to the software stack, making backing out such changes at a later date very difficult.
Nowhere is this more obvious than with the recent transition from CPUs to GPUs as the dominant architecture on HPC systems.
Many popular simulation packages still do not support GPUs and those that do are only able to because of an immense community effort involving large numbers of developers (e.g.~\cite{millsPETScTAODevelopments2024,millsPerformancePortablePETScGPUbased2020,brownLibCEEDFastAlgebra2021,abdelfattahGPUAlgorithmsEfficient2021}).

In order to avoid this sort of huge re-engineering effort, packages exist that automate the transition between the different implementation choices.
Examples include the SYCL~\cite{SYCL2020Specification2020} and OpenCL~\cite{stoneOpenCLParallelProgramming2010} programming models, which provide unified interfaces for programming heterogeneous hardware, and, at a much higher level, PETSc~\cite{petsc-user-ref,petsc-web-page,petsc-efficient}, a library for massively parallel linear algebra that allows the user to very easily try different solvers and preconditioners (e.g.~\cite{brownComposableLinearSolvers2012,smithPETScSoftwareStrategy}).
With packages such as these, users can easily experiment with different implementations to find the optimum configuration for their platform.

This sort of automation is achieved with the use of appropriate, high-level \emph{abstractions}.
Instead of reasoning about a program in terms of primitives of a low-level programming language (i.e. loops, scalar instructions, pointers), the user is presented with an interface that better represents the semantics of the problem domain.
An example of this is the Basic Linear Algebra Subprograms (BLAS) linear algebra interface~\cite{lawsonBasicLinearAlgebra1979}.
With it, users can reason about their programs using vectors and matrices as primitives and apply operations to them, without needing to consider the internal details of their implementation.
The use of high-level abstractions leads to a separation of concerns between users of a library, who reason at the level of the abstraction, and the library developers, who implement it. 
The result is a programming environment where users are able to produce high performance code in a productive way.

\subsubsection{Code generation}
\label{sec:intro_software_codegen}

It is especially challenging to develop high-level abstractions for mesh stencil calculations because of the level at which problem-specific information is exposed.
Compared with BLAS-like linear algebra routines, where operations are expressed over entire matrices and vectors, the specifics of the PDE and discretisation manifest in the performance-critical local kernel and pack/unpack operations.
Developing abstract interfaces for these operations is therefore not possible without harming performance.

Code generation can be used to solve this problem.
Instead of hiding implementation details behind abstract operations at run-time, a \emph{compiler} can be used to take a symbolic representation of the problem and convert it into fast code.
Since the details of the abstraction are only present at compile-time, code generation can be thought of as a `zero-cost' abstraction.

Examples of continuum mechanics simulation packages where code generation is used for the mesh stencil step include the finite element libraries DUNE-FEM~\cite{dednerGenericInterfaceParallel2010}, FEniCSx~\cite{barattaDOLFINxNextGeneration2023}, and Firedrake~\cite{FiredrakeUserManual}, as well as the finite difference library Devito~\cite{devito-api,luporiniArchitecturePerformanceDevito2020}.

In addition to providing a zero-cost abstraction, by representing the problem at a high level, it is sometimes possible for the compiler to apply specialised optimisations that would be difficult to apply by hand.
Whilst always the case that any automatically generated code could also have been written by hand, compilers frequently generate code with comparable or even superior performance to hand-written implementations~\cite{ragan-kelleyHalideLanguageCompiler}.
One example of this is sum factorisation, a FLOP-reduction technique applicable to finite element simulations with tensor-product structure~\cite{homolyaExposingExploitingStructure2017}.
The compiler can even sometimes do auto-tuning, where a space of possible solutions is explored to find the fastest one.
Examples include ATLAS (dense linear algebra)~\cite{whaleyAutomatedEmpiricalOptimizations2001}, SPIRAL (signal processing)~\cite{puschelSPIRALCodeGeneration2005}, FFTW (discrete Fourier transforms)~\cite{frigoDesignImplementationFFTW32005}, Halide (image processing)~\cite{ragan-kelleyHalideLanguageCompiler}, and the Sparse Polyhedral Framework (sparse linear algebra)~\cite{stroutSparsePolyhedralFramework2018}.
If done at runtime, auto-tuning is also sometimes referred to as an \emph{inspector-executor strategy}.

\subsubsection{Abstraction design}

In order for any abstraction to be effective it must adequately capture the semantics of the problem domain so that the desired program can be automated.
The choice of appropriate types and interfaces is therefore critical to its utility.
For the design of a mesh stencil abstraction there are a number of clear requirements:
\begin{itemize}
  \item There must be a mechanism for declaring loops over mesh entities.
  \item It should either accept, or be able to express, arbitrary local kernels that accept any number of inputs and outputs.
  \item There must be a mechanism for specifying the pack/unpack operations that relate global data to the current iterate.
  \item Scalar, vector and (preferably sparse) matrix types should all be supported.
\end{itemize}

Further, it is not always the case that stencil algorithms consist of a single loop and single local kernel.
Whilst this model holds for finite element assembly, other algorithms important for solving continuum mechanics problems are not expressible.
Examples include hybridisation~\cite{arnoldMixedNonconformingFinite1985,boffi2013mixed} and additive-Schwartz methods~\cite{arnoldPRECONDITIONINGDivAPPLICATIONS1997}.
Hybridisation requires that multiple local kernels be evaluated at each stencil iterate~\cite{gibsonSlateExtendingFiredrake2020}, and additive-Schwartz methods need to execute extra nested loops~\cite{farrellPCPATCHSoftwareTopological2021}.
If we want our abstraction to be able to express these algorithms then it is important that it has a flexible and composable interface.
This sort of interface is often called a \emph{domain-specific language} (DSL).

\subsection{Desirable characteristics for a mesh stencil package}
\label{sec:intro_desirable_traits}

In addition to the requirements of a mesh stencil abstraction listed above, there a number of characteristics that it is desirable for a mesh stencil package to have support for.
These are:

\subsubsection{Distributed memory parallelism}
\label{sec:intro_parallelism}

\begin{figure}
  \centering
  \includegraphics{split_mesh.pdf}
  \caption{
    Diagram showing the DoFs for a function distributed between 2 processes.
    DoFs existing on multiple processes are represented by diamonds, with filled diamonds being those belonging to that process and hollow ones being ghost DoFs.
    DoFs that are not duplicated across processes are shown as circles.
    Sample stencils (cell-wise) are shown in grey to demonstrate that the ghost DoFs are required for the stencils to be computable.
  }
  \label{fig:pyop2_split_mesh}
\end{figure}

In massively parallel simulations the data structures are often too large to be stored in the memory of any individual node.
Instead, they are \emph{distributed} between all of the processes, with each process owning, and seeing, only a small piece of the entire structure.

For vectors, where DoFs are associated with topological entities, it is usual to store the DoFs according to some partitioning of the mesh.
Depending on the stencil calculation, this requires that processes store additional DoFs, called \emph{ghost} DoFs, at their boundaries that belong to different processes.
An example of a vector partitioned according to a mesh partition is shown in \cref{fig:pyop2_split_mesh}.
Ghost DoFs are shown as existing on both processes which ensures that all of the values accessed by the stencil are present locally.

For matrices, the DoF partitioning process is less straightforward.
Rather than implementing a parallel matrix type by-hand it is common to use existing library implementations instead.
PETSc, for example, provides a parallel matrix type that has routines for accessing values that hide the complexities of reading off-process DoFs.

\begin{figure}
  \centering
  \begin{subfigure}{\textwidth}
    \centering
    \includegraphics{two_cell_mesh_lagrange.pdf}
    \vspace{1em}
  \end{subfigure}
  \begin{subfigure}{\textwidth}
    \centering
    \includegraphics[width=\textwidth]{two_cell_mesh_lagrange_data_layout_flat.pdf}
  \end{subfigure}
  \caption{
    Data layout for a $P_3$ function space on a two cell mesh.
    The mesh data has been renumbered such that edge and vertex DoFs are `close' to their incident cell.
  }
  \label{fig:mesh_renumbering_demo}
\end{figure}

\subsubsection{Mesh renumbering}
\label{sec:intro_mesh_numbering}

During the execution of a mesh stencil, the input and output global data structures are accessed with indirection maps.
Accessing data indirectly incurs a performance penalty because it defeats hardware-implemented memory optimisations such as prefetching, where adjacent blocks of memory are loaded into a cache line together.
To improve data locality, a common optimisation is to \emph{renumber} the mesh entities such that, say, edge DoFs are `close' to the DoFs of their incident cell, as well as ensuring that adjacent cells are also close in memory (e.g.~\cite{langeEfficientMeshManagement2016}, Algorithm 3).
This increases both the chance that adjacent required DoFs will be loaded onto the same cache line (spatial locality), and also the chance that DoFs shared between successive iterates will already be in cache (temporal locality).
An example of such a renumbering is shown in \cref{fig:mesh_renumbering_demo}; the data for the vector is arranged such that incident edge and vertex DoFs are adjacent in memory to their cell.
Whilst not necessary for low-order methods, where DoFs are only stored on a single entity type, a truly generic mesh stencil code should have support for `interleaving' DoFs of different entity types.

\begin{figure}
  \centering
  \includegraphics{scott_vogelius_element_dof_layout_swap.pdf}
  \caption{
    An alternative, `struct-of-array' data layout for the Scott-Vogelius function space of \cref{fig:scott_vogelius_element_dof_layout_packed}.
    Notice how the components of the vector space $V_h$ are now stored apart from one another.
  }
  \label{fig:scott_vogelius_element_dof_layout_swap}
\end{figure}

\subsubsection{Data layout flexibility}
\label{sec:intro_data_layout_flex}

In order to get optimal performance on different platforms, especially when transitioning from CPU to GPU architectures, the way in which data are laid out must be altered (e.g.~\cite{markallFiniteElementAssembly2013,sulyokImprovingLocalityUnstructured2018}).
This poses a challenge to many simulation packages as they often commit to a particular representation of the data, typically one optimised for CPU execution, making performance portability difficult to achieve without a substantial rewrite of much of the software.
One possible way in which this manifests is whether to use a `struct-of-array' (SoA) or `array-of-structs' (AoS) data layout.
Applied to a finite element function space these data layouts can manifest as, say, having vector components DoFs stored adjacent to one another (AoS, \cref{fig:scott_vogelius_element_dof_layout_packed}) or as separate contiguous blocks (SoA, \cref{fig:scott_vogelius_element_dof_layout_swap}).

For mesh stencil codes is is therefore desirable that a variety of different data layouts are supported.
One way in which this can be achieved is to decouple the semantics of the data from its implementation.
Taichi~\cite{huTaichiLanguageHighperformance2019}, a DSL for graphics simulations, and TACO~\cite{kjolstadTacoToolGenerate2017}, a compiler for tensor algebra, both provide high-level interfaces for constructing data layouts.
These specifications exist independently of any algorithm, and code generation is used to bridge the gap.
This lets one write a generic algorithm once and experiment with a range of different storage formats to find the one that performs best.

\subsection{Existing mesh stencil packages}
\label{sec:intro_existing_software}

A number of mesh stencil packages already exist, each meeting a subset of the requirements laid out above (see \cref{tab:existing_stencil_code_capabilities}).
\textbf{Liszt}~\cite{devitoLisztDomainSpecific2011}, \textbf{Simit}~\cite{kjolstadSimitLanguagePhysical2016}, \textbf{Ebb}~\cite{bernsteinEbbDSLPhysical2016}, and \textbf{MeshTaichi}~\cite{yuMeshTaichiCompilerEfficient2022} are DSLs for mesh stencil computations embedded in Scala, \cplusplus, Lua, and Python respectively.
Each provides a rich API for expressing a variety of loop constructs and stencil operations that may be compiled and executed on a selection of different backends.
Liszt has support for shared-memory, distributed-parallel, and GPU backends whereas Simit, Ebb, and MeshTaichi only support shared-memory and GPUs.

\begin{table}
  \centering

  \small
  \begin{tblr}{|[1pt]l|[1pt]c|[1pt]c|[1pt]c|[1pt]c|[1pt]c|[1pt]c|[1pt]c|[1pt]}
    \hline[1pt]
    & \textbf{Liszt} & \textbf{Simit} & \textbf{Ebb} & \textbf{MeshTaichi} & \textbf{OP2} & \textbf{\pyop2} & \textbf{\pyop3} \\
    \hline[1pt]
    Flexible DSL & \mytick & \mytick & \mytick & \mytick & \mycross & \mycross & \mytick \\
    \hline[1pt]
    Distributed parallelism & \mytick & \mycross & \mycross & \mycross & \mytick & \mytick & \mytick \\
    \hline[1pt]
    Mesh renumbering & \mycross & \mytick & \mycross & \mycross & \mytick & \mytick & \mytick \\
    \hline[1pt]
    Data layout flexibility & \mycross & \mycross & \mycross & $\sim$ & \mytick & \mycross & \mytick \\
    \hline[1pt]
    Actively maintained & \mycross & \mycross & \mycross & \mytick & \mytick & \mytick & \mytick \\
    \hline[1pt]
  \end{tblr}
  \normalsize
  \caption{
    Comparison of the features of \pyop3 with pre-existing mesh stencil packages.
    `$\sim$' indicate partial implementation of a feature.
  }
  \label{tab:existing_stencil_code_capabilities}
\end{table}

In contrast with these expressive DSLs, \textbf{OP2}~\cite{mudaligeOP2ActiveLibrary2012} and \textbf{\pyop2}~\cite{rathgeberPyOP2HighLevelFramework2012} are mesh stencil packages with much more restrictive interfaces.
Rather than providing a full problem-solving environment, OP2 and \pyop2 programs have a single entrypoint, termed a \emph{parallel loop}, that takes an externally provided local kernel along with an \emph{iteration set} and a list of \emph{arguments}, representing the pack instructions for the kernel inputs and outputs.
This approach maps well to FEM-style assembly loops such as that shown in \cref{alg:fem_assembly}, but more complex loop or kernel structures cannot be represented without sui-generis, non-composable changes.

OP2 is an \emph{active library} that provides source-to-source translation from C, \cplusplus or Fortran to a range of different backends.
By contrast, \pyop2 is a reimplementation of the OP2 API in Python, using run-time code generation to compile the parallel loops instead of source-to-source translation.
By using code generation, \pyop2 is easier to integrate with other code generation packages and in particular it is a core component of the Firedrake finite element framework~\cite{FiredrakeUserManual}.

Of the packages that support distributed memory parallelism, OP2 and \pyop2 differ from Liszt in that their abstractions have no native mesh type, they instead deal in more abstract \emph{sets} and \emph{mappings} between sets.
Being able to do this is convenient as building a mature unstructured mesh package is difficult, making the development and maintenance of the stencil package harder.
Further, by dealing with sets instead of topological entities, OP2 and \pyop2 have the freedom to store data for function spaces where the DoFs are tied to multiple types of topological entity.
This makes mesh renumbering optimisations possible.
Simit also has support for abstract set types and so is also capable of expressing mesh renumbering.

Regarding data layout flexibility, Liszt, Simit, Ebb, and \pyop2 all eagerly commit to a particular data layout which cannot be changed.
MeshTaichi inherits some of the data layout flexibility from Taichi but this only applies to fields per topological entity. Higher levels of partitioning, such as the \pycode{"space"} partitioning in \cref{fig:scott_vogelius_element_dof_layout_packed}, are not possible.
OP2 alternates between AoS and SoA layouts depending on the target backend and whether or not the data are addressed directly or indirectly.

\subsection{The missing abstraction}
\label{sec:intro_missing_abstraction}

\begin{figure}
  \centering
  \includegraphics[width=\textwidth]{scott_vogelius_element_dof_layout_topological.pdf}
  \caption{
    The closest possible data layout for \cref{fig:scott_vogelius_element_dof_layout_packed} for a library that associates unknowns with topological entities.
    Data for each topological entity are stored in separate arrays.
  }
  \label{fig:scott_vogelius_element_dof_layout_topological}
\end{figure}

Of the packages listed above, none of them are able to meet all of the requirements in \cref{tab:existing_stencil_code_capabilities} because of fundamental limitations in their abstractions for representing mesh data.

Liszt, Ebb, and MeshTaichi all store mesh data by associating fields with mesh entities.
This facilitates the development of an expressive DSL because mesh entities are first-class citizens of both the DSL and the data layout specification, making mapping between them straightforward.
However, this approach to storing data means that high-order function spaces, that store DoFs on multiple entity types (e.g. \cref{fig:scott_vogelius_element}), are difficult to express and have to be stored with the DoFs on each entity type stored separately.
An example of this for the Scott-Vogelius element pair is shown in \cref{fig:scott_vogelius_element_dof_layout_topological}.
Since the entity arrays are stored separately, this means that mesh renumbering data locality optimisations are impossible to express.

\begin{figure}
  \centering
  \includegraphics{scott_vogelius_element_dof_layout_pyop2.pdf}
  \caption{
    The data layout matching \cref{fig:scott_vogelius_element_dof_layout_packed} as it would be stored by \pyop2.
    Data for each function space ($V_h$ and $Q_h$) are stored in separate arrays.
  }
  \label{fig:scott_vogelius_element_dof_layout_pyop2}
\end{figure}

As an alternative approach, Simit, OP2, and \pyop2 are able to represent function space DoFs more generically as \emph{sets of nodes}.
Being able to treat all the nodes of a function space equivalently means that mesh renumbering is possible, but it comes at the expense of topological information.
This is demonstrated in \cref{fig:scott_vogelius_element_dof_layout_pyop2}, which shows the data layout for a Scott-Vogelius function space as it would be stored in \pyop2.
Observe that the topological information - the \pycode{"mesh"} layer - is no longer present, illustrating that the relation between nodes and their owning mesh entities is lost.
Since topological information is lost, it is difficult to build a composable DSL, explaining why OP2 and \pyop2 both only have restricted interfaces.
Simit can define data structures using either mesh entities or node sets and so it can either be a composable DSL or support high-order function spaces, but not both at the same time.

To meet the requirements of \cref{tab:existing_stencil_code_capabilities} it is therefore necessary that a new abstraction for mesh data be developed that is capable of representing complex, interleaved data layouts whilst retaining sufficient topological information to enable an expressive DSL.
Developing such an abstraction forms the central contribution of this thesis.

\section{Thesis outline}

The remainder of this thesis proceeds as follows:
\begin{itemize}
  \item
    \Cref{chapter:foundations} introduces in more detail the abstractions and software packages that contributed to the design of the new abstractions.
  \item
    \Cref{chapter:axis_trees,chapter:indexing} introduce the new abstractions developed for describing mesh data layouts: the \emph{axis tree} and \emph{index tree}.
  \item
    \Cref{chapter:pyop3} presents \pyop3, a new mesh stencil package built upon these abstractions that meets the full set of requirements of \cref{tab:existing_stencil_code_capabilities}
    Additional considerations to enable distributed memory parallelism in \pyop3 are then discussed in \cref{chapter:parallel}.
  \item \Cref{chapter:firedrake} describes how \pyop3 has been integrated into the finite element framework Firedrake, replacing \pyop2, and \cref{chapter:demonstrator_applications} demonstrates this integration with a number of examples.
  \item Lastly we conclude our findings in \cref{chapter:conclusions} and consider a number of potential directions for future research.
\end{itemize}

\end{document}
