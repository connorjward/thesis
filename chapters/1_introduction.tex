\documentclass[thesis]{subfiles}

\begin{document}

\chapter{Introduction}
\label{chapter:introduction}

% TODO: don't love this sentence
The numerical solution of \glspl{pde} is ubiquitous across a wide range of applications, from climate modelling to structural mechanics and manufacturing.
In order to solve them numerically, depending on the numerical method chosen, different elements of the geometry and/or function spaces of the physical problem must be \textit{discretised} such that they may be represented on a computer.

% Solving such problems requires enormous amounts of computational power
% Software for solving them is important

\begin{figure}
  \centering
  \includegraphics[scale=1.5]{mesh_stencil.pdf}
  \caption{
    An example of a mesh stencil operation where cells are looped over and adjacent entities are accessed by the stencil (grey).
    DoFs stored on the cells, edges, and vertices are represented by blue, red, and green dots respectively.
  }
  \label{fig:mesh_stencil}
\end{figure}

For a number of numerical methods such as the finite volume, finite difference, and finite element methods, a \textit{mesh stencil} approach is taken.
With such methods the geometry of the problem is discretised to form a \textit{mesh} composed of topological entities termed \textit{cells}, with each having sub-entities such as faces, edges and vertices.
Depending on the method and implementation choices, this mesh may be either \textit{structured}, where adjacent entities may be addressed by applying offsets, or \textit{unstructured}, where an indirection map of the form \ccode{A[B[i]]} is needed.
To these topological entities are associated unknowns, termed \glspl{dof}, representing physical quantities in the PDE like, say, the pressure.

For all of these methods, an integral part of their usage is the application of a stencil over mesh entities.
One loops over topological entities, typically cells or faces, and computes some value using data that is `close' the current iterate (e.g. \cref{fig:mesh_stencil}).

Mesh stencil computations crop up repeatedly in continuum mechanics simulations and developing methods for their application is the foundational concept behind this thesis.

\section{Mesh stencils in practice: the finite element method}
\label{sec:stokes_equations}

As an introductory example to a mesh stencil calculation, we consider solving the Stokes equations, a linearisation of the Navier-Stokes equations in common use in the fluid dynamics community, using the \gls{fem}.
Our exposition will focus on the aspects of the computation that are relevant for mesh stencils, for a more complete review of \gls{fem} we refer the reader to~\cite{brennerMathematicalTheoryFinite2008} and~\cite{larsonFiniteElementMethod2013}.
The Stokes equations are given as follows: for domain $\Omega$ with boundary $\Gamma$, find the fluid velocity ($u$) and pressure ($p$) such that

\begin{subequations}
  \begin{align}
    - \nu \Delta u + \nabla p &= 0 \quad \textrm{in} \ \Omega, \\
    %
    \nabla \cdot u &= 0 \quad \textrm{in} \ \Omega, \\
    %
    u &= g \quad \textrm{on} \ \Gamma,
  \end{align}
  %
  \label{eq:strong_stokes}
\end{subequations}

\todo[inline]{I don't talk about the viscosity at all. It is convenient for later if it is in P3.}

where $\nu$ is the viscosity and $g$ is some prescribed Dirichlet boundary condition fixing the velocity across the entire boundary.
Forcing terms are omitted for simplicity.
As we have a coupled system of two variables ($u$ and $p$), we refer to this system as being \textit{mixed}.

\subsection{Deriving a weak formulation}

\todo[inline]{I need someone to check over my function spaces. In particular when do I use $V_0$ vs $V$?}
\todo[inline]{Once that's done I need some consistent notation: $V_0$ or $\hat V$?}

For the finite element method we seek the solution to the \textit{variational}, or \textit{weak}, formulation of these equations.
These are obtained by multiplying each equation by a suitable \textit{test function} and integrating over the domain.
For \cref{eq:strong_stokes}, using $v$ and $q$ as the test functions and integrating by parts this gives

% taken from 
% https://nbviewer.org/github/firedrakeproject/firedrake/blob/master/docs/notebooks/06-pde-constrained-optimisation.ipynb
% and Larson and Bengzon (pg. 293)
\begin{subequations}
  \begin{align}
    \int \nu \nabla u : \nabla v \, \textrm{d}\Omega
    - \int p \nabla \cdot v \, \textrm{d}\Omega
    - \int \nu (\nabla u \cdot n) \cdot v \, \textrm{d}\Gamma
    - \int p n \cdot v \, \textrm{d}\Gamma
    &= 0
    &\forall v \in \hat V
    \label{eq:weak_stokes_extra_V} \\
    %
    \int q \, \nabla \cdot u \, \textrm{d}\Omega
    &= 0
    &\forall q \in Q.
  \end{align}
\end{subequations}

From these weak forms it is now possible to classify the function spaces for $u$ and $p$.
For $u$, we already know that the space must be vector-valued and constrained to $g$ on the boundary.
\Cref{eq:weak_stokes_extra_V} further shows us that $u$ must have at least one weak derivative.
We can therefore say that $u \in V$ where, for dimension $d$,

\begin{equation}
  V = \{ \ v \in [H^1(\Omega)]^d : v |_{\Gamma} = g \ \}.
  \label{eq:stokes_velocity_space}
\end{equation}

Unlike $u$, $p$ is scalar-valued, needs no derivatives of $p$, and does not have any boundary conditions applied to it, so we can write that $p \in Q$ where

\begin{equation}
  Q = \{ \ q \in L^2(\Omega) \ \}
  \label{eq:stokes_pressure_space}
\end{equation}

\todo[inline]{This is the bit I really don't know how to properly explain.}

Since the values of $u$ at the boundary are already prescribed, the function space of the test function $v$ is defined to be zero at those nodes

\begin{equation}
  \hat V = \{\ v \in [H^1(\Omega)]^d : v|_{\Gamma} = 0 \ \}.
\end{equation}

This allows us to drop some terms from \cref{eq:weak_stokes_extra_V}, allowing us to state the final problem as follows: find $(u, p) \in V \times Q$ such that

\begin{subequations}
  \begin{align}
    \int \nu \nabla u : \nabla v \, \textrm{d}\Omega
    - \int p \nabla \cdot v \, \textrm{d}\Omega
    &= 0
    &\forall v \in \hat V
    \label{eq:weak_stokes_V} \\
    %
    \int q \, \nabla \cdot u \, \textrm{d}\Omega
    &= 0
    &\forall q \in Q.
    \label{eq:weak_stokes_Q}
  \end{align}
  \label{eq:weak_stokes}
\end{subequations}

\subsection{Discretising the system of equations}

In order to solve this weak formulation using the finite element method we discretise the function spaces in use by replacing them with finite dimensional equivalents:

\begin{align*}
  &V \quad \to \quad V_h \subset V, \\
  &\hat V \quad \to \quad \hat V_h \subset \hat V, \\
  &Q \quad \to \quad Q_h \subset Q.
\end{align*}

Each of these discrete spaces is spanned by a set of basis functions, so any function can be expressed as a linear combination of the basis functions and their coefficients.
For example, we can write the function $u_h \in V_h$ as

\begin{equation}
  u_h = \sum^N_{i=1} \hat u_i \psi^{V_h}_i
\end{equation}

for basis functions $\psi^{V_h}_i$ and coefficients $\hat u_i$.

Substituting these discrete spaces back into \cref{eq:weak_stokes}, and discarding the basis coefficients for the arbitrary functions $v_h$ and $q_h$, we obtain the discrete problem: find $(u_h, p_h) \in V_h \times Q_h$ such that

\begin{subequations}
  \begin{align}
    \int \nu u_h \nabla \psi^{V_h} : \nabla \psi^{\hat V_h} \, \textrm{d}\Omega
    - \int p_h \psi^Q \nabla \cdot \psi^{\hat V_h} \, \textrm{d}\Omega
    &= 0
    \quad \forall \psi^{\hat V} \\
    %
    \int \psi^Q \, u_h \nabla \cdot \psi^{V_h} \, \textrm{d}\Omega
    &= 0
    \quad \forall \psi^{Q},
  \end{align}
  \label{eq:weak_stokes_discrete}
\end{subequations}

which can be written as the (saddle point) linear system

\todo[inline]{Do I want $V$ or $\hat V$ in the matrix? I feel like we should want the former...}

\begin{equation}
  \left (
  \begin{array}{c|c}
    \int \nu \nabla \psi^{V_h} : \nabla \psi^{\hat V_h} \, \textrm{d}\Omega
    &
    - \int \psi^Q \nabla \cdot \psi^{\hat V_h} \, \textrm{d}\Omega \\
    \hline
    \int \psi^Q \, \nabla \cdot \psi^{V_h} \, \textrm{d}\Omega
    &
    0
  \end{array}
  \right )
  \left (
  \begin{array}{c}
    u_h \\
    \hline
    p_h
  \end{array}
  \right )
  =
  \left (
  \begin{array}{c}
    0 \\ \hline 0
  \end{array}
  \right )
  %
  \label{eq:stokes_linear_system}
\end{equation}

Solving the Stokes equations using the finite element method therefore boils down to constructing, or \textit{assembling}, the left-hand-side matrix and the - here trivial - right-hand-side vector before solving for the coefficients of $u_h$ and $p_h$.

\subsection{Choosing a basis}

\begin{figure}
  \centering
  \begin{subfigure}{.4\textwidth}
    \centering
    \includegraphics{lagrange_element_2.pdf}
    \vspace{1em}
    \caption{
      The $P_2$ (Lagrange, degree 2) finite element for a triangle.
      Point evaluation DoFs are associated with edges (red) and vertices (green).
    }
    \label{fig:lagrange_element_2}
  \end{subfigure}
  %
  \begin{subfigure}{.58\textwidth}
    \centering
    \begin{tabular}{c c c}
      \includegraphics[width=.3\textwidth]{element_Lagrange_2_dof0.pdf}
      &
      \includegraphics[width=.3\textwidth]{element_Lagrange_2_dof1.pdf}
      &
      \includegraphics[width=.3\textwidth]{element_Lagrange_2_dof2.pdf}
      \\
      \includegraphics[width=.3\textwidth]{element_Lagrange_2_dof3.pdf}
      &
      \includegraphics[width=.3\textwidth]{element_Lagrange_2_dof4.pdf}
      &
      \includegraphics[width=.3\textwidth]{element_Lagrange_2_dof5.pdf}
    \end{tabular}
    %
    \caption{
      The basis functions of a $P_2$ finite element.
      The images were reused (with permission) from DefElement~\cite{defelement}.
    }
    \label{fig:lagrange_element_2_basis}
  \end{subfigure}
\end{figure}

In order to numerically evaluate the integrals in \cref{eq:stokes_linear_system} the basis functions $\psi^{V_h}$ and $\psi^Q$ must be known.
In the finite element method these are generated from the choice of \textit{finite element}.

First formalised by Ciarlet~\parencite{ciarletElement2002}, a finite element is the triple $(K, P, N)$, where:

\begin{itemize}
  \item $K$ is a bounded closed set, or \textit{cell}, with non-empty interior and piecewise smooth boundary,
  \item $P$ is a finite-dimensional space of functions on $K$, and
  \item $N$ is a set of linear functionals that form a basis for the dual space of $P$.
\end{itemize}

A simple example of a finite element, the Lagrange degree 2 element, is shown in \cref{fig:lagrange_element_2}.
For this element $K$ (the cell) is a triangle, $P$ (the function space) is the space of order 2 polynomials, and $N$ (the dual basis) is defined to be point evaluation at each of the nodes.
That is to say, linear functional $l_i$, taking as input some function $v$, is defined to be

\begin{equation*}
  l_i(v) = v(x_i),
\end{equation*}

\noindent
where $x_i$ is the coordinates of the $i$-th node.

Given this definition, one can determine a basis, $\psi_j$, for $P$ by imposing that

\todo[inline]{Not sure how to write forall i, j bit. And $n_k$ currently undefined.}

\begin{equation*}
  l_i(\psi_j) = \delta_{ij} \quad i, j = 0, 1, \dots, n_k,
\end{equation*}

\noindent
which for the $P_2$ element yields the basis functions (\cref{fig:lagrange_element_2_basis}):

% see https://defelement.com/elements/examples/triangle-lagrange-equispaced-2.html
\begin{align}
  &\psi_0 = 2x^2 + 4xy - 3x + 2y^2-3y+1,
  &
  &\psi_3 = 4xy, \\
  %
  &\psi_1 = x(2x-1),
  &
  &\psi_4 = 4y(-x-y+1), \\
  %
  &\psi_2 = y(2y-1),
  &
  &\psi_5 = 4x(-x-y+1).
  \label{eq:basis_functions}
\end{align}

\subsubsection{Mapping to reference space}

\todo[inline]{bit ick, reword}

The basis functions enumerated above are given in \textit{reference space}, that is, relative to the reference finite element.
By contrast, the basis functions of the variational problem exist in \textit{physical space}: cells in the mesh are rarely exactly regular in shape.
% not a lot of detail, revisit
In order to be able to use the reference basis functions one needs to introduce Jacobian-like terms into the integrals termed \textit{pullbacks}.

From \cref{fig:lagrange_element_2} one can see that the linear functionals, and hence the basis functions and DoFs, are associated with different topological entities, and this is emphasised by showing them in different colours.
When the finite element definition above is transformed from \textit{reference space} to \textit{physical space}, DoFs associated with topological entities of the reference cell are associated with equivalent entities on the mesh.
\Cref{fig:mesh_stencil} for example shows a mesh storing DoFs for a $P_3$ finite element.
Just as cell sub-entities (e.g. edges and vertices) are shared between adjacent cells, so are the DoFs associated with those entities.


\subsubsection{Choosing a basis for the Stokes equations}

\begin{figure}
  \centering
  \begin{subfigure}{.35\textwidth}
    \centering
    \includegraphics{lagrange_element_3_vec.pdf}
    \label{fig:scott_vogelius_element_P3}
  \end{subfigure}
  %
  \begin{subfigure}{.35\textwidth}
    \centering
    \includegraphics{lagrange_element_2_dg.pdf}
  \end{subfigure}
  %
  \caption{
    The Scott-Vogelius element, degree 3 ($[P_3]^2 \oplus P_2^\mathrm{disc}$).
    The lower-order space (right) is discontinuous and hence all of the DoFs are marked as belonging to the cell (blue).
  }
  \label{fig:scott_vogelius_element}
\end{figure}

The choice of basis functions used by the function spaces has significant implications for the convergence and stability of the method.
For the two-dimensional Stokes equations, a common choice of element pair, or \textit{mixed} element, with properties matching the constraints given in \cref{eq:stokes_velocity_space} and \cref{eq:stokes_pressure_space} is the Scott-Vogelius element~\cite{scottNormEstimatesMaximal1985}.
The element consists of a continuous vector-valued degree $k$ Lagrange element for the velocity space, and a discontinuous Lagrange element of degree $k-1$.
This is shown in \cref{fig:scott_vogelius_element} for $k = 3$\footnote{The two-dimensional Scott-Vogelius element is technically known to be stable only for $k \geq 4$~\cite{guzmanScottVogeliusFiniteElements2018}, but we show $k = 3$ for simplicity.}.

\subsection{Finite element assembly}

Having established the basis functions and pullbacks of the discrete function spaces we are now able to numerically evaluate, via a quadrature scheme, the integrals of \cref{eq:stokes_linear_system}.
This step is referred to as \textit{local assembly}, and code for its evaluation is termed the \textit{local kernel}.
The final piece required for finite element assembly is an outer loop, where local kernel is evaluated for each basis function in the discrete space.
This process is termed \textit{global assembly}.

If we again consider the Stokes problem above, to assemble the top left block of the matrix in \cref{eq:stokes_linear_system} we see that we need to evaluate

\begin{equation}
  \int \nu \nabla \psi^{V_h}_i : \nabla \psi^{\hat V_h}_j \, \textrm{d}\Omega
  \label{eq:stokes_top_left}
\end{equation}

\noindent
for each possible pair of basis functions $\psi^V_i$ and $\psi^V_j$, suggesting an assembly algorithm with quadratic complexity.
However, this may be improved by observing that finite element basis functions are only non-zero in a small region around their owning mesh entity (e.g. \cref{fig:lagrange_element_2_basis}) - we say that they have \textit{local support}.
This means that most of the integrals are \textit{zero by definition}, and the assembled matrix is \textit{sparse}.

\begin{algorithm}
  \caption{
    Algorithm for assembling a finite element data structure with a single coefficient in the expression.
  }
  %
  \begin{algorithmic}[1]
    \Require \textit{coefficient}, \textit{output} \Comment{Input and output variables}
    \Require \textit{temp0}, \textit{temp1} \Comment{Work arrays}

    \For{\textit{cell} \textbf{in} \textit{mesh.cells}}
      \State \Call{Read}{\textit{coefficient}, \textit{cell}, \textit{temp0}}
      \State \Call{Zero}{\textit{temp1}}
      \State \Call{LocalKernel}{\textit{temp0}, \textit{temp1}} \Comment{Evaluate the integral for each cell}
      \State \Call{Increment}{\textit{output}, \textit{temp1}} \Comment{Scatter the result}
    \EndFor
  \end{algorithmic}
  \label{alg:fem_assembly}
\end{algorithm}

\begin{figure}
  \centering
  \includegraphics[scale=1.4]{fem_assembly.pdf}
  \caption{
    Diagram of the assembly process for \cref{eq:stokes_top_left}.
    For each cell in the mesh the viscosity coefficient is packed into a temporary array before the local kernel is evaluated.
    The result, a dense matrix, is then scattered to the global sparse matrix.
  }
  \label{fig:fem_assembly}
\end{figure}

In order to avoid this quadratic complexity, finite element assembly typically proceeds by iterating over cells and evaluating the local kernel on each cell.
Only basis functions with non-zero support over the cell - DoFs on the cell or its sub-entities - need be considered for the assembly.
The global assembly may therefore be expressed as the sum of cell-wise integrals

\todo[inline]{define $\mathcal{T}$ somewhere above}

\begin{equation}
  \sum_{K \in \mathcal{T}} \int_K \nu \nabla \psi_i^K : \nabla \psi_j^K \, \textrm{d}\Omega,
\end{equation}

\noindent
with $\phi^K$ are the basis functions over the cell.

This process is shown in \cref{fig:fem_assembly}, and an algorithm for its execution is summarised in \cref{alg:fem_assembly}.
From these, it is clear why finite element assembly is referred to as a mesh stencil process.

\subsection{Finite element data structures}

\begin{figure}
  \centering
  \includegraphics{scott_vogelius_element_dof_layout_packed.pdf}
  \caption{
    The packing transformation taking the global data (top) to the packed local data (bottom) for a two-dimensional Scott-Vogelius function space.
    The global data stores all DoFs for the entire function space whilst the packed data only contains the DoFs present in the current stencil iterate.
    \\
    The data layout has been split into multiple levels to emphasise the hierarchical nature of function spaces.
    For instance, every edge in the mesh (red) for the velocity space $[P_3]^2$ ($V_h$) has 2 nodes: $n_0$ and $n_1$, with each node having 2 components: \labelComponent{0} and \labelComponent{1}.
    \\
    As the packed local data only exists as a temporary, its contents are parametrised by the current cell iterate, $c_i$.
    Sub-entities are identified via a relation from this, for instance the first vertex incident on $c_i$ is represented by $f_0^v(c_i)$.
  }
  \label{fig:scott_vogelius_element_dof_layout_packed}
\end{figure}

Before we can write any assembly code we need to formalise what it means to pack and unpack the local data (lines~\ref{code:pack_demo} and~\ref{code:unpack_demo} in \cref{alg:fem_assembly}).
This requires that the \textit{data layouts} of both the global and local data structures be defined.

Despite the fact that the mesh is unstructured, and hence `adjacent' topological entities may be far from it when stored in memory, function spaces have an amount of inherent structure that must be taken into account when determining the right pack/unpack transformations.
A potential structure for a two-dimensional Scott-Vogelius function space is demonstrated in \cref{fig:scott_vogelius_element_dof_layout_packed}.
The top part of the figure represents the data layout for the global data structure and the bottom for the local packed one passed to the local kernel.
The size of the global data layout is given by

\begin{equation*}
  \underbrace{2 ( 1 \times n_\textnormal{cells} + 2 \times n_\textnormal{edges} + 1 \times n_\textnormal{vertices} )}_{V_h} + \underbrace{6 \times n_\textnormal{cells}}_{Q_h},
\end{equation*}

\noindent
and the size of the local layout is

\begin{equation*}
  \underbrace{2 ( 1 \times 1 + 2 \times 3 + 1 \times 3 )}_{V_h} + \underbrace{6 \times 1}_{Q_h} = 26.
\end{equation*}

From \cref{fig:scott_vogelius_element_dof_layout_packed} it is clear that both the global and local data layouts have some inherent structure: DoFs for the different spaces $V_h$ and $Q_h$ are kept apart and nodes and components are stored contiguously.
Representing this hierarchical structure in code is challenging and forms one of the main contributions of this thesis (\cref{chapter:axis_trees,chapter:indexing}).

\section{Software for mesh stencil calculations}

Developing software for numerical methods involving mesh stencils is difficult for a number of reasons:

% Writing these codes by hand is prohibitively difficult: writing a performant and scalable simulation would take months or years of programmer effort and any changes to the \glspl{pde}, discretisation or hardware might constitute a substantial rewrite.
% NOTE: I think that this should probably be put together into a simple paragraph, but not too sure
\paragraph{It is time-consuming.}{
  A piece of scientific software can take many person-years to develop.
  Finite element packages, for example, need to not only implement the assembly routines, but also need to have the right linear and nonlinear solvers, mesh implementation, input/output routines and many more.
  This can take years of developer effort to implement.
}

\paragraph{It is a multidisciplinary effort.}{
  Implementing a high-performance, parallel code requires expertise in a wide range of disciplines ranging from high-level numerical analysis and algorithms to low-level optimisation of loop nests and vectorisation.
  A team of experts or a handful of very talented individuals are therefore needed.
}

\paragraph{The code would be `brittle'.}{
  Code written by hand is naturally specialised to the problem of interest.
  Due to the complexity and tightly-coupled nature of the code this makes it difficult to extend its functionality to other circumstances.
  Even solving continuum mechanics problems on different computers can require \textit{substantial} changes to the numerical methods being used in order to get good performance~\cite{betteridgeCodeGenerationProductive2021}, again a prohibitive cost for handwritten codes.
}

\vspace{\baselineskip}

% "library packages" vs "DSL packages"?

Clearly a scientist looking to solve a continuum mechanics problem should avoid writing their codeby hand from scratch and they should instead look to use one of the \textit{many} pre-existing packages as a toolkit to build their simulation.
These packages may be broadly split into two categories: `library' packages and run-time code generation packages.

Library packages work in the traditional way: the developers build a suite of routines, often in a compiled language like C or \cplusplus, and these are called by the user via the package's API.
Examples of such packages include deal.ii~\cite{}, Dune-FEM~\cite{}, MFEM~\cite{}, libCEED~\cite{} nektar++~\cite{}.
Whilst straightforward to implement, choosing a library-style approach for numerical software has a number of drawbacks:

\begin{itemize}
  \item Either slow (generic functions, lots of internal logic), specialised to a particular application or a nightmare to maintain (large codebase) or templated (really hard to reason about).
  \item OR, pushes responsibility for performance onto the user - provide the kernels!
  \item Portability is difficult too
  \item Commit to a lot of design choices early (e.g. data layout) - GPU paper shows that this can't always be fixed
\end{itemize}

These issues may be resolved somewhat by `metaprogramming' - effectively \textit{compile-time} code generation - with techniques such as templating and using device-agnostic programming languages such as Kokkos and SYCL.
These methods are, however, fundamentally limited:

\begin{itemize}
  \item
    Representing sufficient information about the program at compile-time is very challenging.
    For example separate templated variables would be required for XXX, YYY and ZZZ.
    Need to hardcode loop bounds at the innermost loops for performance reasons.

  \item
  Template programming is notoriously complicated and difficult to debug.

  \item
  Compile times and binary sizes can become excessive as every possible combination of parameters needs to be separately compiled.
\end{itemize}

Run-time code generation is a favourable alternative.
Effectively templating on steroids - codegen can be done in a traditional programming language like Python, facilitating reasoning and debugging experience etc.
% have full ability to reason about the program as actual data is accessible - can do search-space Halide-like stuff (inspector executor!)
% % talk about BLIS, Spiral, FFTW, Halide etc
% an underappreciated aspect of it is that there is far less code to maintain

  % also composable (high-level) abstractions
In addition, we get "no cost" high-level abstractions -> huge increase in productivity, separation of concerns (easier for domain experts to contribute), high-level reasoning, automation == layer abstractions on top.
Example is the Firedrake/FEniCSx projects with UFL.
% also devito
% having a high-level representation of the problem enables optimisations best expressed at the level of the mathematics that would otherwise be very challenging to implement (e.g.~\cite{homolyaExposingExploitingStructure2017}).

% "not a panacea"
% but mention that it is harder to implement and can be harder to get top-tier performance as
% performance is hidden behind a compiler
% also a performance hit at runtime as compilation needs to happen then - but much less time than
% compiling all of the templates!
% also, importantly, if we can't generate code for things then we can't run them - can be hard to prototype new methods if they lie outside of the abstraction
% "for the purposes of this thesis we regard these less as drawbacks and more as important avenues of research"
This thesis focuses on the development of new abstractions for mesh stencil computations to facilitate the implementation of numerical methods and performance optimisations.

\subsection{Further design considerations}
\label{sec:mesh_stencil_requirements}

In addition to having the right high-level abstractions and backing code generation infrastructure, there a number of essential requirements that a mesh stencil package must meet in order for it to be effective.
These requirements are:

\begin{figure}
  \centering
  \includegraphics{split_mesh.pdf}
  \caption{
    Diagram showing the DoFs for a function distributed between 2 processes.
    DoFs existing on multiple processes are represented by diamonds, with filled diamonds being those belonging to that process and hollow ones being ghost DoFs.
    DoFs that are not duplicated across processes are shown as circles.
    Sample (cell-wise) stencils are shown in grey to demonstrate that the ghost DoFs are required for the stencils to be computable.
  }
  \label{fig:pyop2_split_mesh}
\end{figure}

\subsubsection{Distributed memory parallelism}

In massively parallel simulations the data structures are often too large to be stored in the memory of any individual node.
Instead, they are \textit{distributed} between all of the processes, with each process owning, and seeing, only a small piece of the entire structure.
For vectors it is usual to store the DoFs according to some partitioning of the mesh (\cref{fig:pyop2_split_mesh}).
Depending on the stencil calculation, processes often need to store DoFs at their boundaries belonging to different processes; these entities are frequently referred to as \textit{ghost points}.
For matrices the DoF partitioning process is less straightforward.
Linear algebra libraries such as PETSc~\cite{petsc-user-ref,petsc-web-page,petsc-efficient} provide routines for accessing parallel matrices in a manner such that off-process DoF accesses are hidden.
\textbf{Mesh stencil packages need to have support for distributed data structures as well as have the right routines for ensuring the correctness of the ghost points.}

\subsubsection{Performance portability}

Due to the search for ever-increasing computational power and energy efficiency, it is no longer possible to write a piece of low-level code and deploy it on all supercomputers.
GPU machines now beginning to dominate the HPC landscape.
Also ARM (RISC) CPUs are popular (e.g. Fugaku).
Programming for these disparate architectures requires both new programming languages/models, in the case of GPUs, as well as high-level algorithmic changes~\cite{betteridgeCodeGenerationProductive2021} to get acceptable performance.
The situation is further complicated by the programming models being vendor-specific with languages such as CUDA, HIP and Sycl all being specially designed to get performance from a particular vendor's accelerators.
\textbf{A successful mesh stencil code should be able to switch between backends with only minimal changes to both user and internal code, and be reasonably performant for them all.}

\todo[inline]{This bit seems a bit forced but I do want to make a point that implementing your own mesh, matrices, solvers etc is dumb. Perhaps this should just be "parallel sparse matrices".}

\subsubsection{Interoperability with external software}

As described above, the work needed to successfully implement a piece of simulation software is enormous and prohibitive.
\textbf{To make such an endeavour tractable, and maintainable in the long term, mesh stencil codes should avoid `reinventing the wheel' by leveraging existing software wherever possible.}
In particular, meshing software is notoriously complicated to implement as a large array of features from geometry to partitioning and input/output require implementing; it is preferable to use existing, mature, implementations for such features (e.g. PETSc~\cite{petsc-user-ref,petsc-web-page,petsc-efficient}).

\begin{figure}
  \centering
  \begin{subfigure}{\textwidth}
    \centering
    \includegraphics{two_cell_mesh_lagrange.pdf}
    \vspace{1em}
  \end{subfigure}
  \begin{subfigure}{\textwidth}
    \centering
    \includegraphics{two_cell_mesh_lagrange_data_layout_flat.pdf}
  \end{subfigure}
  \caption{
    Data layout for a $P_3$ function space on a two cell mesh.
    The mesh data has been renumbered such that edge and vertex DoFs are `close' to their incident cell.
  }
  \label{fig:mesh_renumbering_demo}
\end{figure}

\subsubsection{Mesh renumbering}

During the execution of a mesh stencil the input and output global data structures are accessed with indirection maps.
Accessing the data indirectly incurs a performance penalty because it defeats hardware-implemented memory optimisations such as prefetching, where adjacent blocks of memory are loaded into a cache line together.
To improve data locality, a common optimisation is to \textit{renumber the mesh entities} such that, say, edge DoFs are `close' to the DoFs of their incident cell (e.g. Algorithm 3 of~\cite{langeEfficientMeshManagement2016}).
This increases both the chance that adjacent required DoFs will be loaded onto the same cache line (spatial locality), and also the chance that DoFs shared between successive iterates will already be in cache (temporal locality).
An example of such a renumbering is shown in \cref{fig:mesh_renumbering_demo}; the data for the vector is arranged such that incident edge and vertex DoFs are adjacent in memory to their cell.
Whilst not necessary for low-order methods, where DoFs are only stored on a single entity type, \textbf{a truly generic mesh stencil code needs to have support for `interleaving' DoFs of different entity types.}

\begin{figure}
  \centering
  \includegraphics{scott_vogelius_element_dof_layout_swap.pdf}
  \caption{
    An alternative, `struct-of-array' data layout for the Scott-Vogelius function space of \cref{fig:scott_vogelius_element_dof_layout_packed}.
    Notice how the components of the vector space $V_h$ are now stored apart from one another.
  }
  \label{fig:scott_vogelius_element_dof_layout_swap}
\end{figure}

\subsubsection{Data layout flexibility}

In order to get optimal performance on different platforms, especially when transitioning from CPU to GPU architectures, the way in which data is laid out must be altered (e.g.~\cite{markallFiniteElementAssembly2013,sulyokImprovingLocalityUnstructured2018}).
This poses a challenge to many simulation packages as they often commit to a particular representation of the data, typically one optimised for CPU execution, posing difficulties for achieving performance portability without a substantial rewrite of much of the software.
One possible way in which this manifests is whether or not to use a `struct-of-array' (SoA) or `array-of-structs' (AoS) data layout.
Applied to a finite element function space these data layouts can manifest as, say, having vector components stored adjacent to one another (AoS, \cref{fig:scott_vogelius_element_dof_layout_packed}) or separately (SoA, \cref{fig:scott_vogelius_element_dof_layout_swap}).

\textbf{As a consequence, we naturally want mesh stencil codes to be able to support a variety of data layout representations, ideally without any major changes to the code.}
Taking a high-level abstraction approach is useful for this as a separation of concerns may be drawn between the semantics of the data and its materialisation.
Examples of such abstractions providing high-level interfaces for constructing differing data layouts include Taichi~\cite{huTaichiLanguageHighperformance2019}, a DSL for graphics simulations, and TACO~\cite{kjolstadTacoToolGenerate2017}, a compiler for tensor algebra.
Both packages separate the specification of the data layouts from the implementation of the algorithms and have been shown to have excellent performance.

\subsection{Existing mesh stencil packages}

% TODO: Maybe have macros to make sure the headings agree with above
% FIXME: "Data layout flexibility" is far too vague here to have a tick or cross - perhaps "Dynamic data layouts"
\begin{table}
  \centering

  \begin{tblr}{|[1pt]l|[1pt]c|[1pt]c|[1pt]c|[1pt]c|[1pt]c|[1pt]c|[1pt]}
    \hline[1pt]
    & \textbf{Liszt} & \textbf{Simit} & \textbf{Ebb} & \textbf{MeshTaichi} & \textbf{OP2} & \textbf{PyOP2} \\
    \hline[1pt]
    Distributed memory parallelism & \mytick & \mycross & \mycross & \mycross & \mytick & \mytick \\
    \hline
    Performance portability & \mytick & \mytick & \mytick & \mytick & \mytick & \mycross\footnotemark \\
    \hline
    Sparse matrix support & ? & \mytick & \mytick & \mytick & \mycross & \mytick \\
    \hline
    Mesh renumbering\footnotemark & \mycross & \mytick & \mycross & \mycross & \mytick & \mytick \\
    \hline
    Dynamic data layouts & \mycross & \mytick & \mytick & \mytick & \mycross & \mycross \\
    \hline
    Actively maintained & \mycross & \mycross & \mycross & \mytick & \mytick & \mytick \\
    \hline[1pt]
  \end{tblr}
  \caption{Comparison of the features of some pre-existing mesh stencil packages.}
  \label{tab:existing_stencil_code_capabilities}
\end{table}

% FIXME: These don't label correctly
\footnotetext{Proof-of-concept GPU support has been demonstrated for PyOP2~\cite{fenics2021-kulkarni}.}
\footnotetext{Simit, OP2, and PyOP2 all support storing data on generic set-like structures so any DoF ordering may be achieved.}

A number of mesh stencil packages already exist, each meeting a subset of the requirements laid out above (see \cref{tab:existing_stencil_code_capabilities}).
\textbf{Liszt}~\cite{devitoLisztDomainSpecific2011}, \textbf{Simit}~\cite{kjolstadSimitLanguagePhysical2016}, \textbf{Ebb}~\cite{bernsteinEbbDSLPhysical2016}, and \textbf{MeshTaichi}~\cite{yuMeshTaichiCompilerEfficient2022} are DSLs for mesh stencil computations embedded in Scala, \cplusplus, Lua, and Python respectively.
Each provides a rich API for expressing a variety of loop constructs and stencil operations that may be compiled and executed on a selection of different backends.
Liszt has support for shared-memory, distributed-parallel, and GPU backends whereas Simit, Ebb, and MeshTaichi only support shared-memory and GPUs.

In contrast with these expressive DSLs, \textbf{OP2}~\cite{mudaligeOP2ActiveLibrary2012} and \textbf{PyOP2}~\cite{rathgeberPyOP2HighLevelFramework2012} are mesh stencil packages with much more restrictive interfaces.
Rather than providing a full problem-solving environment, OP2 and PyOP2 programs have a single entrypoint, termed a \textit{parallel loop}, that takes an externally provided local kernel along with an \textit{iteration set} and directly or indirectly addressed \textit{arguments}.
This approach maps cleanly to FEM-style assembly loops such as that shown in \cref{alg:fem_assembly}, which in PyOP2 would be expressed as

\begin{pyinline}
  par_loop(local_kernel, cells, coefficient(cell_node_map, READ), output((cell_node_map, cell_node_map), INC))
\end{pyinline}

\noindent
OP2 is an \textit{active library} that provides source-to-source translation from C, \cplusplus or Fortran to a range of different backends.
By contrast, PyOP2 is a reimplementation of the OP2 API in Python, using run-time code generation to compile the parallel loops.
By using code generation, PyOP2 is easier to integrate with other code generation packages; in particular it is a core component of the Firedrake finite element framework~\cite{FiredrakeUserManual}, which is written in Python.

Of the packages that support distributed-memory parallelism, OP2 and PyOP2 differ from Liszt in that their abstractions have no native mesh type; instead dealing in more abstract \textit{sets} and \textit{mappings} between sets, delegating actual topological considerations to other packages such as PETSc DMPlex (\cref{sec:foundations_dmplex}).
Being able to do this is convenient as building a mature unstructured mesh package is difficult, making the development and maintenance of the stencil package harder.

\subsection{The missing abstraction}

% TODO: make this consistent, and add "..."
\begin{figure}
  \centering
  \includegraphics{scott_vogelius_element_dof_layout_topological.pdf}
  \caption{
    The closest possible data layout for \cref{fig:scott_vogelius_element_dof_layout} for a library that associates unknowns with topological entities.
    Data for each topological entity are stored in separate arrays.
  }
  \label{fig:scott_vogelius_element_dof_layout_topological}
\end{figure}

\begin{figure}
  \centering
  \includegraphics{scott_vogelius_element_dof_layout_pyop2.pdf}
  \caption{
    The data layout matching \cref{fig:scott_vogelius_element_dof_layout} as it would be stored by \pyop2.
    Data for each function space ($V_h$ and $Q_h$) are stored in separate arrays.
  }
  \label{fig:scott_vogelius_element_dof_layout_pyop2}
\end{figure}


All of the packages listed above follow one of two possible approaches to storing mesh data.
Liszt, Ebb, and MeshTaichi all store mesh data by associating fields with mesh entities (e.g. vertices).
This means that high-order function spaces that store DoFs on multiple entities (e.g. \cref{fig:scott_vogelius_element}) are difficult to express and would have to be stored in a manner similar to that shown in \cref{fig:scott_vogelius_element_dof_layout_topological}.
Consequently, mesh renumbering data locality optimisations are impossible to express.

By contrast, Simit, OP2, and PyOP2 are able to represent function space DoFs more generically as a \textit{set of nodes}.
Being able to treat all mesh entities equivalently means that mesh renumbering is possible.
However, this comes at the expense of topological information.
By associating DoFs with \textit{nodes} instead of mesh points the system no longer knows to which mesh point a given node is related.
% TODO: HERE!!! Need to have a stronger argument as to why composability is necessary. Are there other benefits?
% I guess that I have produced a language *as elegant as Simit etc* (no offset maps at all) but with locality stuff too.
% As a result, (bad composability, etc, how to say?)

% I mean, OP2 looks great! but bad composability... like pyop2
% also OP2 is clunky to use.

% Not flexible:
  % "Whilst appropriate for a great many operations, there are occasions where one needs to be able to execute multiple kernels or have nested loops - for example when developing certain types of preconditioners (e.g. \cite{gibsonSlateExtendingFiredrake2020}, \cite{farrellPCPATCHSoftwareTopological2021}).
  % "In these cases one has to extend the compiler in a sui-generis manner to achieve the desired result, resulting in code that is harder to maintain and not composable with other features.

% Further, the treatment of data as belonging to sets can lead to information being discarded.
% Considering the data layout shown in \cref{fig:scott_vogelius_element_dof_layout}, \pyop2 would store it as a tuple of two \textit{node sets} (\cref{fig:scott_vogelius_element_dof_layout_pyop2}).
% Compared with the ``lossless" representation in \cref{fig:scott_vogelius_element_dof_layout}, this approach simplifies the implementation because the data can now be stored as 2 rectangular arrays with shapes \pycode{(10, 2)} and \pycode{(6, 1)}.
% However, this approach \textit{discards topological information}: the ``Mesh" layer of the representation is gone.
% This means that the library user has to do the bookkeeping to correctly handle the mappings from mesh entities to nodes.

% This mostly belongs in chapter 3
Clearly, there is something missing here.
The designs of the existing libraries all require that one either use topological information in a simplified way - associating data with particular mesh entities only - or that one take ownership of the data, discarding topological information that is helpful for having a composable abstraction.
To get around this difficulty we have developed a new abstraction for data layouts, termed \textit{axis trees}, that bridges the gap between these worlds.
Axis trees allow the user to describe complex data layouts of the sort shown in \cref{fig:scott_vogelius_element_dof_layout} fully, without needing to discard any of the topological information.
As a convenient side benefit, expressing data layout transformations (\cref{sec:data_layout_transformations}) becomes natural to do.

The axis tree abstraction is included in the new Python library \pyop3.
\pyop3 is a near-total rewrite of \pyop2 that aims to substantially improve its expressivity power and composability.
It has support for distributed memory parallelism and integrates with PETSc.

% NOTE: I don't like including this but I think I have to be honest about this.
% put in a box
Disclaimer:
Whilst all of the functionality of \pyop3 in the following thesis has been tested and shown to work, the time constraints of the PhD mean that \pyop3 is unfinished.
Thus not all of the provided functionality has been merged into the \texttt{main} branch yet.
This is considered very high priority future work for the project.

\section{Thesis outline}

The remainder of this work is structured as follows\dots

\end{document}
