\documentclass[thesis]{subfiles}

\begin{document}

\chapter{Demonstrator applications}
\label{chapter:demonstrator_applications}

In order to demonstrate \pyop3 as a worthy successor to \pyop2 inside the Firedrake framework, we now present a number of examples in detail that compare the old and new software stacks in terms of both functionality and performance.
In particular, we consider a prototypical finite element assembly problem, that runs under \pyop3 without modification, and a more complex slope limiter computation, that \pyop3 significantly facilitates the development of.

\subsection{Performance metrics}

\begin{figure}
  \centering
  \input{roofline.pgf}
  \caption{
    An example roofline plot.
    Being strongly memory-bound, the demonstrator applications of this chapter will be to the left of this figure where peak performance is limited by the memory bandwidth.
    Note that additional lines for the bandwidths of the different cache levels, as well as one for vectorised instructions, have been omitted for simplicity.
  }
  \label{fig:roofline}
\end{figure}

Analysing the performance of OP2-like mesh stencil codes is challenging because, since the specification of the local kernel is left to the user, the framework itself does no computation.
For such a package to have `good' performance, therefore, it must simply be able to \emph{efficiently transfer data in and out of the local kernel}.
Since the majority of mesh stencil computations are memory-bound, doing this efficiently can be critical to achieving good performance.

A suitable technique for quantifying memory performance is to produce a \emph{roofline plot}.
Shown in \cref{fig:roofline}, a roofline plot compares \emph{arithmetic intensity}, defined as
\begin{equation*}
  \textnormal{arithmetic intensity (FLOP/byte)} = \frac{\textnormal{operation count (FLOPs)}}{\textnormal{memory volume (bytes)}},
\end{equation*}
against computational throughput.
Roofline plots are useful because they provide theoretical bounds for the maximum attainable performance.
Given a computation with a high arithmetic intensity, performing many floating-point operations per byte of data loaded, the performance will be limited by the rate at which these FLOPs may be executed - we say that it is \emph{compute-bound}.
By contrast, computations with low arithmetic intensities are limited by the rate at which data may be streamed from main memory - these we call \emph{memory-bound}.
Since the demonstrator applications are strongly memory-bound, we want our floating-point throughput to be as close to the roofline as possible, where the limit is imposed by the peak memory bandwidth.

\subsubsection{Hardware}

\begin{table}
  \centering
  \begin{tblr}{|[1pt]l|[1pt]l|[1pt]}
    \hline[1pt]
    \textbf{Physical cores} & 32 \\
    \hline[1pt]
    \textbf{Base frequency} & 3.6 GHz \\
    \hline[1pt]
    \textbf{L1 cache} & $32 \times 32$ kB \\
    \hline[1pt]
    \textbf{L2 cache} & $32 \times 512$ kB \\
    \hline[1pt]
    \textbf{L3 cache} & $4 \times 32$ MB \\
    \hline[1pt]
    \textbf{System memory} & $4 \times 16$ GB DDR4 3200 MT/s\footnote{Only half of the DIMMs are populated so only half of the memory channels are available.} \\
    \hline[1pt]
  \end{tblr}
  \caption{Details of the hardware used to run the experiments.}
  \label{tab:pyramus_specs}
\end{table}

\begin{figure}
  \includegraphics{experiments/bandwidth/bandwidth.pdf}
  \caption{TODO}
  \label{fig:pyramus_bandwidth_flops}
\end{figure}

Experiments were be performed on a workstation with a single AMD Ryzen Threadripper PRO 5975WX processor.
%  (specs in table)

% discuss memory, flop figure
% * made from likwid
% * increasing cores quickly saturates memory channels, more cores != more memory bandwidth
% * but FLOP throughput increases much more rapidly

\section{One form assembly}

% NOTE: code is unchanged from PyOP2

% NOTE: FLOP count measured with likwid

\begin{listing}
  \centering
  \caption{
    Firedrake code to assemble the one form of \cref{eq:app1_form} in 2D with $P_3$ elements and 2 coefficients.
  }
  \begin{minipage}{.9\textwidth}
    \begin{pyalg2}
      from firedrake import *

      mesh = UnitSquareMesh(200, 200)
      V = FunctionSpace(mesh, "P", 3)

      coeff1 = Function(V)
      coeff2 = Function(V)
      v = TestFunction(V)
      form = (coeff1 + coeff2) * v * dx

      cofunction = assemble(form)
    \end{pyalg2}
  \end{minipage}
  \label{listing:app1_code}
\end{listing}

% As a first example of \pyop3 in practice we consider ...

% the job of pyop3 is to deliver data efficiently to the local kernel

When assembling a finite element form there are effectively three possible performance bottlenecks:
(a) the computational cost of the local kernel,
(b) the cost of sparse matrix insertion, or
(c) the cost of streaming data to and from the local kernel.
Of these, \pyop3 only influences the performance of (c); the local kernel is provided externally and matrix insertion is managed by PETSc.
As a result, to measure \pyop3's performance for finite element assembly we want to choose a `simple' form with low arithmetic intensity, and to assemble vectors instead of matrices.

With these constraints in mind we therefore assemble the one form
\begin{equation}
  \label{eq:app1_form}
  \int \sum_i^N f_i v \textnormal{d}\Omega.
\end{equation}
where $f_i$ a set of $N$ coefficients and $v$ a test function.

By restricting the degree of the finite elements in use the arithmetic intensity of the local kernel will be low and the overall stencil operation will be strongly memory bound.
As such, performance is limited by the rate at which data can be transferred between main memory and the local kernel, exactly what \pyop3 is supposed to facilitate.
Ideally, we want \pyop3 to deliver a significant fraction of the available memory bandwidth.

% details about the experiment
Firedrake code for assembling such a form is shown in \cref{listing:app1_code}.

% NOTE: mesh size of 200*200 (triangles) was chosen as fewer elements led to significant cache effects and 200 was sufficient for this effect to diminish

% number of FLOPs measured with LIKWID (cite)

\begin{figure}
  \centering
  \includegraphics{experiments/one_form_assembly/roofline1.pdf}
  \caption{TODO}
  \label{fig:app1_roofline1}
\end{figure}

\begin{figure}
  \centering
  \includegraphics{experiments/one_form_assembly/roofline2.pdf}
  \caption{TODO}
  \label{fig:app1_roofline2}
\end{figure}

% (what we varied)
% * can compare with and without additional map payload
% * we can assume optimal caching by computing the streamed memory by adding the
%   number of DoFs together, could also have a pessimal case where we just multiply the local
%   kernel by the number of cells.
% * we can play with number of functions and degree to shift the AI

% The results are shown in ...
% * show a plot of FLOP/s vs AI and demonstrate near roofline

% NOTE: show varying the degree first, then can vary the number of functions (with fixed degree)
% the example shown is a reasonable "worst case" as little data movement is happening and little computation

% commentary:
% * performance is 3x slower for pyop3 than pyop2 in the many-funcs case. Looking at the generated code the only
%   differences are that we have this double indirection and we are also passing in more arguments than needed (a bug)
% * ultimately the code should be near-identical for them both, so performance should approach pyop2.

% *** the roofline shows that we are compute-bound, but only because single core. If bandwidth is shared, the slope will be the same but the top lines will shift up by x64. thus memory bound.
% the main observation to make here is that we are near the roofline, and near PyOP2 (cite roofline paper that shows that we are OK).

% \cref{fig:pyramus_bandwidth_flops} shows that the roofline for multi-core is much much higher but the memory bandwidth is much the same. We are therefore memory-bound in the parallel case.

\section{Vertex-based slope limiter}

\todo[inline]{This is perfectly possible to do with a loop over cells, but our approach is more general.}

\begin{figure}
  \centering
  \includegraphics[scale=2]{slope_limiter.pdf}
  \caption{
    Stencil pattern for a slope limiter.
    The vertex value is set to the maximum value of the surrounding cells.
    Diamond-shaped and circular DoFs represent the $P_0^\textnormal{disc}$ (input) and $P_1$ (output) functions respectively.
  }
  \label{fig:slope_limiter_stencil}
\end{figure}

\begin{algorithm}
  \caption{
    Slope limiter algorithm.
  }
  %
  \begin{algorithmic}[1]
    \Require \textit{cg}, \textit{dg} \Comment{Input and output functions}

    \For{\textit{vertex} \textbf{in} \textit{mesh.vertices}}
      \For{\textit{cell} \textbf{in} $\textnormal{\it vertex2cells}(\textnormal{\it vertex})$} \Comment{Loop over incident cells}
        \State \textit{cg}[\textit{vertex}] $\gets$ \Call{Max}{\textit{cg}[\textit{vertex}],\textit{dg}[\textit{cell}]}
      \EndFor
    \EndFor
  \end{algorithmic}
  \label{alg:slope_limiter}
\end{algorithm}

For our other demonstrator application we implement part of a vertex-based slope limiter.
Slope limiters are postprocessing filters that bound solutions to prevent spurious oscillations near discontinuities~\cite{biswasParallelAdaptiveFinite1994}.
They are useful for solving problems with discontinuous finite element methods where it is important to ensure conservation of particular physical quantities.
Following the explanation in \cite{kuzminVertexbasedHierarchicalSlope2010}, slope-limiting a solution ($u_h$) means redefining it using the cell-averaged solution ($u_c$) to
\begin{equation}
  u_h(\mathbf{x}) = u_c + \alpha_e(\nabla u)_c \cdot (\mathbf{x} - \mathbf{x}_c), \quad 0 \leq \alpha_e \leq 1 .
\end{equation}
Crucially for our purposes, the correction factor $\alpha_e$ depends on both the minimum ($u_i^{\textnormal{min}}$) and maximum ($u_i^{\textnormal{max}}$) values of $u_h$ around each vertex.

If we restrict our attention to only the lowest-order discontinuous space, $P_0$, the algorithm for determining $u_i^{\textnormal{min}}$ and $u_i^{\textnormal{max}}$ is simple.
Shown in \cref{alg:slope_limiter,fig:slope_limiter_stencil}, one needs to, at each vertex, loop over adjacent cells and set the vertex value, kept in a $P_1$ function space, to the minimum/maximum value.

Despite this conceptual simplicity, implementing such an algorithm using existing Firedrake/\pyop2 functionality is difficult because the map between vertices and incident cells has non-constant arity - the number of cells touching a vertex is variable.
\pyop2 does not support non-constant arity maps and so one must resort to tricks involving multiple loops over cells followed by pointwise divisions.
This is effective, but the code is harder to follow as it no longer resembles the pseudocode of \cref{alg:slope_limiter}.

\begin{listing}
  \centering
  \caption{
    Firedrake and \pyop3 code for a simple slope limiter (\cref{alg:slope_limiter}).
  }
  \begin{minipage}{.9\textwidth}
    \begin{pyalg2}
      from firedrake import *
      import pyop3 as op3

      mesh = UnitSquareMesh(...)
      V_cg = FunctionSpace(mesh, "CG", 1)
      V_dg = FunctionSpace(mesh, "DG", 0)
      cg = Function(V_cg)
      dg = Function(V_dg)

      # this kernel sets the second argument to the maximum of
      # both it and the first argument
      max_kernel = op3.Function(..., [op3.READ, op3.RW])

      op3.loop(v := mesh.vertices.index(),?\label{code:slope_limiter_expr}?
               op3.loop(c := mesh.star(v, k=2).index(),
                        max_kernel(dg.dat[c], cg.dat[v])))
    \end{pyalg2}
  \end{minipage}
  \label{listing:app2_code}
\end{listing}

\todo[inline]{This needs more cleaning up}

\begin{listing}
  \centering
  \begin{minipage}{.9\textwidth}
    \inputminted[linenos]{c}{./experiments/slope_limiter/c_code_tidy.c}
  \end{minipage}
  \caption{TODO}
  \label{listing:app2_codegen}
\end{listing}

% to explain the 4x nested indirection:
% array_4 is the cellnodemap and array_1 is the star map f(v, i).
% since the star map is a ragged array it has layout function offset(i, j) = g[i] +j
% so f(v, i) = f[g[i], j]

By contrast, implementing the algorithm using \pyop3 is straightforward, and achievable in only a handful of lines of code.
The loop expression is shown in \cref{listing:app2_code} (line~\ref{code:slope_limiter_expr}) and is near-identical to the pseudocode.
The star of the vertex ($\plexstar(v)$, \cref{sec:dmplex_queries}) is used to identify adjacent cells with the extra keyword argument `\pycode{k=2}' used to ensure that the returned map only includes cells in the star, skipping the vertex and edges.
The code that is generated from the expression is shown in \cref{listing:app2_codegen}.

\subsection{Performance analysis}

\begin{table}
  \centering
  \begin{tblr}{|[1pt]l|[1pt]l|[1pt]}
    \hline[1pt]
    \textbf{Arithmetic throughput (FLOP/s)} & $6.3 \times 10^8$ \\
    \hline[1pt]
    \textbf{Optimal arithmetic intensity (FLOP/byte)} & 0.25 \\
    \hline[1pt]
    \textbf{Pessimal arithmetic intensity (FLOP/byte)} & 0.094 \\
    \hline[1pt]
    \textbf{Percentage of peak throughput (optimal)} & 8.1 \\
    \hline[1pt]
    \textbf{Percentage of peak throughput (pessimal)} & 22 \\
    \hline[1pt]
  \end{tblr}
\end{table}

% now discuss performance
% * ideally have good roofline behaviour as before, not much else to see

\end{document}
