\documentclass[thesis]{subfiles}

\begin{document}

\chapter{Demonstrator applications}
\label{chapter:demonstrator_applications}

In order to demonstrate \pyop3's capability to replace \pyop2 inside the Firedrake framework, we now present a number of examples comparing the old and new software stacks in terms of both functionality and performance.
In particular, we consider two prototypical finite element problems, that run under \pyop3 without modification, and a more complex slope limiter computation for which \pyop3 significantly facilitates the development of.

\subsubsection{Performance metrics}

\begin{figure}
  \centering
  \input{roofline.pgf}
  \caption{
    An example roofline plot.
    The solid lines represent a performance `ceiling' that all computations must sit somewhere below.
    Note that additional lines for the bandwidths of the different cache levels, as well as one for vectorised instructions, have been omitted for simplicity.
  }
  \label{fig:roofline}
\end{figure}

Analysing the performance of OP2-like mesh stencil codes is challenging because, since the specification of the local kernel is left to the user, the framework itself does no computation.
For such a package to have `good' performance, therefore, it must simply be able to \emph{efficiently transfer data in and out of the local kernel}.
Since the majority of mesh stencil computations are memory-bound, doing this efficiently can be critical to achieving good performance.

A suitable technique for quantifying memory performance is to produce a \emph{roofline plot}.
Shown in \cref{fig:roofline}, a roofline plot compares \emph{arithmetic intensity}, defined as
\begin{equation}
  \textnormal{arithmetic intensity (FLOP/byte)} = \frac{\textnormal{operation count (FLOP)}}{\textnormal{memory volume (byte)}},
\end{equation}
against arithmetic throughput, measured in FLOP/s.
Roofline plots are useful because they provide theoretical bounds for the maximum attainable performance for a computation.
Given a computation with a high arithmetic intensity, performing many floating-point operations per byte of data loaded, performance is limited by the rate at which FLOPs may be executed - we say that it is \emph{compute-bound}.
By contrast, computations with low arithmetic intensities are limited by the rate at which data may be streamed from main memory - these we call \emph{memory-bound}.

\subsubsection{Hardware}

\begin{table}
  \centering
  \begin{tblr}{|[1pt]l|[1pt]l|[1pt]}
    \hline[1pt]
    \textbf{Physical cores} & 32 \\
    \hline[1pt]
    \textbf{Base frequency} & 3.6 GHz \\
    \hline[1pt]
    \textbf{L1 cache} & $32 \times 32$ kB \\
    \hline[1pt]
    \textbf{L2 cache} & $32 \times 512$ kB \\
    \hline[1pt]
    \textbf{L3 cache} & $4 \times 32$ MB \\
    \hline[1pt]
    \textbf{System memory} & $4 \times 16$ GB DDR4 3200 MT/s \\
    \hline[1pt]
    \textbf{Memory bandwidth (single core)} & 31.2 GB/s \\
    \hline[1pt]
    \textbf{Peak scalar throughput (single core)} & $11.9 \times 10^9$ FLOP/s \\
    \hline[1pt]
    \textbf{Peak AVX2 throughput (single core)} & $51.4 \times 10^9$ FLOP/s \\
    \hline[1pt]
  \end{tblr}
  \caption{
    Details of the hardware used to run the experiments.
    Memory bandwidth and arithmetic throughput were measured with LIKWID~\cite{psti}.
    Note that, since the experiments in this chapter were all run on a single core, only single core metrics are included.
  }
  \label{tab:pyramus_specs}
\end{table}

% \begin{figure}
%   \includegraphics{experiments/bandwidth/bandwidth.pdf}
%   \caption{Memory bandwidth (red) and scalar arithmetic throughput (blue) against number of cores.}
%   \label{fig:pyramus_bandwidth_flops}
% \end{figure}

Experiments were performed on a workstation with a single AMD Ryzen Threadripper PRO 5975WX processor.
Its details are summarised in \cref{tab:pyramus_specs}.

% To provide additional context for interpreting the roofline plots, the scaling behaviour of the memory bandwidth and arithmetic throughput with increasing core count were measured using the performance tool LIKWID~\cite{psti}.
% The results are shown in \cref{fig:pyramus_bandwidth_flops}.
% Crucially, what they show is that \emph{as core count increases the throughput increases by a far larger rate than the memory bandwidth}.
% Between 1 and 32 cores the memory bandwidth increases by 13\%, whereas the throughput increases by a factor of 830\%.
% This is because the memory channels are saturated even with low core count whilst the number of compute units increases linearly with the number of cores and so the rate of increase, whilst not linear, is more dramatic.
%
% Given this result, when interpreting roofline plots the number of cores is therefore a significant factor.
% At low core counts the bandwidth is close to its maximum but the throughput is low.
% This means that \emph{applications that are traditionally thought of as memory-bound may present as compute-bound if they are measured on few cores}.

\section{Solving the Helmholtz equation}

Before any investigation of performance, we first demonstrate that Firedrake remains capable of solving PDEs under \pyop3.
To do so, we solve the Helmholtz equation on a unit square using the formulation from \cite{FiredrakeUserManual} (pp. 91-93).
The formulation reads: find $u \in V$ such that
\begin{equation}
  \int \nabla u \cdot \nabla v + uv \textnormal \, {d}\Omega = \int fv\, \textnormal{d}\Omega,
\end{equation}
where $u$ is a trial function, $v$ a test function, and $f$ a known function.
For this example we set
\begin{equation}
  f = (1 + 8 \pi^2)\cos (2\pi x)\cos(2\pi y),
\end{equation}
which results in a known analytic solution of
\begin{equation}
  u = \cos(2\pi x) \cos(2\pi y).
\end{equation}

\begin{listing}
  \centering
  \caption{Firedrake code for solving the Helmholtz equation using degree 3 Lagrange elements.}
  \begin{minipage}{.9\textwidth}
    \begin{pyalg2}
      from firedrake import *

      mesh = UnitSquareMesh(...)
      V = FunctionSpace(mesh, "P", 3)

      u = TrialFunction(V)
      v = TestFunction(V)

      f = Function(V)
      x, y = SpatialCoordinate(mesh)
      f.interpolate((1+8*pi*pi)*cos(x*pi*2)*cos(y*pi*2))

      # bilinear and linear forms
      a = (inner(grad(u), grad(v)) + inner(u, v)) * dx
      L = inner(f, v) * dx

      u = Function(V)  # function to hold the solution
      solve(a == L, u)
    \end{pyalg2}
  \end{minipage}
  \label{listing:helmholtz}
\end{listing}

Code to solve this system using Firedrake is also taken from \cite{FiredrakeUserManual} and is shown in \cref{listing:helmholtz}.

\begin{figure}
  \centering
  \includegraphics{experiments/conv.pdf}
  \caption{
    Convergence of the $L_2$ error against refinement level for polynomial degrees ($p$) 1, 2, and 3 when solving the Helmholtz equation with Firedrake and \pyop3.
  }
  \label{fig:helmholtz_conv}
\end{figure}

To demonstrate correctness the $L_2$ error between the numerical and analytical solutions, given by
\begin{equation}
  \left( \int (u_{\textnormal{numeric}}-u_{\textnormal{analytic}}) \cdot (u_{\textnormal{numeric}}-u_{\textnormal{analytic}}) \, \textnormal{d}\Omega \right)^{1/2},
\end{equation}
was measured against mesh refinement level for polynomial degree 1, 2, and 3.
The resulting convergence behaviour is plotted in \cref{fig:helmholtz_conv}.
The results are identical to those produced by a pre-\pyop3 Firedrake and show the expected convergence of $\sim O(h^{p+1})$~\cite{brennerMathematicalTheoryFinite2008}, with $p=2$ demonstrating even better convergence.
Thus we may safely conclude that Firedrake works as expected under \pyop3.

\section{One form assembly}
\label{sec:demo_apps_one_form_assembly}

\begin{listing}
  \centering
  \caption{
    Firedrake code to assemble the one form of \cref{eq:app1_form} in 2D with $P_3$ elements and 2 coefficients.
  }
  \begin{minipage}{.9\textwidth}
    \begin{pyalg2}
      from firedrake import *

      mesh = UnitSquareMesh(200, 200)
      V = FunctionSpace(mesh, "P", 3)

      coeff1 = Function(V)
      coeff2 = Function(V)
      v = TestFunction(V)
      form = (coeff1 + coeff2) * v * dx

      cofunction = assemble(form)
    \end{pyalg2}
  \end{minipage}
  \label{listing:app1_code}
\end{listing}

For a simpler example where the contribution of \pyop3 may be better analysed we assembled the one form
\begin{equation}
  \label{eq:app1_form}
  \int \sum_i^N f_i v\, \textnormal{d}\Omega,
\end{equation}
with $f_i$ known functions and $v$ a test function.
This form was chosen because it is `simple', so the local kernel evaluation will not dominate the computation time, and to avoid invoking any sparse matrix insertion, which is often a bottleneck.
The majority of the computation time is therefore spent performing pack/unpack operations, which is where the differences between \pyop2 and \pyop3 manifest.
Firedrake code for assembling such a form is shown in \cref{listing:app1_code}.

For this case, `good' performance can be quantified as the generated code hitting a reasonable fraction of peak performance, according to a roofline plot, and also being close in performance to \pyop2.

In order to enable roofline analysis, two methods were used to approximate the total amount of data streamed from main memory.
The first, termed \emph{optimal caching}, assumes that values are only loaded from main memory once, remaining in cache for all subsequent uses.
The other, called \emph{pessimal caching}, does the opposite; values are never in cache so must be loaded from memory at each local kernel invocation.

For optimal caching the amount of data streamed from main memory (in bytes) is given by
\begin{equation}
  \label{eq:optimal_caching}
  \textnormal{dim}(V) \times (N + 1) \times 8 + n_{\textnormal{map}},
\end{equation}
where $\textnormal{dim}(V)$ represents the number of DoFs associated with the function space and the $N+1$ term accounts for the fact that there are $N$ input arrays (one per coefficient) and a single output array.
The factor of 8 is included as the values being streamed are in double precision (8 bytes).
The additional term $n_{\textnormal{map}}$ accounts for the cell-node map that the packing code needs because the mesh is unstructured.
It is given by
\begin{equation}
  n_{\textnormal{map}} = n_{\textnormal{cells}} \times n_{\textnormal{nodes}} \times 4
\end{equation}
where $n_{\textnormal{cells}}$ is the number of cells in the mesh, $n_{\textnormal{nodes}}$ is the number of nodes addressed by the map per iteration, equivalently the size of the packed temporary, and the factor of 4 is included because the map uses 4 byte integers.
For pessimal caching, where the cache is always missed, we expect to load
\begin{equation}
  \label{eq:pessimal_caching}
  n_{\textnormal{cells}} \times (N + 1) \times n_{\textnormal{nodes}} \times 8 + n_{\textnormal{map}}
\end{equation}
bytes.

Due to the various data locality optimisations that \pyop2 and \pyop3 perform we expect that data locality is good, and so optimal caching is likely a better approximation of the true arithmetic intensity.

\begin{figure}
  \centering
  \includegraphics{experiments/one_form_assembly/roofline1_new.pdf}
  \caption{
    Roofline plot comparing the performance of \pyop2 (blue, diamonds) and \pyop3 (red, circles) for one form assembly (\cref{eq:app1_form}) where the polynomial degree is varied between 1 and 7.
    The top and bottom flat lines represent peak vector and scalar throughput respectively.
    As polynomial degree is increased so does arithmetic intensity, and so the points are ordered with lowest degree to the left.
    Entries are plotted as pairs of points to show both pessimal (\cref{eq:pessimal_caching}, left) and optimal (\cref{eq:optimal_caching}, right) caching assumptions.
  }
  \label{fig:app1_roofline1}
\end{figure}

\begin{figure}
  \centering
  \includegraphics{experiments/one_form_assembly/roofline2_new.pdf}
  \caption{
    Roofline plot comparing \pyop2 (blue, diamonds) and \pyop3 (red, circles) performance for one form assembly (\cref{eq:app1_form}) with a varying number of coefficients between 0 and 7.
    Increasing the number of coefficients reduces the arithmetic intensity and so the left-most values are for the 7 coefficient case.
    For the arithmetic intensity the average from the pessimal (\cref{eq:pessimal_caching}) and optimal (\cref{eq:optimal_caching}) caching assumptions was used.
  }
  \label{fig:app1_roofline2}
\end{figure}

\subsubsection{Performance results}

To evaluate the performance of this assembly routine we performed two separate experiments, both run in serial on a $200\times 200$ triangular mesh.
First, the polynomial degree was increased between 1 and 7 for the single coefficient version of \cref{eq:app1_form} ($N=1$).
The results are shown in \cref{fig:app1_roofline1}.
One can see that with increasing degree (increasing arithmetic intensity) the throughput generally increases, eventually become compute-bound.
This is because the computational payload of the local kernel is increasing and so the overhead from pack/unpack operations is amortized.
At higher degree, some of the recorded throughput results surpass the measured scalar peak.
This is because a sizeable number of vector instructions were generated by the compiler for these kernels.

In \cref{fig:app1_roofline2} the polynomial degree was fixed to 1, and the number of coefficients varied between 0 and 7.
As the number of coefficients increases a larger volume of data are loaded with relatively few floating point operations and thus the arithmetic intensity decreases.
Both \pyop2 and \pyop3 show the same trend where increasing the number of functions reduces the arithmetic throughput.
We anticipate that this is caused by the larger working set needed for the computation, though more investigation would be required to validate this.

Both sets of results show that the performance of \pyop3 is competitive, achieving in the majority of cases over 70\% of the arithmetic throughput recorded for \pyop2 as well as operating at close to peak theoretical performance as defined by the roofline.
Since the code generated by \pyop3 is very similar to that of \pyop2 we are confident that, with additional tuning, performance parity is achievable.

One possible source of confusion with both figures is the misleading fact that the kernels are shown to be compute-bound even at low degree.
This only happens because we are running in serial.
At full-node utilisation the total memory bandwidth increases by only a small amount, whilst the total throughput should increase by (approximately) a factor of 32.
Performing these experiments in parallel would therefore lead to a significantly different roofline plot: the peak throughput would be much higher whilst the slope of the memory bandwidth would stay approximately the same.
Shifting the lines like this moves the inflection point between memory-bound and compute-bound further to the right, which would move most of the points into the memory-bound regime.

\section{Vertex-based slope limiter}
\label{sec:demo_apps_slope_limiter}

\begin{figure}
  \centering
  \includegraphics[scale=2]{slope_limiter.pdf}
  \caption{
    Stencil pattern for a slope limiter.
    The vertex value is set to the maximum value of the surrounding cells.
    Diamond-shaped and circular DoFs represent the $P_0^\textnormal{disc}$ (input) and $P_1$ (output) functions respectively.
  }
  \label{fig:slope_limiter_stencil}
\end{figure}

\begin{algorithm}
  \caption{
    Algorithm that writes the maximum values from a $P_0^\textnormal{disc}$ function (\textit{dg}) to a $P_1$ function (\textit{cg}) around each vertex.
    Values in \textit{cg} are assumed to be initialised to a sufficiently large negative number that \textsc{Max} always overwrites the initial value.
  }

  \begin{algorithmic}[1]
    \Require \textit{cg}, \textit{dg} \Comment{Input and output functions}

    \For{\textit{vertex} \textbf{in} \textit{mesh.vertices}}
      \For{\textit{cell} \textbf{in} $\textnormal{\it vertex2cells}(\textnormal{\it vertex})$} \Comment{Loop over incident cells}
        \State \textit{cg}[\textit{vertex}] $\gets$ \Call{Max}{\textit{cg}[\textit{vertex}],\textit{dg}[\textit{cell}]}
      \EndFor
    \EndFor
  \end{algorithmic}
  \label{alg:slope_limiter}
\end{algorithm}

As a final demonstrator application we implement part of a slope limiter algorithm.
Slope limiters are postprocessing filters that bound solutions to prevent spurious oscillations near discontinuities~\cite{biswasParallelAdaptiveFinite1994}.
Following the explanation in \cite{kuzminVertexbasedHierarchicalSlope2010}, one applies a slope limiter by correcting some function $u$ with a scalar correction factor $\alpha_e$ to
\begin{equation}
  u(\mathbf{x}) = u_c + \alpha_e(\nabla u)_c \cdot (\mathbf{x} - \mathbf{x}_c), \quad 0 \leq \alpha_e \leq 1 ,
\end{equation}
where $(\cdot)_c$ denote cell-averaged quantities and $\mathbf{x}$ the coordinates.
For our demonstrator application we are interested in determining the value of the correction factor $\alpha_e$, which depends on both the minimum ($u^{\textnormal{min}}$) and maximum ($u^{\textnormal{max}}$) values of $u$ around each vertex.

\begin{listing}
  \centering
  \caption{
    Firedrake and \pyop3 code for a simple slope limiter (\cref{alg:slope_limiter}).
  }
  \begin{minipage}{.9\textwidth}
    \begin{pyalg2}
      from firedrake import *
      import pyop3 as op3

      mesh = UnitSquareMesh(200, 200)
      V_cg = FunctionSpace(mesh, "CG", 1)
      V_dg = FunctionSpace(mesh, "DG", 0)
      cg = Function(V_cg)
      dg = Function(V_dg)

      # this kernel sets the second argument to the maximum of
      # both it and the first argument
      max_kernel = op3.Function(..., [op3.READ, op3.RW])

      op3.loop(v := mesh.vertices.index(),?\label{code:slope_limiter_expr}?
               op3.loop(c := mesh.star(v, k=2).index(),
                        max_kernel(dg.dat[c], cg.dat[v])))
    \end{pyalg2}
  \end{minipage}
  \label{listing:app2_code}
\end{listing}

If we restrict our attention to only the lowest-order discontinuous space, $P_0^{\textnormal{disc}}$, the algorithm for determining both $u^{\textnormal{min}}$ and $u^{\textnormal{max}}$ is simple.
Shown in \cref{alg:slope_limiter,fig:slope_limiter_stencil}, one needs to visit each vertex, loop over adjacent cells, and set the vertex value, kept in a $P_1$ function space, to the minimum/maximum value from the surrounding cells.

Implementing the algorithm is straightforward in \pyop3 and can be done in only a few lines (\cref{listing:app2_code}, line~\ref{code:slope_limiter_expr}).
To identify the cells surrounding the current vertex a star ($\plexstar$, \cref{sec:dmplex_queries}) map is used.
The extra keyword argument \pycode{k=2} is passed to ensure that the map only includes the cells in the star, skipping the vertex and edges.

\begin{listing}
  \centering
  \begin{minipage}{.9\textwidth}
    \inputminted[linenos,escapeinside=??]{c}{./experiments/slope_limiter/c_code_tidy.c}
  \end{minipage}
  \caption{
    (Abbreviated) C code generated from the loop expression in \cref{listing:app2_code}.
    To better show the code's behaviour the nested indirections have not been optimised away.
  }
  \label{listing:app2_codegen}
\end{listing}

The code that is generated from the loop expression is shown in \cref{listing:app2_codegen}.
Notice that the inner loop is ragged (line~\ref{code:ragged_loop}) with extent set by the variable \ccode{p_0}, drawn from an externally provided array \ccode{array_0} (line~\ref{code:ragged_cell_loop}).
We also remark that the index expression for packing the $P_0^{\textnormal{disc}}$ cell data (line~\ref{code:messy_expr})
\begin{cinline}
  array_3[array_4[array_1[array_2[i_0] + i_1]]]
\end{cinline}
is relatively complex, illustrating \pyop3's ability to compose advanced index expressions.

As written, \pyop2 is not capable of expressing this type of operation because the map from vertices to cells has variable arity.
One can reframe the algorithm as a loop over cells, writing the maximum values to the vertices in the cell's closure, but that is specific to this example; more complex algorithms like vertex-centered patch smoothers~\cite{farrellPCPATCHSoftwareTopological2021} cannot be expressed in this way.
\pyop3 therefore provides a more general solution.

\subsection{Performance results}

\begin{table}
  \centering
  \begin{tblr}{|[1pt]l|[1pt]l|[1pt]}
    \hline[1pt]
    \textbf{Arithmetic throughput} & $1.9 \times 10^9$ FLOP/s \\
    \hline[1pt]
    \textbf{Arithmetic intensity} & 0.072 FLOP/byte \\
    \hline[1pt]
    \textbf{Peak throughput} & $2.2 \times 10^9$ FLOP/s \\
    \hline[1pt]
    \textbf{Percentage of peak throughput} & 85\% \\
    \hline[1pt]
  \end{tblr}
  \caption{
    Roofline results from running the slope limiter code (\cref{listing:app2_code}).
    The arithmetic intensity was measured by LIKWID.
    Peak throughput was computed as the product $\textnormal{\it arithmetic intensity} \times \textnormal{\it memory bandwidth}$.
  }
  \label{tab:slope_limiter_performance}
\end{table}

Lacking independent variables to change, the code was profiled for a single configuration, again in serial.
The roofline results are shown in \cref{tab:slope_limiter_performance}.
The arithmetic intensity is extremely low because only one floating point operation (a comparison) is performed, with many arrays being streamed from memory.
The calculation is therefore strongly memory-bound.

Irrespective of this, the code achieves a good fraction of theoretical peak performance, meaning that \pyop3 is clearly generating performant code that makes effective use of the available hardware.

\section{Outlook}

This chapter demonstrates that \pyop3 has the capability to be used to solve real-world mesh stencil problems.
Integration with Firedrake is sufficiently complete that PDEs may be solved, and one can even express new types of stencil computations not previously expressible with \pyop2.

In terms of performance, we consider the results to be encouraging.
The performance of the code that \pyop3 generates is both comparable to \pyop2 and sufficiently close to theoretical peak to suggest that the approach is sound.
The slope limiter results additionally show that reasonable performance is attainable even for circumstances not expressible with \pyop2.

\end{document}
