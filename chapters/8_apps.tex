\documentclass[thesis]{subfiles}

\begin{document}

\chapter{Demonstrator applications}
\label{chapter:demonstrator_applications}

In order to demonstrate \pyop3 as a worthy successor to \pyop2 inside the Firedrake framework, we now present a number of examples in detail that compare the old and new software stacks in terms of both functionality and performance.
In particular, we consider a prototypical finite element assembly problem, that runs under \pyop3 without modification, and a more complex slope limiter computation, that \pyop3 significantly facilitates the development of.

\subsection{Performance metrics}

\begin{figure}
  \centering
  \input{roofline.pgf}
  \caption{
    An example roofline plot.
    Note that additional lines for the bandwidths of the different cache levels, as well as one for vectorised instructions, have been omitted for simplicity.
  }
  \label{fig:roofline}
\end{figure}

Analysing the performance of OP2-like mesh stencil codes is challenging because, since the specification of the local kernel is left to the user, the framework itself does no computation.
For such a package to have `good' performance, therefore, it must simply be able to \emph{efficiently transfer data in and out of the local kernel}.
Since the majority of mesh stencil computations are memory-bound, doing this efficiently can be critical to achieving good performance.

A suitable technique for quantifying memory performance is to produce a \emph{roofline plot}.
Shown in \cref{fig:roofline}, a roofline plot compares \emph{arithmetic intensity}, defined as
\begin{equation*}
  \textnormal{arithmetic intensity (FLOP/byte)} = \frac{\textnormal{operation count (FLOP)}}{\textnormal{memory volume (byte)}},
\end{equation*}
against computational throughput.
Roofline plots are useful because they provide theoretical bounds for the maximum attainable performance.
Given a computation with a high arithmetic intensity, performing many floating-point operations per byte of data loaded, the performance will be limited by the rate at which these FLOPs may be executed - we say that it is \emph{compute-bound}.
By contrast, computations with low arithmetic intensities are limited by the rate at which data may be streamed from main memory - these we call \emph{memory-bound}.

\subsubsection{Hardware}

\begin{table}
  \centering
  \begin{tblr}{|[1pt]l|[1pt]l|[1pt]}
    \hline[1pt]
    \textbf{Physical cores} & 32 \\
    \hline[1pt]
    \textbf{Base frequency} & 3.6 GHz \\
    \hline[1pt]
    \textbf{L1 cache} & $32 \times 32$ kB \\
    \hline[1pt]
    \textbf{L2 cache} & $32 \times 512$ kB \\
    \hline[1pt]
    \textbf{L3 cache} & $4 \times 32$ MB \\
    \hline[1pt]
    \textbf{System memory} & $4 \times 16$ GB DDR4 3200 MT/s\footnote{Only half of the DIMMs are populated so only half of the memory channels are available.} \\
    \hline[1pt]
  \end{tblr}
  \caption{Details of the hardware used to run the experiments.}
  \label{tab:pyramus_specs}
\end{table}

\begin{figure}
  \includegraphics{experiments/bandwidth/bandwidth.pdf}
  \caption{Memory bandwidth (red) and scalar arithmetic throughput (blue) against number of cores.}
  \label{fig:pyramus_bandwidth_flops}
\end{figure}

Experiments were performed on a workstation with a single AMD Ryzen Threadripper PRO 5975WX processor.
Its details are summarised in \cref{tab:pyramus_specs}.

To provide additional context for interpreting the roofline plots, the scaling behaviour of the memory bandwidth and arithmetic throughput with increasing core count were measured using the performance tool LIKWID~\cite{psti}.
The results are shown in \cref{fig:pyramus_bandwidth_flops}.
Crucially, what they show is that \emph{as core count increases the throughput increases by a far larger rate than the memory bandwidth}.
Between 1 and 32 cores the memory bandwidth increases by 13\%, whereas the throughput increases by a factor of 830\%.
This is because the memory channels are saturated even with low core count whilst the number of compute units increases linearly with the number of cores and so the rate of increase, whilst not linear, is more dramatic.

Given this result, when interpreting roofline plots the number of cores is therefore a significant factor.
At low core counts the bandwidth is close to its maximum but the throughput is low.
This means that \emph{applications that are traditionally thought of as memory-bound may present as compute-bound if they are measured on few cores}.

\section{One form assembly}
\label{sec:demo_apps_one_form_assembly}

\begin{listing}
  \centering
  \caption{
    Firedrake code to assemble the one form of \cref{eq:app1_form} in 2D with $P_3$ elements and 2 coefficients.
  }
  \begin{minipage}{.9\textwidth}
    \begin{pyalg2}
      from firedrake import *

      mesh = UnitSquareMesh(200, 200)
      V = FunctionSpace(mesh, "P", 3)

      coeff1 = Function(V)
      coeff2 = Function(V)
      v = TestFunction(V)
      form = (coeff1 + coeff2) * v * dx

      cofunction = assemble(form)
    \end{pyalg2}
  \end{minipage}
  \label{listing:app1_code}
\end{listing}

\todo[inline]{I don't like this phrasing very much.}

As a first example of using \pyop3 in practice we consider the typical case of finite element assembly.
In order to evaluate the performance of \pyop3 versus \pyop2 we profile the assembly of the one form
\begin{equation}
  \label{eq:app1_form}
  \int \sum_i^N f_i v \textnormal{d}\Omega.
\end{equation}
The form was chosen because it is `simple', and so the local kernel evaluation will not dominate the computation time, and because sparse matrix insertion, which is often a bottleneck, is avoided.
A large chunk of the computation time will therefore be spent performing pack/unpack operations, which is where the differences between \pyop2 and \pyop3 manifest.

`Good' performance can be quantified as the generated code hitting a reasonable fraction of peak performance, according to a roofline plot, and also being close in performance to \pyop2.
It is not expected for \pyop3 to perform equivalently to \pyop2 because the optimisations from \cref{sec:codegen_optimisation} have not yet been implemented.
In particular the maps used in \pyop2 have already been `compressed' (\cref{sec:pyop3_expression_compression}) leading to better data locality and reducing the amount of data that needs streaming from memory.

Firedrake code for assembling such a form is shown in \cref{listing:app1_code}.
PETSc logging infrastructure was used to time the generated code.

In order to enable roofline analysis, two approximations were used for the total amount of data streamed from main memory.
The first, termed \emph{optimal caching}, assumes that values are only loaded from main memory once, remaining in cache for all subsequent uses.
The other, called \emph{pessimal caching}, does the opposite; values are never in cache so must be loaded from memory at each local kernel invocation.

For optimal caching the amount of data streamed from main memory (in bytes) is given by
\begin{equation*}
  \textnormal{dim}(V) \times (N + 1) \times 8 + n_{\textnormal{map}}
\end{equation*}
where $\textnormal{dim}(V)$ represents the number of DoFs associated with the function space and there are $N$ input arrays (one per coefficient) and a single output array.
The factor of 8 is included as the values being streamed are in double precision.
The additional term $n_{\textnormal{map}}$ accounts for the cell-node map that the packing code needs because the mesh is unstructured.
It is given by
\begin{equation*}
  n_{\textnormal{map}} = n_{\textnormal{cells}} \times n_{\textnormal{nodes}} \times 4
\end{equation*}
where $n_{\textnormal{cells}}$ is the number of cells in the mesh, $n_{\textnormal{nodes}}$ is the number of nodes addressed by the map per iteration, equivalently the size of the packed temporary, and the factor of 4 is included because the map uses 4 byte integers.

For pessimal caching, where the cache is always missed, we expect to load
\begin{equation*}
  n_{\textnormal{cells}} \times (N + 1) \times n_{\textnormal{nodes}} \times 8 + n_{\textnormal{map}}
\end{equation*}
bytes.

Due to the various data locality optimisations that \pyop2/\pyop3 performs we would expect that data locality is good, and so the true arithmetic intensity lies closer to the optimal caching approximation.
As an aside, we remark that, under the optimal caching assumption, the cell-node map constitutes a non-trivial amount of the streamed data, demonstrating the potential benefit of using a structured mesh.

\begin{figure}
  \centering
  \includegraphics{experiments/one_form_assembly/roofline1.pdf}
  \caption{
    Roofline plot comparing the performance of \pyop2 (blue, diamonds) and \pyop3 (red, circles) for one form assembly where the polynomial degree is varied between 1 and 7.
    The top and bottom flat lines represent peak vector and scalar throughput respectively.
    As polynomial degree is increased so does arithmetic intensity, and so the points are ordered with lowest degree to the left.
    Entries are plotted as pairs of points to show both pessimal (left) and optimal (right) caching assumptions.
  }
  \label{fig:app1_roofline1}
\end{figure}

\begin{figure}
  \centering
  \includegraphics{experiments/one_form_assembly/roofline2.pdf}
  \caption{
    Roofline plot comparing \pyop2 (blue, diamonds) and \pyop3 (red, circles) performance for one form assembly with a varying number of coefficients between 0 and 7.
    Increasing the number of coefficients reduces the arithmetic intensity and so the left-most values are for the 7 coefficient case.
    For the arithmetic intensity the average from the pessimal and optimal caching assumptions was used.
  }
  \label{fig:app1_roofline2}
\end{figure}

\subsection{Performance results}

Performance results are shown in the roofline plots of \cref{fig:app1_roofline1,fig:app1_roofline2}.
Both experiments were run in serial with a $200 \times 200$ triangular mesh.

In \cref{fig:app1_roofline1}, the polynomial degree was increased between 1 and 7 for the single coefficient version of \cref{eq:app1_form} ($N=1$).
With increasing degree (increasing arithmetic intensity) the throughput generally increases, eventually become compute-bound.
At higher degree, some of the recorded throughput results surpass the measured scalar peak.
This is because a sizable number of vector instructions were generated by the compiler for these kernels.

In \cref{fig:app1_roofline2} the polynomial degree was fixed to 1, and the number of functions ($N$) varied between 0 and 7.
As the number of functions increases a larger amount of data are loaded with relatively few floating point operations and thus the arithmetic intensity decreases.

A possible source of confusion with both figures is the misleading fact that we are compute-bound even at low degree.
This only happens because we are running in serial.
As shown in \cref{fig:pyramus_bandwidth_flops}, with increasing parallelism the bandwidth does not increase by much, but the arithmetic throughput does increase dramatically.
At full-node utilisation therefore the roofline plot would look quite different: the peak throughput lines would be much higher whilst the slope of the memory bandwidth would stay approximately the same.
Shifting the lines like this would move the transition point from memory-bound to compute-bound further to the right, shifting most of the points into the memory-bound regime.

Both sets of results show that at lower arithmetic intensities the throughput of \pyop3 lags \pyop2 by approximately a factor of 3.
This corresponds with our earlier hypothesis: the additional indirections coming from not `compressing' the cell-node maps inhibits data locality.
With such an optimisation the code generated by \pyop3 will be near-identical to that made by \pyop2 and the performance will be restored.

Ultimately, we consider these results to be encouraging.
\pyop3 has comparable performance to \pyop2, with a clear roadmap towards achieving equivalent performance, and performance is also near to the roofline, suggesting that the generated code is working as expected and competetive with hand-written alternatives.

\section{Vertex-based slope limiter}
\label{sec:demo_apps_slope_limiter}

\begin{figure}
  \centering
  \includegraphics[scale=2]{slope_limiter.pdf}
  \caption{
    Stencil pattern for a slope limiter.
    The vertex value is set to the maximum value of the surrounding cells.
    Diamond-shaped and circular DoFs represent the $P_0^\textnormal{disc}$ (input) and $P_1$ (output) functions respectively.
  }
  \label{fig:slope_limiter_stencil}
\end{figure}

\begin{algorithm}
  \caption{
    Algorithm that writes the maximum values from a $P_0^\textnormal{disc}$ function (\textit{dg}) to a $P_1$ function (\textit{cg}) around each vertex.
    The \textit{cg} function is assumed to be initialised to a sufficiently large negative number that \textsc{Max} always overwrites the initial value.
  }

  \begin{algorithmic}[1]
    \Require \textit{cg}, \textit{dg} \Comment{Input and output functions}

    \For{\textit{vertex} \textbf{in} \textit{mesh.vertices}}
      \For{\textit{cell} \textbf{in} $\textnormal{\it vertex2cells}(\textnormal{\it vertex})$} \Comment{Loop over incident cells}
        \State \textit{cg}[\textit{vertex}] $\gets$ \Call{Max}{\textit{cg}[\textit{vertex}],\textit{dg}[\textit{cell}]}
      \EndFor
    \EndFor
  \end{algorithmic}
  \label{alg:slope_limiter}
\end{algorithm}

For our other demonstrator application we implement part of a slope limiting algorithm.
Slope limiters are postprocessing filters that bound solutions to prevent spurious oscillations near discontinuities~\cite{biswasParallelAdaptiveFinite1994}.
They are useful for solving problems with discontinuous finite element methods where it is important to ensure conservation of particular physical quantities.
Following the explanation in \cite{kuzminVertexbasedHierarchicalSlope2010}, one applies a slope limiter by correcting a function with some scalar correction factor $\alpha_e$ to
\begin{equation}
  u_h(\mathbf{x}) = u_c + \alpha_e(\nabla u)_c \cdot (\mathbf{x} - \mathbf{x}_c), \quad 0 \leq \alpha_e \leq 1 ,
\end{equation}
where $\cdot_c$ denote cell-averaged quantities.
For our demonstrator application we are interested in determining the value of the correction factor $\alpha_e$, which depends on both the minimum ($u_i^{\textnormal{min}}$) and maximum ($u_i^{\textnormal{max}}$) values of $u_h$ around each vertex.

\begin{listing}
  \centering
  \caption{
    Firedrake and \pyop3 code for a simple slope limiter (\cref{alg:slope_limiter}).
  }
  \begin{minipage}{.9\textwidth}
    \begin{pyalg2}
      from firedrake import *
      import pyop3 as op3

      mesh = UnitSquareMesh(...)
      V_cg = FunctionSpace(mesh, "CG", 1)
      V_dg = FunctionSpace(mesh, "DG", 0)
      cg = Function(V_cg)
      dg = Function(V_dg)

      # this kernel sets the second argument to the maximum of
      # both it and the first argument
      max_kernel = op3.Function(..., [op3.READ, op3.RW])

      op3.loop(v := mesh.vertices.index(),?\label{code:slope_limiter_expr}?
               op3.loop(c := mesh.star(v, k=2).index(),
                        max_kernel(dg.dat[c], cg.dat[v])))
    \end{pyalg2}
  \end{minipage}
  \label{listing:app2_code}
\end{listing}

If we restrict our attention to only the lowest-order discontinuous space, $P_0$, the algorithm for determining both $u_i^{\textnormal{min}}$ and $u_i^{\textnormal{max}}$ is simple.
Shown in \cref{alg:slope_limiter,fig:slope_limiter_stencil} for the $u_i^{\textnormal{max}}$ case, one needs to, at each vertex, loop over adjacent cells and set the vertex value, kept in a $P_1$ function space, to the minimum/maximum value.

Implementing the algorithm is straightforward in \pyop3 and can be done in only a few lines (\cref{listing:app2_code}, line~\ref{code:slope_limiter_expr}).
The star of the vertex ($\plexstar(v)$, \cref{sec:dmplex_queries}) is used to identify adjacent cells with the extra keyword argument `\pycode{k=2}' used to ensure that the returned map only includes cells in the star, skipping the vertex and edges.

\begin{listing}
  \centering
  \begin{minipage}{.9\textwidth}
    \inputminted[linenos,escapeinside=??]{c}{./experiments/slope_limiter/c_code_tidy.c}
  \end{minipage}
  \caption{(Abbreviated) C code generated from the loop expression in \cref{listing:app2_code}.}
  \label{listing:app2_codegen}
\end{listing}

The code that is generated from the loop expression is shown in \cref{listing:app2_codegen}.
The inner loop over cells is ragged and so the extent of the loop is set by the variable \ccode{p_0}, drawn from an externally provided array \ccode{array_0} (line~\ref{code:ragged_cell_loop}.
The index expression for packing the $P_0^{\textnormal{disc}}$ cell data (line~\ref{code:messy_expr})
\begin{mycinline}
  array_3[array_4[array_1[array_2[i_0] + i_1]]]
\end{mycinline}
is fairly non-trivial and illustrates some of the power of the ability to compose index expressions and layout functions that \pyop3 has.
The subexpression
\begin{mycinline}
  array_1[array_2[i_0] + i_1]]
\end{mycinline}
constitutes the ragged access of the $\plexstar$ map.
Being ragged, the stride is non-uniform and so the tabulation of steps is encoded in \ccode{array_2}.
With this, the correct value from \ccode{array_3} can now be read through substituting the $\plexstar$ index into its layout function \ccode{array_4}.

As written, it is not possible to implement the algorithm in \pyop2 because the map from vertices to cells has variable arity.
One can reframe the algorithm as a loop over cells, writing the maximum values to the vertices in the cell's closure, but that is specific to this example; more complex algorithms like, say, vertex-centered patch smoothers~\cite{farrellPCPATCHSoftwareTopological2021} cannot be expressed in \pyop2.
\pyop3 therefore provides a more general solution.

\subsection{Performance results}

\begin{table}
  \centering
  \begin{tblr}{|[1pt]l|[1pt]l|[1pt]}
    \hline[1pt]
    \textbf{Arithmetic throughput} & $6.3 \times 10^8$ FLOP/s \\
    \hline[1pt]
    \textbf{Arithmetic intensity} & 0.052 FLOP/byte \\
    \hline[1pt]
    \textbf{Peak throughput} & $1.6 \times 10^9$ FLOP/s \\
    \hline[1pt]
    \textbf{Percentage of peak throughput} & 39\% \\
    \hline[1pt]
  \end{tblr}
  \caption{Roofline results from running the slope limiter code (\cref{listing:app2_code}).}
  \label{tab:slope_limiter_performance}
\end{table}

Lacking independent variables to vary, the code was executed just once in serial for a $200\times200$ triangular mesh.

The roofline results are shown in \cref{tab:slope_limiter_performance}.
The arithmetic intensity, measured by LIKWID, is extremely low because only a single floating point operation (a comparison) is performed, and many arrays are streamed from memory.
The calculation is therefore strongly memory-bound.
Regardless, the code achieves a reasonable fraction of theoretical peak performance, meaning that
\pyop3 is clearly generating reasonably performant code that makes effective use of the available hardware.

\section{Outlook}

The performance results from these demonstrator applications are encouraging.
The performance of the code that \pyop3 generates is both comparable to \pyop2 and sufficiently close to theoretical peak to suggest that the approach is sound.
The slope limiter results additionally show that reasonable performance is attainable even for circumstances not expressible with \pyop2.

Optimisations have also been identified that would enable performance parity with \pyop2, which we class as high priority future work.

\end{document}
