\documentclass[thesis]{subfiles}

\begin{document}

\chapter{Foundations}
\label{chapter:foundations}

% TODO: Reference the checklist from the previous chapter

% NOTE: This is sooo hard to write well.
\pyop3 was created to be a successor to \pyop2, and so it is instructive to review how \pyop2 works and identify any shortcomings.
We will then review a number of libraries whose abstractions capture the missing behaviour.

\section{\pyop2}
% NOTE: should mention partial GPU support (cite Kaushik)
% NOTE: should mention extruded (then mention at end of doc)

Just like \pyop3, \pyop2 is an execution model for the application of compact computational kernels over unstructured meshes~\cite{rathgeberPyOP2HighLevelFramework2012}.
It was introduced to provide the same abstractions as OP2~\cite{mudaligeOP2ActiveLibrary2012} but using runtime code generation instead of source-to-source translation.
It is a core component of the Firedrake finite element framework~\cite{FiredrakeUserManual}.

\subsection{Data structures}

\pyop2 has no innate concept of what an unstructured mesh is.
Instead, topological entities are treated as \textit{sets}, with \textit{mappings} between the different sets.

There are 3 different types of data structures defined in \pyop2: globally constant values, vectors and matrices.
These are termed \pycode{Globals}, \pycode{Dats} and \pycode{Mats} respectively.

For more complex problems like the Stokes equations in \cref{sec:stokes_equations} the \glspl{dof} are associated with multiple types of topological entity.
In \cref{fig:scott_vogelius_element_P3} for example the unknowns are associated with the cell, edges and vertices.
This means that one has to associate the \glspl{dof} for that function space with a distinct \textit{node set}, rather than a set for a particular topological entity.
As a consequence, the data structures do not know to what topological entity they refer and the library user must take responsibility for constructing the right maps from, say, cells to nodes.

\subsection{Loops}

In order to apply kernels to these data structures, a \textit{parallel loop} (\pycode{par_loop}) is constructed and executed.
The loop takes as arguments a \textit{local kernel}, \textit{iteration set} and zero or more \textit{arguments} that provide the data structures needed by the local kernel.

% mention loopy here briefly (cref{sec:pyop2_codegen})

% arguments may need map from iteration set to its own (node) set, also accessors


An example loop statement is shown in \cref{fig:pyop2_parloop}.

\subsection{Code generation}
\label{sec:pyop2_codegen}

\begin{figure}
  \includegraphics{pyop2_codegen_flowchart.pdf}
  \caption{Simplified code generation pathway for a \pyop2 parallel loop.}
  \label{fig:pyop2_codegen}
\end{figure}

\begin{figure}
  \centering
  \begin{subfigure}{.49\textwidth}
    \begin{pylisting}
      knl = loopy.make_kernel(
        "{ [i]: 0 <= i < n }",  # domains
        "y[i] = 2*x[i]",        # instructions
        [                       # arguments
          loopy.GlobalArg("x", dtype=float),
          loopy.GlobalArg("y", dtype=float),
          loopy.ValueArg("n", dtype=int),
        ],
      )
    \end{pylisting}
    \caption{Python code to construct the kernel. Some arguments to \pycode{make_kernel} have been omitted for simplicity.}
    \label{fig:loopy_codegen_kernel}
  \end{subfigure}
  %
  \begin{subfigure}{.49\textwidth}
    \inputminted[fontsize=\footnotesize]{c}{./scripts/artefacts/loopy_example_c_code_tidy.c}
    \caption{The generated C code.}
    \label{fig:loopy_codegen_c}
  \end{subfigure}

  \caption{
    An example loopy kernel.
    The kernel takes two array arguments, \pycode{x} and \pycode{y}, and sets the values in \pycode{y} to twice those in \pycode{x}.
    Both arrays have the same unknown length \pycode{n} which is also passed in to the kernel as an argument.
  }
  \label{fig:loopy_codegen}
\end{figure}

The code generation pipeline is summarised in \cref{fig:pyop2_codegen}.
Having constructed a parallel loop, \pyop2 executes it by first lowering the loop object through a sequence of intermediate representations before compiling and running the generated low-level code.

The first intermediate representation is loopy~\cite{klocknerLooPyTransformationbased2014}, a polyhedral model inspired Python code generation library.

With loopy, the main entry point is the declaration of a \pycode{LoopKernel}.
To construct such a kernel, the user needs to specify \textit{domains}, \textit{instructions} and \textit{arguments}.
An example loopy kernel is shown in \cref{fig:loopy_codegen_kernel}, with the generated C string shown in \cref{fig:loopy_codegen_c}.

% TODO: talk about GPUs
loopy is capable of generating code for multiple backends including CPUs and GPUs and so \pyop2 targeting loopy should, in principle, allow the same \pyop2 code to target different architectures.

% TODO: talk about vectorisation (cite TJ)
Once a \pycode{LoopKernel} has been created, loopy also provides a wealth of different code transformations such as loop tiling, vectorisation and loop-invariant code motion.

\subsection{Parallel execution}
\label{sec:pyop2_parallel}

% NOTE: closure is novel concept...
\begin{figure}
  \centering
  \includegraphics{pyop2_split_mesh.pdf}
  \caption{
    \pyop2 entity classes for a mesh distributed between 2 processes.
    Points belonging to process 1 (left) are shown in red and points belonging to process 2 (right) are shown in blue.
    \textit{Ghost} points are indicated by points whose colour match the other process, \textit{owned} points are faded and the rest are labelled \textit{core}.
    We assume that one is only computing cell-wise integrals on the mesh and so the mesh overlap need only contain enough ghost points to ensure that all the cells have a complete closure.
  }
  \label{fig:pyop2_split_mesh}
\end{figure}

\begin{algorithm}
  \begin{verbatim}
Trigger required halo exchanges
FOR EACH item IN iterset.core:
  compute(item)

Await halo exchanges
FOR EACH item IN iterset.owned:
  compute(item)
  \end{verbatim}
  \caption{The \pyop2 parallel loop execution algorithm to interleave computation and communication.}
  \label{alg:pyop2_comp_comm_overlap}
\end{algorithm}

By keeping careful track of the parallel decomposition of sets, \pyop2 is capable of interleaving computation and communication when executing parallel loops.
To do so each set is split into 3 parts:

\begin{itemize}
  \item \textit{Core}: Set elements that do not require any data from other processes during a parallel loop.

  \item \textit{Owned}: Set elements that belong to the current process but do require data from other processes.

  \item \textit{Ghost}: Set elements present on a process that belong to another process.
\end{itemize}

% NOTE: closure is a new term here
The number of \textit{ghost} points is known to the mesh already, as it has knowledge of its overlap.
To determine the \textit{core} and \textit{owned} partitions one loops over the cells of a mesh and inspecting all the points in the closure of the cell.
If any of the points in the closure are \textit{ghost}, then all other points in the closure are marked as \textit{owned}.
Any remaining points without a label are labelled \textit{core}.
An example partitioning of a distributed mesh into \textit{core}, \textit{owned} and \textit{ghost} points is shown in \cref{fig:pyop2_split_mesh}.

From this partitioning it is possible to interleave computation and communication.
Computations over points marked \textit{core} are not influenced by any ghost data and so can proceed before ghost data has been communicated. \textit{Owned} points need to have up-to-date ghost data and so must wait for all communication to be completed before beginning.
This is shown in \cref{alg:pyop2_comp_comm_overlap}.

% NOTE: This is an acceptable repeat of the sorts of things said in chapter 1.
\subsection{Limitations}

% NOTE: Not sure I have anything to actually ref here
As mentioned in \cref{sec:???}, \pyop2 is negatively affected by a number of design choices that limit its suitability:

% NOTE: bad spacing
\paragraph{Poor composability}{
  Associating mesh data with \textit{node sets} discards topological information and places a burden on the library user to keep track of the relations between the mesh topology and the nodes.
}

\paragraph{Inflexible interface}{
  Not all mesh operations are expressible as a single kernel executed within a single loop over entities.
  Algorithms for physics-based preconditioners such as hybridisation~\cite{gibsonSlateExtendingFiredrake2020} and additive Schwartz methods~\cite{farrellPCPATCHSoftwareTopological2021} involve multiple kernels and nested loops and so implementing them required sui-generis additions to \pyop2 that are difficult to extend.
  It would be preferable to have an abstraction for mesh computations that was sufficiently flexible for these algorithms to be expressible.
}

\pyop3 aims to resolve both of these limitations with \pyop2 by rethinking the core abstractions.
The design of a number of libraries were used as inspiration for approaches to solving these problems.
These will be reviewed below.

\section{Relating unknowns to mesh topology: DMPlex}

To tackle the problem of composability - sets discarding information about mesh topology - we look to DMPlex, an abstraction for unstructured meshes.
% * DMPlex is part of PETSc, a toolkit for...

\begin{figure}
  \centering
  \begin{subfigure}{.49\textwidth}
    \centering
    \includegraphics{two_cell_mesh.pdf}
  \end{subfigure}
  %
  \begin{subfigure}{.49\textwidth}
    \centering
    \includegraphics{dmplex_hasse.pdf}
  \end{subfigure}
  \caption{TODO, mention Hasse diagram}
  \label{fig:dmplex_hasse}
\end{figure}

With DMPlex, an unstructured mesh is represented as a \gls{dag} (\cref{fig:dmplex_hasse}).
All topological entities are referred to as \textit{points}, with each entity type belonging to a particular \textit{strata} of the overall \gls{dag}.
By treating all entities as points, DMPlex is capable of expressing unstructured meshes of any dimension.

% rich query language

\begin{figure}
  \centering
  \begin{subfigure}{.49\textwidth}
    \centering
    \includegraphics{dmplex_hasse_cone.pdf}
    \caption{$\cone(0) = \{6,7,8\}$}
  \end{subfigure}
  %
  \begin{subfigure}{.49\textwidth}
    \centering
    \includegraphics{dmplex_hasse_support.pdf}
    \caption{$\support(8) = \{0,1\}$}
  \end{subfigure}

  \begin{subfigure}{.49\textwidth}
    \centering
    \includegraphics{dmplex_hasse_closure.pdf}
    \caption{$\closure(0) = \{0,6,7,8,2,3,4\}$}
  \end{subfigure}
  %
  \begin{subfigure}{.49\textwidth}
    \centering
    \includegraphics{dmplex_hasse_star.pdf}
    \caption{$\plexstar(3) = \{3,7,8,9,0,1\}$}
  \end{subfigure}

  \caption{
    The possible DMPlex covering queries applied to the Hasse diagram from Figure~\ref{fig:dmplex_hasse}.
  }
  \label{fig:dmplex_queries}
\end{figure}

% TODO: MUST talk about mesh renumbering here, also parallel stuff (SF)
% No, parallel can be later, just mention here that the mesh is distributed and
% point to the parallel chapter.
% Numbering can reference the FEM pseudo-code - we need cellwise data to be "close"

\subsection{Representing data layouts with DMPlex}
% * DMPlex
  % * layouts with petscsection

% data layouts on meshes
  % stencils
    % stencils in dmplex, topological query language
%
%
% In order to associated data with these mesh points, a typical PETSc application will construct a PETSc \ccode{Section}.
% These are simple CSR-like (?) data structures that encode a data layout by associating a particular number of DoFs with each mesh point.
% Sections are a powerful tool for describing data layouts but they have a number of limitations:
%
% \begin{itemize}
%   \item
%     Sections are fully ragged.
%     They only store DoF information per point in a completely unstructured way and are incapable of knowing, say, that every cell in the mesh stores exactly one DoF.
%     This can prohibit the compiler from making certain optimisations (e.g. loop unrolling) that it would have been able to do were it to know of a constant loop extent.
%     Additionally, this variable size increases memory pressure as redundant arrays of constant sizes need to be streamed through memory.
%
%   \item
%     DoFs per point are treated as a flat array.
%     This means that shape information is lost for, say, vector-valued functions.
% \end{itemize}
%
% With PETSc/DMPlex, the P3 DoF layout would be represented as shown in Figure~\ref{???}.
% % figure showing PetscSection info

% End the section by showing some pseudocode for FEM assembly

\section{A language for structured data: numpy}

% (cite paper, search for "citing numpy")
% although not a JIT, numpy is important for introducing ergonomic ways of
% expressing array computations
% there are some numpy JIT options out there: JAX, pytato, pytorch?

\subsection{N-dimensional arrays}

The key abstraction introduced by numpy is the \textit{N-dimensional array}, or \pycode{ndarray}.

% shape
% broadcast operations

% implemented as a header and a buffer
% stride information (for views)

%TODO: cleanup
\subsection{Indexing arrays}
\label{sec:numpy_indexing_arrays}

\begin{table}
  \centering

  \begin{tabular}{|l|l|l|l|}
    \hline
    \textbf{Index operation} & \textbf{Example} & \textbf{Return value} & \textbf{Array return type} \\
    \hline
    Single element indexing & \pycode{array[1]} & \pycode{"B"} & N/A \\
    \hline
    Slicing & \pycode{array[1:6:2]} & \pycode{["B", "D", "F"]} & View \\
    \hline
    Integer array indexing & \pycode{array[[0, 3, 4]]} & \pycode{["A", "D", "E"]} & Copy \\
    \hline
  \end{tabular}

  \caption{
    Common indexing operations for numpy arrays.
    The examples shown apply the index to the string array \pycode{["A", "B", "C", "D", "E", "F"]} (called \pycode{array} above).
    The array return type for single element indexing is marked as ``N/A" because a string is returned instead of an array.
  }
  \label{tab:numpy_indexing_ops}
\end{table}

Broadcasting operations, that handle the entire array monolithically, are not adequate for many programs.
numpy, therefore, allows for array \textit{indexing}, where portions of the full array are extracted to yield a new array.

Some of the ways that a numpy array may be indexed are shown in \cref{tab:numpy_indexing_ops}:

\begin{itemize}
  \item \textbf{Single element indexing}

    Single element indexing takes an integer and simply returns the value stored at that point.

  \item \textbf{Slicing}

    Slices are a standard Python concept for describing ranges of indices and have the syntax \pycode{[start:stop:step]}.
    Omitting \pycode{start}, \pycode{stop} or \pycode{step} will default to 0, the end of the array, and 1 respectively.
    In the example shown in \cref{tab:numpy_indexing_ops} the slice \pycode{[1:6:2]} corresponds to asking for ``the values in the array from index 1 (inclusive) to index 6 (exclusive), striding by 2".

  \item \textbf{Integer array indexing}

    Integer array indexing returns a new array containing values stored at the requested indices, for the example in \cref{tab:numpy_indexing_ops} this simply being 0, 3 and 4.

\end{itemize}

Although the examples provided are all for a 1-dimensional array, it is completely permissible to index N-dimensional arrays with a collection of these indexing operations, one per axis of the array.
This collection of indices is termed a \textit{multi-index}.

numpy draws a distinction between ``basic" indexing, single element indexing and slicing, and ``advanced" indexing like using integer arrays.
For the former, the array returned from the indexing operation is a \textit{view}, whereas for the latter a \textit{copy} is returned.
Alongside the obvious memory advantages, views are also preferable to copies because they are \textit{composable}.
One can take views of views repeatedly without triggering a copy, allowing for changes to the indexed array to be propagated back to the original.
In \pyop3, as well as generalising the indexing operations above to axis trees, we overcome this shortcoming of numpy's advanced indexing such that views are always used regardless of the indexing method used.

\end{document}
