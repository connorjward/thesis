\documentclass[thesis]{subfiles}

\begin{document}

\chapter{Foundations}
\label{chapter:foundations}

Before introducing \pyop3, it is instructive to review its `foundations', that is, pieces of software that inspired its design.
In order these are: \numpy, PETSc DMPlex and \pyop2.

\section{Relating unknowns to mesh topology: DMPlex}
\label{sec:foundations_dmplex}

% NOTE: Where is PETSc first mentioned? Possibly here is a good place for more detail about it.
% Should be mentioned in the introduction
To tackle the problem of composability - sets discarding information about mesh topology - we look to DMPlex~\cite{knepleyMeshAlgorithmsPDE2009,langeEfficientMeshManagement2016,knepleyUnstructuredOverlappingMesh2015}.
DMPlex is an abstraction for unstructured meshes bundled with PETSc, a scientific software toolkit providing implementations of parallel vectors and matrices as well as a plethora of linear and non-linear solvers~\cite{petsc-web-page,petsc-user-ref,petsc-efficient}.
It has support for a range of complex mesh operations such as mesh refinement, I/O~\cite{hamEfficientNtoMCheckpointing2024}, all in parallel (\cref{sec:dmplex_parallel}).  % TODO: needs more...

\begin{figure}
  \centering
  \begin{subfigure}{.49\textwidth}
    \centering
    \includegraphics{two_cell_mesh.pdf}
  \end{subfigure}
  %
  \begin{subfigure}{.49\textwidth}
    \centering
    \includegraphics{dmplex_hasse.pdf}
  \end{subfigure}
  \caption{TODO, mention Hasse diagram}
  \label{fig:dmplex_hasse}
\end{figure}

With DMPlex, the unstructured mesh is represented as a \gls{dag} (\cref{fig:dmplex_hasse}).
All topological entities are referred to as \textit{points}, with each entity type belonging to a particular \textit{strata} of the overall \gls{dag}.
\textit{Arrows} connect points between strata to represent the connectivity of the different entities.
For example every edge in a mesh (depth 1) has two arrows pointing at its two \textit{covered} vertices (depth 0).
By treating all entities as points, DMPlex is capable of expressing unstructured meshes of any dimension.

\begin{figure}
  \centering
  \begin{subfigure}{.49\textwidth}
    \centering
    \includegraphics{dmplex_hasse_cone.pdf}
    \caption{$\cone(0) = \{6,7,8\}$}
  \end{subfigure}
  %
  \begin{subfigure}{.49\textwidth}
    \centering
    \includegraphics{dmplex_hasse_support.pdf}
    \caption{$\support(8) = \{0,1\}$}
  \end{subfigure}

  \begin{subfigure}{.49\textwidth}
    \centering
    \includegraphics{dmplex_hasse_closure.pdf}
    \caption{$\closure(0) = \{0,6,7,8,2,3,4\}$}
  \end{subfigure}
  %
  \begin{subfigure}{.49\textwidth}
    \centering
    \includegraphics{dmplex_hasse_star.pdf}
    \caption{$\plexstar(3) = \{3,7,8,9,0,1\}$}
  \end{subfigure}

  \caption{
    The possible DMPlex covering queries applied to the Hasse diagram from Figure~\ref{fig:dmplex_hasse}.
  }
  \label{fig:dmplex_queries}
\end{figure}

DMPlex has a simple query language that can be used to traverse the \gls{dag} and build appropriate stencils.
It consists of two core operations: \textit{cone} and \textit{support}.
The cone of a point, written $\cone(p)$, is the set of points that are pointed to from $p$.
The support of a point, written $\support(p)$, is the dual of cone and yields the set of points that point to $p$.
Transitive closures for both of these operations, where the operation is applied repeatedly and all points are accumulated, are also supplied by DMPlex.
The transitive closure of cone is termed the \textit{closure} ($\closure(p)$) and the transitive closure of support is called the \textit{star} ($\plexstar(p)$).
These operations are shown in \cref{fig:dmplex_queries}.

It is possible to compose these operations to yield larger stencils.
For instance $\support(\cone(p))$ with $p$ a cell produces the set of cells sharing an edge with $p$ (and $p$ itself).
Likewise, $\closure(\plexstar(p))$ is the \textit{adjacency} relation for finite element calculations: basis functions on points within this stencil have non-zero support with the basis functions on $p$.

\subsection{Representing data layouts}
\label{sec:dmplex_data_layout}

\begin{figure}
  \begin{clisting}
    // set cell DoFs
    DMPlexGetDepthStratum(dmplex, 2, &start, &end);
    for (int cell=start; cell<end; cell++)
      PetscSectionSetDof(section, cell, 2);

    // set edge DoFs
    DMPlexGetDepthStratum(dmplex, 1, &start, &end);
    for (int edge=start; edge<end; edge++)
      PetscSectionSetDof(section, edge, 4);

    # set vertex DoFs
    DMPlexGetDepthStratum(dmplex, 0, &start, &end);
    for (int vertex=start; vertex<end; vertex++)
      PetscSectionSetDof(section, vertex, 2);

    PetscSectionSetUp(section);
  \end{clisting}

  \caption{
    C code constructing an appropriate \ccode{Section} for a $[P_3]^2$ finite element (\cref{fig:scott_vogelius_element_P3}).
    Some boilerplate code is omitted.
  }
  \label{fig:section_p3}
\end{figure}

\begin{algorithm}
  \begin{verbatim}
counter = 0
offset = 0
FOR EACH point IN chart
  renumbered_point = renumber(point)
  offsets[counter] = offset
  counter += 1
  offset += PetscSectionGetDof(renumbered_point)
  \end{verbatim}

  \caption{
    The tabulation algorithm that determines the right offsets from a \ccode{Section}.
    This code is executed during \ccode{PetscSectionSetUp()}.
  }
  \label{alg:petsc_section_tabulate}
\end{algorithm}

In order to associate data with these mesh points, a user typical constructs a \ccode{Section} object.
These are simple CSR-like data structures that associate mesh points with offsets into an array.

\ccode{Sections} are constructed by assigning a number of \glspl{dof} with each mesh point (\cref{fig:section_p3}).
Once the number of \glspl{dof} has been specified, calling \ccode{PetscSectionSetUp()} traverses the input points and accumulates the offset for each point.
As a simple example, given the \gls{dof} count \ccode{[1, 0, 3, 2, 0, 1]} the \ccode{Section} would tabulate the following offsets: \ccode{[0, 1, 1, 4, 6, 6]} (\cref{alg:petsc_section_tabulate}).

\ccode{Sections} are also able to express the sorts of renumbering locality optimisations described in \cref{sec:intro_mesh_numbering}.
One simply provides the \ccode{Section} with a \textit{permutation} that is accounted for during set up (\cref{alg:petsc_section_tabulate}).

Whilst a powerful tool for describing data layouts, \ccode{Sections} have a number of limitations:

\begin{itemize}
  \item
    \textbf{They are fully ragged}\\
    \ccode{Sections} do not distinguish between different types of topological entity and so important structure cannot be represented.
    They only store DoF information per point in a completely unstructured way and are incapable of knowing, say, that every cell in the mesh stores exactly one DoF.
    This can prohibit the compiler from making certain optimisations (e.g. loop unrolling) that it would have been able to do were it to know of a constant loop extent.
    Additionally, this variable size increases memory pressure as redundant arrays of constant sizes need to be streamed through memory.

  \item
    \textbf{Shape information is lost}\\
    Though \ccode{Sections} allow one to directly associate \glspl{dof} with mesh entities of different dimension, they still lose information about the structure of the function space.
    For instance, to a \ccode{Section}, a point with a single vector-valued node is indistinguishable from a point with multiple nodes.
\end{itemize}

These limitations prevent \pyop3 from directly using \ccode{Sections} to describe its data layouts, but it takes a similar approach:

\begin{itemize}
  \item All topological entities in the mesh should be equivalent,
  \item Interleaved points require one to tabulate an array of offsets (\cref{alg:petsc_section_tabulate}).
\end{itemize}

\subsection{Parallel}
\label{sec:dmplex_parallel}

As touched upon in \cref{sec:intro_mpi}, DMPlex works in parallel by partitioning the entire ``global" mesh into local pieces that are kept on each process.
Each local piece of the mesh is \textit{closed}, meaning that all points in the cell closures are represented locally.
This means that there are \textit{ghost} points kept on each process that are owned by other processes.

\subsubsection{Star forests}

\begin{figure}
  \centering
  %
  \begin{subfigure}{\textwidth}
    \centering
    \includegraphics{star_forest_point.pdf}
    \caption{TODO}
    \label{fig:star_forest_point}
  \end{subfigure}
  %
  \begin{subfigure}{\textwidth}
    \centering
    \includegraphics{star_forest_dof.pdf}
    \caption{TODO}
    \label{fig:star_forest_dof}
  \end{subfigure}
  %
  \caption{TODO}
  \label{fig:star_forest}
\end{figure}

\begin{figure}
  \centering
  \begin{subfigure}{.6\textwidth}
    \centering
    \includegraphics{star_forest_abstract_mesh.pdf}
    \caption{Star forest suitable for describing the point sharing in \cref{fig:star_forest_point}.}
  \end{subfigure}
  %
  \begin{subfigure}{.39\textwidth}
    \centering
    \includegraphics{star_forest_abstract_global.pdf}
    \caption{Star forest appropriate for storing the communication pattern of a globally consistent value.}
  \end{subfigure}
  \caption{
    Common star forest patterns.
    Each point is labelled with its owning process and root and leaf points are shown in black and white respectively.
  }
  \label{fig:star_forest_abstract}
\end{figure}

% supports broadcasts from the root and reductions from the leaves

Parallel communication in DMPlex is handled by \textit{star forest} objects (\ccode{PetscSF}) \parencite{zhangPetscSFScalableCommunication2021}.
Star forests relate equivalent points across processes as a collection - or ``forest" - of ``stars".
A ``star" is a directed graph with depth 1 and a single root.

Star forests are effective for describing point-to-point MPI operations because they naturally encode the source and destination nodes as roots and leaves of the stars.
They can flexibly describe a range of different communication patterns.
For example, a value shared globally across $n$ ranks can be represented as a star forest containing a single star with the root node on rank 0 and $n-1$ leaves, 1 for each other rank.
This is shown in Figure~\ref{fig:???}.
Star forests are also suitable for describing the overlap between parts of a distributed mesh.
In this case, each star in the forest represents a single point (cell, edge, vertex) in the mesh with the root on the ``owning" rank and leaves on the ranks where the point appears as a ``ghost".
An example of such a distribution is shown in Figure~\ref{???}.

% Because local numbers on one process are related to local numbers on other processes no global enumeration of points is strictly required.

Upon construction a DMPlex holds a \textit{point star forest} that stores the information about ghost points (\cref{fig:star_forest_point}).
This is insufficient for function data to be communicated correctly though because the number of \glspl{dof} associated with each entity can change.
To transfer function data, PETSc composes the point star forest with a \ccode{Section} to produce a new star forest (\cref{fig:star_forest_dof}).

% maybe use alg. from chapter 6 here?

        % def create_dof_sf_inner(axis: Axis, component: AxisComponent, ...):
        %   section = create_section(axis, component)
        %   dof_sf = component.sf.createSectionSF(section, ...)
        %   return dof_sf

\section{A language for structured data: \numpy}

Rather more well-known than \pyop2 or DMPlex, \numpy~\cite{harrisArrayProgrammingNumPy2020} is the de-facto package for scientific computation in Python.
Underpinning a great many popular libraries, ...

%ndarray

Given that much of this thesis is dedicated to representing and transforming hierarchically structured data, it is valuable to review how this is done in \numpy as many of the concepts and syntactic elements are reused in \pyop3.

\subsection{N-dimensional arrays}

The key abstraction introduced by \numpy is the \textit{N-dimensional array}, or \pycode{ndarray}.
It is an intuitive and widely used interface for operating on multi-dimensional data.

% not suitable for pyop3 because we have more heterogeneous data layouts (but works for pyop2 node sets)

% shape
% broadcast/vectorised operations

% implemented as a header and a buffer
% stride information (for views)

\subsection{Indexing arrays}
\label{sec:numpy_indexing_arrays}

\begin{table}
  \centering

  \begin{tabular}{|l|l|l|l|}
    \hline
    \textbf{Index operation} & \textbf{Example} & \textbf{Return value} & \textbf{Array return type} \\
    \hline
    Single element indexing & \pycode{array[1]} & \pycode{"B"} & N/A \\
    \hline
    Slicing & \pycode{array[1:6:2]} & \pycode{["B", "D", "F"]} & View \\
    \hline
    Integer array indexing & \pycode{array[[0, 3, 4]]} & \pycode{["A", "D", "E"]} & Copy \\
    \hline
  \end{tabular}

  \caption{
    Common indexing operations for \numpy arrays.
    The examples shown apply the index to the string array \pycode{["A", "B", "C", "D", "E", "F"]} (called \pycode{array} above).
    The array return type for single element indexing is marked as ``N/A" because a string is returned instead of an array.
  }
  \label{tab:numpy_indexing_ops}
\end{table}

Broadcasting operations, that handle the entire array monolithically, are not adequate for many programs.
\numpy, therefore, allows for array \textit{indexing}, where portions of the full array are extracted to yield a new array.

Some of the ways that a \numpy array may be indexed are shown in \cref{tab:numpy_indexing_ops}:

\begin{itemize}
  \item \textbf{Single element indexing}

    Single element indexing takes an integer and simply returns the value stored at that point.

  \item \textbf{Slicing}

    Slices are a standard Python concept for describing ranges of indices and have the syntax \pycode{[start:stop:step]}.
    Omitting \pycode{start}, \pycode{stop} or \pycode{step} will default to 0, the end of the array, and 1 respectively.
    In the example shown in \cref{tab:numpy_indexing_ops} the slice \pycode{[1:6:2]} corresponds to asking for ``the values in the array from index 1 (inclusive) to index 6 (exclusive), striding by 2".

  \item \textbf{Integer array indexing}

    Integer array indexing returns a new array containing values stored at the requested indices, for the example in \cref{tab:numpy_indexing_ops} this simply being 0, 3 and 4.

\end{itemize}

% These operations work together with the header specs to figure out the right offsets.

Although the examples provided are all for a 1-dimensional array, it is completely permissible to index N-dimensional arrays with a collection of these indexing operations, one per axis of the array.
This collection of indices is termed a \textit{multi-index}.

\numpy draws a distinction between ``basic" indexing, single element indexing and slicing, and ``advanced" indexing like using integer arrays.
For the former, the array returned from the indexing operation is a \textit{view}, whereas for the latter a \textit{copy} is returned.
Alongside the obvious memory advantages, views are also preferable to copies because they are \textit{composable}.
One can take views of views repeatedly without triggering a copy, allowing for changes to the indexed array to be propagated back to the original.

% mention in chapter 4!
% In \pyop3, as well as generalising the indexing operations above to axis trees, we overcome this shortcoming of numpy's advanced indexing such that views are always used regardless of the indexing method used.

\section{PyOP2}

% NOTE: should mention extruded (then mention at end of doc)

As first introduced in \cref{sec:intro_existing_software}, PyOP2 is a library for doing mesh stencil calculations~\cite{rathgeberPyOP2HighLevelFramework2012}.
It was originally introduced to provide the same abstractions as OP2~\cite{mudaligeOP2ActiveLibrary2012}, but using runtime code generation instead of source-to-source translation.
It is a core component of the Firedrake finite element framework~\cite{FiredrakeUserManual}.

\subsection{Data structures}

\begin{table}
  \centering
  \begin{tblr}{|[1pt]l|[1pt]l|[1pt]l|[1pt]l|[1pt]}
    \hline[1pt]
    & \textbf{Global} & \textbf{Dat} & \textbf{Mat} \\
    \hline[1pt]
    Usage & Globally constant value & Distributed vector & Distributed matrix \\
    \hline
    Constructor & \pycode{GlobalDataSet} & \pycode{DataSet} & 2 \pycode{DataSets} \\
    \hline
    Storage type & \numpy \pycode{ndarray} & \numpy \pycode{ndarray} & PETSc \ccode{Mat} \\
    \hline[1pt]
  \end{tblr}
  \caption{\pyop2 data structures.}
  \label{tab:pyop2_data_types}
\end{table}

Like \numpy, \pyop2 data structures are composed of a semi-opaque piece of memory and a data layout descriptor providing semantic information about said memory.
This data layout descriptor, termed a \emph{data set}, is constructed by taking a set, say, of vertices and associating some shape information with it.

Summarised in \cref{tab:pyop2_data_types}, \pyop2 has three supported data structures: \pycode{Globals} (constants), \pycode{Dats} (vectors), and \pycode{Mats} (matrices).
Each is constructed with a different data layout specification: \pycode{Globals} take a \pycode{GlobalDataSet}, \pycode{Dats} take a \pycode{DataSet}, and \pycode{Mats} take two \pycode{DataSets}, one each for the rows and columns.

\pycode{Globals} and \pycode{Dats} both store data as \numpy arrays under the hood, whereas \pycode{Mats} use PETSc matrices (also termed \ccode{Mats}).

For multi-field problems such as the Stokes example in \cref{chapter:introduction} one has \textit{mixed data sets}, which are simply a collection of regular data sets.
Mixed data sets yield \pycode{MixedDats} which are, again, simply a collection of regular \pycode{Dats}.
An example of a \pyop2 data layout for a multi-field problem is shown in \cref{fig:scott_vogelius_element_dof_layout_pyop2}.
Observe that, since the DoFs are associated with multiple mesh entities, data is stored on a \emph{node set}, losing topological information (\cref{sec:pyop2_limitations}).

\subsection{Parallel loops}

\begin{listing}
  \centering
  \begin{minipage}{.9\textwidth}
    \begin{pyalg2}
      par_loop(kernel,                          # local kernel
               src_set,                         # iteration set
               dat0(READ, src_to_target_map),   # arguments
               dat1(WRITE, src_to_target_map))
    \end{pyalg2}
  \end{minipage}
  \caption{
    Code to construct and execute a \pyop2 parallel loop.
  }
  \label{listing:pyop2_parloop_demo}
\end{listing}

In order to apply kernels to these data structures, a \textit{parallel loop} is constructed and executed.
The loop takes as arguments a \textit{local kernel}, \textit{iteration set} and zero or more \textit{arguments} that provide the data structures needed by the local kernel.
For an example parloop see \cref{listing:pyop2_parloop_demo}.

\begin{itemize}
  \item
    \pycode{kernel} is the local kernel representing the stencil operation to be performed per iteration.
    It may be either a string of C code or a \textit{loopy kernel} (see \cref{sec:pyop2_codegen}).
  \item
    \pycode{src_set} is the set that is iterated over.
  \item
    \pycode{dat0} and \pycode{dat1} are \pycode{Dat} objects (vectors) that are passed through to the local kernel.
    Both associate data with some \pycode{target_set} and hence are addressed with the map \pycode{src_to_target_map}.
    \pycode{dat0} is accessed using the \pycode{READ} access descriptor, meaning that it only needs to be packed and passed into the local kernel, whereas \pycode{dat1} is accessed with \pycode{WRITE}, so it only requires unpacking.
\end{itemize}

\subsubsection{Interleaving computation and communication}

\begin{algorithm}
  \caption{The PyOP2 parallel loop execution algorithm to interleave computation and communication.}
  %
  \begin{algorithmic}[1]
    \State \Call{BeginHaloExchanges}{} \Comment{Begin sending halo data}

    \For{\textit{item} \textbf{in} \textit{iterset.core}} \Comment{Do the computations that \textbf{do not} need halo data}
      \State \Call{Compute}{\textit{item}}
    \EndFor

    \State \Call{AwaitHaloExchanges}{} \Comment{Wait for halo exchanges to complete}

    \For{\textit{item} \textbf{in} \textit{iterset.owned}} \Comment{Do the computations that \textbf{do} need halo data}
      \State \Call{Compute}{\textit{item}}
    \EndFor
  \end{algorithmic}
  \label{alg:pyop2_comp_comm_overlap}
\end{algorithm}

By keeping careful track of the parallel decomposition of sets, \pyop2 is capable of interleaving computation and communication when executing parallel loops.
To do so each set is split into 3 parts:

\begin{description}
  \item[\coreiter]
    Set elements that do not require any data from other processes during a parallel loop.

  \item[\ownediter]
    Set elements that are owned by the current process but require data from other processes.

  \item[\ghostiter]
    Set elements present on a process that belong to another process.
\end{description}

From this partitioning it is possible to interleave computation and communication.
Computations over \coreiter points do not read any ghost data, and hence can be executed without waiting for any communication to complete, whereas \ownediter points need valid ghost data and so must wait for all communication to be completed before beginning.
This is shown in \cref{alg:pyop2_comp_comm_overlap}.

To facilitate this algorithm, \pyop2 stores data in a partitioned manner, with \coreiter points preceding all \ownediter points and all \ownediter preceding all \ghostiter points:

\begin{center}
  \includegraphics{pyop2_partition.pdf}
\end{center}

This is convenient as it provides a no-copy method for dropping \ghostiter points - just return a truncated view of the same buffer - but, in our view, conflates several concepts that should be kept separate.
This is discussed in detail in \cref{sec:communication_optimisations}.

\subsection{Code generation}
\label{sec:pyop2_codegen}

\begin{figure}
  \includegraphics{pyop2_codegen_flowchart.pdf}
  \caption{Simplified code generation pathway for a \pyop2 parallel loop.}
  \label{fig:pyop2_codegen}
\end{figure}

To turn the abstract parallel loop definition into executable, fast code, \pyop2 utilises run-time code generation.
First, the parallel loop is \emph{lowered} to loopy (see below), which is then lowered to C code and compiled to a binary.
This binary may then be called passing in the arguments from the original parallel loop.
This process is summarised in \cref{fig:pyop2_codegen}.

\subsubsection{Loopy}

\begin{listing}
  \centering
  \begin{minipage}{.9\textwidth}
    \inputminted{text}{./scripts/artefacts/pyop2_example_loopy_kernel_tidy.txt}
  \end{minipage}
  \caption{
    Abbreviated textual representation of the loopy kernel generated for the example parallel loop in \cref{listing:pyop2_parloop_demo}.
  }
  \label{listing:pyop2_example_loopy_kernel}
\end{listing}

\begin{listing}
  \centering
  \begin{minipage}{.9\textwidth}
    \inputminted{c}{./scripts/artefacts/pyop2_example_c_code_tidy.c}
  \end{minipage}
  \caption{
    The C code generated from the loopy kernel in \cref{listing:pyop2_example_loopy_kernel}.
  }
  \label{listing:pyop2_example_c_code}
\end{listing}

Loopy~\cite{klocknerLooPyTransformationbased2014}, a polyhedral-model-inspired code generation library written in Python, is the primary intermediate representation of \pyop2.
Given a parallel loop, \pyop2 generates a loopy \pycode{LoopKernel} which is then lowered to C code and compiled.
In order to construct a \pycode{LoopKernel} the user needs to provide \emph{domains} (loops), \emph{instructions}, and \emph{arguments} (data passed into the kernel).
For the parallel loop in \cref{listing:pyop2_parloop_demo}, the \pycode{LoopKernel} and generated C code are shown in \cref{listing:pyop2_example_loopy_kernel} and \cref{listing:pyop2_example_c_code} respectively.
The local kernel, here omitted, is also (usually) a \pycode{LoopKernel} and would be included in the generated code as a separate function.

Given its high level of abstraction, loopy is a convenient target for \pyop2 for a number of reasons:

\begin{itemize}
  \item
    \pycode{LoopKernels} may be transformed to apply optimisations such as loop tiling, common subexpression elimination and vectorisation~\cite{sunStudyVectorizationMatrixfree2020}.
  \item
    Code may be generated for targets other than CPUs including the GPU languages OpenCL and CUDA.
    This means that \pyop2 is capable of executing parallel loops on GPUs without making intrusive changes to the code - a fact demonstrated (as a proof-of-concept) by \cite{fenics2021-kulkarni}.
\end{itemize}

\subsection{Limitations}
\label{sec:pyop2_limitations}

As mentioned in \cref{sec:intro_missing_abstraction}, the design of \pyop2 has two key shortcomings:

\begin{description}
  \item[Inflexible interface]
    Not all mesh operations are expressible as a single kernel executed within a single loop.
    Algorithms for physics-based preconditioners such as hybridisation~\cite{gibsonSlateExtendingFiredrake2020} and additive Schwartz methods~\cite{farrellPCPATCHSoftwareTopological2021} involve multiple kernels and nested loops and so implementing them has required sui-generis additions to \pyop2 that are difficult to extend.

  \item[Loses topological information]
    Associating mesh data with \textit{node sets} discards topological information.
    This places a burden on the library user to keep track of the relations between the mesh topology and the nodes, and also makes composition (e.g. of maps) harder.
\end{description}

\pyop3 aims to resolve both of these limitations with \pyop2 by rethinking the core abstractions.
In particular it introduces a new abstraction for data layouts, superseding \pycode{DataSets}, that is capable of representing nodal data layouts without losing reference to the mesh topology.
This new abstraction is called an \emph{axis tree}.

\end{document}
