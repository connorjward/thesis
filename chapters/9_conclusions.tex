\documentclass[thesis]{subfiles}

\begin{document}

\chapter{Conclusions}
\label{chapter:conclusions}

This thesis introduces a novel suite of abstractions that together are suitable for representing general-purpose mesh stencil calculations.
Compared with existing frameworks, the new abstractions are better able to represent the semantics of the problem domain, enabling a greater range of algorithms, transformations, and optimisations to be expressed and applied.

The core abstractions introduced by this work are the axis tree (\cref{chapter:axis_trees}) and index tree (\cref{chapter:indexing}).
Axis trees provide a means to flexibly describe a wide range of data layouts common to continuum mechanics simulations.
They were inspired by the observation that data layouts are better described as trees, instead of traditional N-dimensional arrays or unstructured sets.

Index trees take axis trees and transform them into other, indexed, axis trees that carry information about the structure of the data access.
They are valuable for creating \emph{views} of arrays: new versions of arrays that impose additional structure symbolically, without requiring any copying of data.
Views are essential for stencil calculations as they enable large, global arrays to be \emph{packed} and \emph{unpacked} into small local temporaries at each iteration.

In combination, axis trees and index trees allow for a multiple different representations of the same underlying data to exist - for instance representing a mesh as a uniform set of points or as distinct collections of cells, edges, etc.
This allows one to reason about, for example, renumbering the data layout (\cref{sec:firedrake_renumbering_mesh}), whilst still supporting an expressive interface that deals with maps between classes of mesh entities.
The result is an abstraction that `bridges the gap' between existing stencil codes, allowing one to solve problems with \emph{both} complex data structures \emph{and} complex algorithms.

This thesis is also accompanied by a new Python library, \pyop3, that provides an implementation of these new abstractions (\cref{chapter:pyop3}).
From a high-level specification of the algorithm, expressed in a loop-based DSL, \pyop3 generates high performance low-level code that efficiently evaluates the computation, even in parallel (\cref{chapter:parallel}).

In \cref{chapter:firedrake} we demonstrated \pyop3's functionality by integrating it into the finite element library Firedrake, replacing its precursor \pyop2.
The code generated by \pyop3 is currently slower than \pyop2 (\cref{chapter:demonstrator_applications}) but optimisations have been identified that would eliminate this gap.

\section{Future research directions}
\label{sec:future_work}

The core motivation for creating \pyop3 was to enable the development of computational methods not possible with other mesh stencil frameworks.
The number of future research directions is therefore very high; we summarise a selection of them below:

\subsubsection{Firedrake on GPUs}

One of the core benefits of having high-level abstractions in scientific software is that implementation-specific detail may be hidden, allowing multiple computational backends to be supported with little to no effort.
Notably this means that GPUs, fast becoming the dominant hardware on the largest supercomputers, may be supported with very few changes to the code.
This contrasts hugely with codes written in low-level languages, where porting functionality to GPUs requires rewriting much of the software stack and can take multiple years to achieve.

In order for Firedrake to be able make effective use of GPUs it is important that finite element assembly is supported on them.
The execution of the assembly loops is the responsibility of \pyop3 (\cref{sec:firedrake_assembly}) and so therefore extending \pyop3 to support evaluating mesh stencils on GPUs is necessary.

The work needed to enable this functionality is not too onerous.
It consists of two parts:
(1) generating GPU code (e.g. CUDA) instead of C from the loop expressions, and
(2) having GPU-aware data structures that can transparently copy data between host and device.
Of these, (2) is straightforward to do, a very similar lazy-transfer process already exists to minimise ghost exchanges (\cref{sec:parallel_minimising_communication}); and functionality for (1) has already been implemented in loopy and \pyop2~\cite{fenics2021-kulkarni}, it must simply be ported over to \pyop3.

Once finite element assembly on a GPU is supported, Firedrake should be able to offload entire solves.
PETSc efforts to support solving linear and non-linear problems on a GPU are becoming increasingly mature~\cite{millsPerformancePortablePETScGPUbased2020,millsPETScTAODevelopments2024}
and the rest of the Firedrake software stack (e.g. TSFC) exist at an abstraction level where the CPU/GPU consideration is not relevant.

\subsubsection{More stencil algorithms}

One of the original inspirations for \pyop3's development was the fact that a number of algorithms, specifically additive-Schwartz methods~\cite{farrellPCPATCHSoftwareTopological2021} and hybridisation~\cite{gibsonSlateExtendingFiredrake2020}, did not fit into the single-loop, single-kernel paradigm enforced by \pyop2 and so sui generis changes were needed to implement them.
\pyop3, with its much more flexible interface, will enable these algorithms to be rewritten in a more general way.
Furthermore, it should also become possible to implement other mesh stencil algorithms that until now have been inexpressible.
Examples of such algorithms include Weighted Essentially Non-Oscillatory (WENO) slope limiters~\cite{liuWeightedEssentiallyNonoscillatory1994} and finite volume schemes.

\begin{figure}
  \centering
  \begin{subfigure}{.49\textwidth}
    \centering
    \includegraphics{h_adaptive_mesh.pdf}
    \caption{$h$-adapted mesh.}
    \label{fig:h_adaptive_mesh}
  \end{subfigure}
  \begin{subfigure}{.49\textwidth}
    \centering
    \includegraphics{p_adaptive_mesh.pdf}
    \caption{$p$-adapted mesh.}
  \end{subfigure}

  \caption{
    A two-cell mesh where $h$-refinement (left) and $p$-refinement (right) have been applied to the right-most cell.
    Diamonds indicated constrained DoFs.
  }
  \label{fig:adaptive_mesh}
\end{figure}

\subsubsection{Runtime transformations}

As described in \cref{sec:firedrake_orientations}, cells in unorientable meshes may disagree on the relative orientation of shared entitities.
Performing local assembly therefore requires that the global DoFs be transformed into some canonical setting.
Since the shared DoFs may need to be transformed differently for each cell, this transformation must occur at runtime, during the pack/unpack operation.
For `simple' function spaces this transformation is merely a permutation, but for others (e.g. \hdiv and \hcurl) the transformation involves the application of a matrix.
With some small modifications, \pyop3 should be capable of representing such transformations symbolically, enabling fast code to be generated that applies them.

A similar situation can occur with adaptive methods.
Shown in \cref{fig:adaptive_mesh} for the $h$-adaptive (left, mesh refinement) and $p$-adaptive (right, polynomial degree refinement) cases, adjacent cells can have `hanging nodes' - DoFs that are visible to one cell but not the other.
These DoFs are not true degrees of freedom and they do not appear in the global matrix/vector.
Instead, they are computed using the `true' unconstrained DoFs.
For example, the extra vertex in \cref{fig:h_adaptive_mesh} is usually constrained to be the average value of the surrounding vertices.
The application of these constraints is another cell-wise transformation of the global data, similar in nature to the orientation transformations above, and so \pyop3 should facilitate the implementation of adaptive methods.
Furthermore, $p$-adaptive methods store a variable number of DoFs per element and so require ragged/non-trivially structured arrays.
These should be expressible in \pyop3 using axis trees.
Note $h$-adaptivity and $p$-adaptivity can be combined to $hp$-adaptivity and any techniques developed should apply equally to this case.

\subsubsection{Different data structures}

The abstractions introduced in this thesis are sufficiently high-level that the specific choices of underlying data structure, currently \numpy{} arrays and PETSc matrices, are not fixed.
For cases like additive-Schwartz methods, using PETSc matrices may have poor performance because they are designed for massively parallel applications, whereas patch problems instead require the assembly of many small matrices.
Instead, using dense arrays or matrix types from other linear algebra libraries like LIBXSMM~\cite{heineckeLIBXSMMAcceleratingSmall2016} may be more suitable.
Similarly, for particle and adaptive methods the data structures may need to be frequently updated to account for moving particles or extra unknowns.
For these cases one may want to use dynamically resizable data structures like the \ccode{vector} type from the \cplusplus{} standard library.
By providing a higher level of abstraction for data structures - \pycode{Globals}, \pycode{Dats}, and \pycode{Mats} - \pyop3 abstracts away the underlying data structures such that alternatives could be used transparently.

\subsubsection{Communication optimisations}

\begin{figure}
  \centering
  \includegraphics[scale=2]{dg_minimal_exchange.pdf}
  \caption{
    Cell-wise parallel stencil for a finite element calculation involving a $Q_1^{\textnormal{disc}}$ function space.
    DoFs are represented as blue circles, with hollow circles representing ghost DoFs owned by the other process.
    The mesh overlap is larger than necessary since interior facet integrals over `macro' cells must be accommodated.
  }
  \label{fig:dg_minimal_exchange}
\end{figure}

In some circumstances the DoF star forests encapsulating the overlap between adjacent processes (\cref{sec:parallel_star_forests}) are suboptimal.
Depending on the stencil being applied, some or all of the ghost data may not need to be transferred.

One example where this is occurs is with cell-wise assembly loops for discontinuous Galerkin (DG) finite element methods.
Since DG methods involve integration over facets (\cref{sec:firedrake_facet_integration}), the mesh overlap between processes must be large enough for the correct macro cells to exist.
This is suboptimal for cell-wise assembly loops because the overlap is now larger than necessary: the extra DoFs in the overlap will never be modified and so they do not need to be communicated.
An example demonstrating this is shown in \cref{fig:dg_minimal_exchange}; the ghost DoFs (hollow circles) are never touched by the stencil and so their values do not need to be exchanged.
This is especially a problem at high order, where the number of redundantly transferred DoFs can be very large.

In order to fix this, one needs to have \emph{per-loop} parallel exchange behaviour.
This is possible to represent in \pyop3 because it is aware of the parallel communication patterns.
Using an algorithm similar to \cref{alg:partition_iterset}, \pyop3 could inspect the mesh stencils to construct per-loop star forests that only exchange the needed data.

\subsubsection{Compiler optimisations}

As well as the optimisations listed above, it may be valuable to pursue implementing more traditional compiler optimisations within \pyop3.
As an example, one could consider performing a \emph{sparse tiling} transformation to DG finite element assembly loops.
It is usual for DG methods to have assembly loops over both the cells and interior facets of the mesh:
\begin{pyinline}
  loop(c := mesh.cells.index(), cell_kernel(...))
  loop(f := mesh.interior_facets.index(), facet_kernel(...))
\end{pyinline}
However, this approach exhibits poor data locality because each loop will load a lot of the same data from memory.
To improve performance we could therefore partition the mesh into small \emph{tiles}, and then perform both assembly loops on each tile in turn:
\begin{pyinline}
  loop(tile := mesh.tiles().index(), [
    loop(c := tile.cells.index(), cell_kernel(...))
    loop(f := tile.interior_facets.index(), facet_kernel(...))
  ])
\end{pyinline}
This will improve data locality since data for the second loop will not yet have been evicted from cache.
\pyop3 should have enough information to represent and perform this transformation.

Note that this is a simplified approach not accounting for complications such as different access descriptors.
More general sparse tiling approaches (e.g.~\cite{kriegerLoopChainingProgramming2013,stroutGeneralizingRunTimeTiling2014,luporiniAutomatedTilingUnstructured2019}) could also be pursued.

\end{document}
