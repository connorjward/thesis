\documentclass[thesis]{subfiles}

\begin{document}

\chapter{Conclusions}
\label{chapter:conclusions}

This thesis introduces a novel suite of abstractions that together are suitable for representing general-purpose mesh stencil calculations.
Compared with existing frameworks, the new abstractions are better able to represent the semantics of the problem domain, enabling a greater range of algorithms, transformations, and optimisations to be expressed and applied.

The core abstractions introduced by this work are the axis tree (\cref{chapter:axis_trees}) and index tree (\cref{chapter:indexing}).
Axis trees provide a means to flexibly describe a wide range of data layouts common to continuum mechanics simulations.
They were inspired by the observation that data layouts are better described as trees, instead of traditional N-dimensional arrays or unstructured sets.

Index trees take axis trees and transform them into other, indexed, axis trees that carry information about the structure of the data access.
They are valuable for creating \emph{views} of arrays: new versions of arrays that impose additional structure symbolically, without requiring any copying of data.
Views are essential for stencil calculations as they enable large, global arrays to be \emph{packed} and \emph{unpacked} into small local temporaries at each iteration.

By better capturing the structure that underlies mesh data, these abstractions allow for far more expressive ways of describing how the data are accessed during mesh stencil algorithms, thus enabling one to program computations intuitively from a high level of abstraction with a DSL (\cref{chapter:pyop3}).
Axis trees and index trees therefore together form a powerful abstraction that `bridges the gap' between the different limitations of existing stencil codes (\cref{???}), allowing one to solve problems with \emph{both} complex data structures \emph{and} complex algorithms.

This thesis is also accompanied by a new Python library, \pyop3, that provides an implementation of these new abstractions (\cref{chapter:pyop3}).
From a high-level specification of the algorithm \pyop3 is able to generate high-performance, low-level code that efficiently evaluates the computation, even in parallel (\cref{chapter:parallel}).

In \cref{chapter:firedrake} we demonstrated \pyop3's functionality by integrating it into the finite element library Firedrake, replacing its precursor \pyop2.
The code generated by \pyop3 has performance that is both comparable with \pyop2 and close to theoretical peak (\cref{chapter:demonstrator_applications}).

\section{Summary of benefits}

Compared with existing frameworks, in particular its predecessor \pyop2, \pyop3 can boast a number of significant improvements:
\begin{itemize}
  \item
    By constructing data layouts programmatically with axis trees, \pyop3 is able to handle a much wider range of different possible data layouts that may appear in mesh stencil calculations.
    In particular, it has support for \emph{ragged} data layouts (\cref{sec:ragged_axis_trees}), which \pyop2 has no capability to handle.
  \item
    Indexing operations may be composed arbitrarily without incurring any copying of data.
    This, among other things, allows \pyop3 to represent complex transformations \emph{symbolically} and hence evaluate them at run-time (e.g. \cref{sec:firedrake_orientations_pyop3}).
  \item
    By decoupling the data \emph{layout} specification (axis trees) from the data \emph{access} specification (index trees), \pyop3 has transparent support for different semantically-equivalent data layouts (\cref{sec:axis_tree_alternative_layouts,sec:indexing_data_layout_transformations}).
  \item
    Unlike \pyop2, which only has support for single-loop, single-kernel algorithms, \pyop3 has a DSL capable of expressing a much wider range of algorithms involving multiple (possibly nested) loops and multiple kernels (e.g. \cref{sec:demo_apps_slope_limiter}).
  \item
    \pyop3 can perform inspector-executor optimisations on the indirection expressions used to address global data structures to improve data locality and reduce data volume (\cref{sec:codegen_optimisation}).
  \item
    Various improvements have been made to the abstractions and algorithms related to distributed memory parallelism (\cref{chapter:parallel}).
    In particular:
    \begin{itemize}
      \item
        The \coreiter{}/\ownediter{}/\ghostiter{} set partitioning has been broken up into \ownediter{}/\ghostiter{} for parallel data layouts and \coreiter{}/\rootiter{}/\leafiter{} for iteration sets (\cref{sec:parallel_data_layouts,sec:parallel_overlap_computation_communication}).
        This decouples the specification of the data layout from the specification of the parallel loop, enabling support for stencils of different sizes.
      \item
        Ghost DoFs for multi-field data structures are always stored at the end of the array, allowing for no-copy transformations between local and global representations (\cref{sec:parallel_data_layouts}).
      \item
        The status of \rootiter{} points is tracked along with \leafiter{} to further reduce the amount of communication that is needed between loop invocations (\cref{sec:parallel_minimising_communication}).
    \end{itemize}
\end{itemize}

\section{Future research directions}
\label{sec:future_work}

The core motivation for creating \pyop3 was to enable the development of computational methods not possible with other mesh stencil frameworks.
The number of future research directions is therefore very high; we summarise a selection of them below:

\subsubsection{Firedrake on GPUs}

One of the core benefits of having high-level abstractions in scientific software is that implementation-specific detail may be hidden, allowing multiple computational backends to be supported with little to no effort.
Notably this means that GPUs, fast becoming the dominant hardware on the largest supercomputers, may be supported with very few changes to the code.
This contrasts hugely with codes written in low-level languages, where porting functionality to GPUs requires rewriting much of the software stack and can take multiple years to achieve.

In order for Firedrake to be able make effective use of GPUs it is important that finite element assembly is supported on them.
The execution of the assembly loops is the responsibility of \pyop3 (\cref{sec:firedrake_assembly}) and so therefore extending \pyop3 to support evaluating mesh stencils on GPUs is necessary.

The work needed to enable this functionality is not too onerous.
It consists of two parts:
(1) generating GPU code (e.g. CUDA) instead of C from the loop expressions, and
(2) having GPU-aware data structures that can transparently copy data between host and device.
Of these, (2) is straightforward to do, a very similar lazy-transfer process already exists to minimise ghost exchanges (\cref{sec:parallel_minimising_communication}); and functionality for (1) has already been implemented in loopy and \pyop2~\cite{fenics2021-kulkarni}, it must simply be ported over to \pyop3.

Once finite element assembly on a GPU is supported, Firedrake should be able to offload entire solves.
PETSc efforts to support solving linear and non-linear problems on a GPU are becoming increasingly mature~\cite{millsPerformancePortablePETScGPUbased2020,millsPETScTAODevelopments2024}
and the rest of the Firedrake software stack (e.g. TSFC) exist at an abstraction level where the CPU/GPU consideration is not relevant.

\subsubsection{Beyond finite elements}
\label{sec:more_stencils}

One of the original inspirations for \pyop3's development was the fact that a number of algorithms, specifically additive-Schwartz methods~\cite{farrellPCPATCHSoftwareTopological2021} and hybridisation~\cite{gibsonSlateExtendingFiredrake2020}, did not fit into the single-loop, single-kernel paradigm enforced by \pyop2 and so sui generis changes were needed to implement them.
\pyop3, with its much more flexible interface, will enable these algorithms to be rewritten in a more general way.

Furthermore, it should also become possible to implement other mesh stencil algorithms outside of a finite element setting.
Examples of such algorithms include:
\begin{description}
  \item[Graph neural networks]
    Graph neural networks (GNNs) are a type of neural network that operates on graphs~\cite{scarselliGraphNeuralNetwork2009}.
    They are valuable for modelling objects with complex structures across a wide range of disciplines including molecular chemistry~\cite{stokesDeepLearningApproach2020} and social networks~\cite{montiFAKENEWSDETECTION2019} where graphs are a natural way of representing the given input.

    One of the core operations needed by a GNN is \emph{message passing}, where nodes exchange information with their neighbours.
    Mathematically, this operation can be represented as the node-wise update
    \begin{equation}
      \mathbf{h}_u = \phi \left( \mathbf{x}_u, \bigoplus_{v \in \mathcal{N}_u} \psi(\mathbf{x}_u, \mathbf{x}_v) \right),
    \end{equation}
    where $\mathbf{x}_{(\cdot)}$ are the current node states and $\mathcal{N}_u$ is the set of nodes sharing an edge with $u$ (its neighbourhood)~\cite{bronsteinGeometricDeepLearning2021}.
    The update is computed by applying a non-parametrised aggregation function $\bigoplus$ (e.g. addition) to the result of a \emph{message function} $\psi$ inside an \emph{update function} $\phi$.
    Both $\psi$ and $\phi$ can be learned by the network.
    This operation, where local updates are computed using local unknowns, \emph{is a stencil operation} - and thus can be computed using \pyop3.

    Further, there is ongoing research in the machine learning community regarding whether more higher-dimensional data structures such as simplices~\cite{hwangMultiorderSimplexBasedGraph2024} and hypergraphs~\cite{fengHypergraphNeuralNetworks2019} have advantages over graphs due to their ability to express relations between more than two vertices at once.
    These data structures are very similar in nature to unstructured meshes, again suggesting that \pyop3 would be effective.
  \item[Molecular dynamics]
    % what are they and why are they useful?
    %   model forces between particles to model their movement
    %   used for:
    %     astrophysics (file:///home/connor/Downloads/1109.5095v1.pdf) horowitzNeutronStarCrust2011
    %     nuclear (file:///home/connor/Downloads/1-s2.0-S0022311514009271-main.pdf) williamsAtomisticInvestigationStructure2015
    %     biology (file:///home/connor/Downloads/PIIS0896627318306846.pdf) hollingsworthMolecularDynamicsSimulation2018

    % how do they constitute a stencil method?
    %   only consider local-ish particles? within cutoff distance
    %   bin in cells at least as large as the cutoff radius - helps with parallelisation
    %   then loop over the particles in these cells
    %   need codegen because of "inner loop" considerations
    % \cite{saundersDomainSpecificLanguage2018,saundersDevelopmentPerformancePortableFramework2019}
    % not possible to use PyOP2 because of raggedness, pyop3 would be suitable
\end{description}

\begin{figure}
  \centering
  \begin{subfigure}{.49\textwidth}
    \centering
    \includegraphics{h_adaptive_mesh.pdf}
    \caption{$h$-adapted mesh.}
    \label{fig:h_adaptive_mesh}
  \end{subfigure}
  \begin{subfigure}{.49\textwidth}
    \centering
    \includegraphics{p_adaptive_mesh.pdf}
    \caption{$p$-adapted mesh.}
  \end{subfigure}

  \caption{
    A two-cell mesh where $h$-refinement (left) and $p$-refinement (right) have been applied to the right-most cell.
    Diamonds indicated constrained DoFs.
  }
  \label{fig:adaptive_mesh}
\end{figure}

\subsubsection{Runtime transformations}

As described in \cref{sec:firedrake_orientations}, cells in unorientable meshes may disagree on the relative orientation of shared entitities.
Performing local assembly therefore requires that the global DoFs be transformed into some canonical setting.
Since the shared DoFs may need to be transformed differently for each cell, this transformation must occur at runtime, during the pack/unpack operation.
For `simple' function spaces this transformation is merely a permutation, but for others (e.g. \hdiv and \hcurl) the transformation involves the application of a matrix.
With some small modifications, \pyop3 should be capable of representing such transformations symbolically, enabling fast code to be generated that applies them.

A similar situation can occur with adaptive methods.
Shown in \cref{fig:adaptive_mesh} for the $h$-adaptive (left, mesh refinement) and $p$-adaptive (right, polynomial degree refinement) cases, adjacent cells can have `hanging nodes' - DoFs that are visible to one cell but not the other.
These DoFs are not true degrees of freedom and they do not appear in the global matrix/vector.
Instead, they are computed using the `true' unconstrained DoFs.
For example, the extra vertex in \cref{fig:h_adaptive_mesh} is usually constrained to be the average value of the surrounding vertices.
The application of these constraints is another cell-wise transformation of the global data, similar in nature to the orientation transformations above, and so \pyop3 should facilitate the implementation of adaptive methods.
Furthermore, $p$-adaptive methods store a variable number of DoFs per element and so require ragged/non-trivially structured arrays.
These should be expressible in \pyop3 using axis trees.
Note $h$-adaptivity and $p$-adaptivity can be combined to $hp$-adaptivity and any techniques developed should apply equally to this case.

\subsubsection{Runtime data layout transformations}

As described in \cref{sec:intro_data_layout_flex}, it is desirable for mesh stencil codes to have support for expressing the same data in a variety of layouts, so as to take advantage of the performance characteristics of different hardware.
In sections~\ref{sec:axis_tree_alternative_layouts} and~\ref{sec:indexing_data_layout_transformations} we established that this is entirely possible within \pyop3.
However, at present data layouts are considered immutable for the duration of a program.
This is not always correct since there are circumstances where one data layout may be optimal for one loop and another layout be optimal for another, for example if one is executed on a GPU and another on the CPU.
By committing to one layout over another \pyop3 is sacrificing performance.

To remedy this, \pyop3 data structures could internally store a number of arrays, each with a different data layout.
The optimal layout for different loops could then be used, transparently to the user.
To ensure data consistency between the arrays, the \pyop3 would need to employ state-tracking to lazily copy between them, similarly to how packages ensure consistency between data on a CPU and GPU (e.g.~\cite{millsPerformancePortablePETScGPUbased2020}).

Despite the differing layouts, axis trees make performing this copy trivial.
For example, given two arrays, \pycode{dat_soa} and \pycode{dat_aos}, with axis trees representing different-but-equivalent data layouts, copying one to the other is as simple as executing:
\begin{pyinline}
  dat_soa.assign(dat_aos)
\end{pyinline}

\subsubsection{Different data structures}

The abstractions introduced in this thesis are sufficiently high-level that the specific choices of underlying data structure, currently \numpy{} arrays and PETSc matrices, are not fixed.
For cases like additive-Schwartz methods, using PETSc matrices may have poor performance because they are designed for massively parallel applications, whereas patch problems instead require the assembly of many small matrices.
Instead, using dense arrays or matrix types from other linear algebra libraries like LIBXSMM~\cite{heineckeLIBXSMMAcceleratingSmall2016} may be more suitable.
Similarly, for particle and adaptive methods the data structures may need to be frequently updated to account for moving particles or extra unknowns.
For these cases one may want to use dynamically resizable data structures like the \ccode{vector} type from the \cplusplus{} standard library.
By providing a higher level of abstraction for data structures - \pycode{Globals}, \pycode{Dats}, and \pycode{Mats} - \pyop3 abstracts away the underlying data structures such that alternatives could be used transparently.

\subsubsection{Communication optimisations}

\begin{figure}
  \centering
  \includegraphics[scale=2]{dg_minimal_exchange.pdf}
  \caption{
    Cell-wise parallel stencil for a finite element calculation involving a $Q_1^{\textnormal{disc}}$ function space.
    DoFs are represented as blue circles, with hollow circles representing ghost DoFs owned by the other process.
    The mesh overlap is larger than necessary since interior facet integrals over `macro' cells must be accommodated.
  }
  \label{fig:dg_minimal_exchange}
\end{figure}

In some circumstances the DoF star forests encapsulating the overlap between adjacent processes (\cref{sec:parallel_star_forests}) are suboptimal.
Depending on the stencil being applied, some or all of the ghost data may not need to be transferred.

One example where this is occurs is with cell-wise assembly loops for discontinuous Galerkin (DG) finite element methods.
Since DG methods involve integration over facets (\cref{sec:firedrake_facet_integration}), the mesh overlap between processes must be large enough for the correct macro cells to exist.
This is suboptimal for cell-wise assembly loops because the overlap is now larger than necessary: the extra DoFs in the overlap will never be modified and so they do not need to be communicated.
An example demonstrating this is shown in \cref{fig:dg_minimal_exchange}; the ghost DoFs (hollow circles) are never touched by the stencil and so their values do not need to be exchanged.
This is especially a problem at high order, where the number of redundantly transferred DoFs can be very large.

In order to fix this, one needs to have \emph{per-loop} parallel exchange behaviour.
This is possible to represent in \pyop3 because it is aware of the parallel communication patterns.
Using an algorithm similar to \cref{alg:partition_iterset}, \pyop3 could inspect the mesh stencils to construct per-loop star forests that only exchange the needed data.

\subsubsection{Compiler optimisations}

As well as the optimisations listed above, it may be valuable to pursue implementing more traditional compiler optimisations within \pyop3.
As an example, one could consider performing a \emph{sparse tiling} transformation to DG finite element assembly loops.
It is usual for DG methods to have assembly loops over both the cells and interior facets of the mesh:
\begin{pyinline}
  loop(c := mesh.cells.index(), cell_kernel(...))
  loop(f := mesh.interior_facets.index(), facet_kernel(...))
\end{pyinline}
However, this approach exhibits poor data locality because each loop will load a lot of the same data from memory.
To improve performance we could therefore partition the mesh into small \emph{tiles}, and then perform both assembly loops on each tile in turn:
\begin{pyinline}
  loop(tile := mesh.tiles().index(), [
    loop(c := tile.cells.index(), cell_kernel(...))
    loop(f := tile.interior_facets.index(), facet_kernel(...))
  ])
\end{pyinline}
This will improve data locality since data for the second loop will not yet have been evicted from cache.
\pyop3 should have enough information to represent and perform this transformation.

Note that this is a simplified approach not accounting for complications such as different access descriptors.
More general sparse tiling approaches (e.g.~\cite{kriegerLoopChainingProgramming2013,stroutGeneralizingRunTimeTiling2014,luporiniAutomatedTilingUnstructured2019}) could also be pursued.

\end{document}
