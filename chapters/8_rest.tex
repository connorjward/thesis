\documentclass[thesis]{subfiles}

\begin{document}

% PERFORMANCE OPTIMISATIONS?

% e.g. can do things like \cite{dasSlicingAnalysisIndirect1994} and "flatten" repeated indexing

\chapter{Performance results}
\label{chapter:performance}

% Run on Pyramus
% There are 3 possible performance bottlenecks in assembly: payload transfer (pyop3) and the computation and matrix insertion (neither pyop3)
% so we want to check only the first of these

% experiment 1: assemble((f0+f1+f2+f3+...)*v*dx)
% * stresses memory bandwidth (minimal computational payload)
% * we are memory bound but ought to be near roofline
% * can compare with and without additional map payload
% * just time the kernel!

% * show a plot of FLOP/s vs AI and demonstrate near roofline
% * we can assume optimal caching by computing the streamed memory by adding the
%   number of DoFs together, could also have a pessimal case where we just multiply the local
%   kernel by the number of cells.
% * we can play with number of functions and degree to shift the AI

% experiment 2: slope limiter style problem
% * DG0 -> CG1, set vertex to max of surrounding cells
% * shows off ragged behaviour as patches are ragged
% * ideally have good roofline behaviour as before, not much else to see

% experiment 3: strong scaling (could be part of exp. 1)
% * just show assembly performance

% NOTE: Could possibly include scaling plots but these need to trigger a halo update every step as otherwise
% worthless. Hard to prove that strong-scaling is relevant/might be slow elsewhere.

\chapter{Summary}
\label{chapter:summary}

%pyop3 is like Ebb and Simit, but the data model is chosen to truly unify PETSc and N-dim arrays. We get maths-y optimisations that the others don't have, like mesh numbering and storing edge and vertex data together and mixed-type things. (plus obvs distributed parallel)
%
%It solves the same problem but from a different direction.
%
%Primary objectives: integration with PETSc, distributed parallel - data model that is compatible with DMPlex etc.
% we don't need our own mesh implementation - so distribution, I/O etc we get "for free"
%
%Simit is also very similar, but not distributed parallel and pyop3 unifies the hypergraph and linear algebra data views.
%
% and Liszt? Just quite old, reimplements a lot

\section{Future work}
\label{sec:future_work}

% highest priority is a complete pyop3 obvs.!

% compression transformation
% sparse data structures - already done a proof of concept, provide index array, we are already CSR so this is straightforward to do
% GPUs, vectorisation (\cref{sec:codegen_loopy_kernel})
% Hanging nodes: AMR? hp-adaptivity
% orientations?
% WENO slope limiters?
% finite volume?
% patchPC

\section{Conclusions}

\end{document}
